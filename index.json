[{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/3-ai/3.1/","title":"Create VPC &amp; Configure Security Groups for RDS and Lambda","tags":[],"description":"","content":"Create VPC \u0026amp; Configure Security Groups for RDS and Lambda Implementation Steps 1. Access the VPC service Go to AWS Management Console → search for VPC.\nSelect Your VPCs → Create VPC. Select VPC and more and name the project. In the VPC configuration section: for Number of Availability Zones select 1, for Number of public subnets select 1, for Number of private subnets select 2, and for NAT gateways select Zonal. NAT gateways choose In 1 AZ, VPC endpoints choose None → Create VPC.\nThis process will take a few minutes to create the NAT gateway.\nAfter creation is complete, we can view how \u0026lsquo;VPC and more\u0026rsquo; created the resources via the Resource map. We will create an additional private subnet located in a different AZ to be able to create the RDS instance. 2. Set up Security Groups We will create 2 separate Security Groups (SG) for maximum security.\nGo to Security Groups, click Create security group. We will create 2 security groups for Lambda and RDS.\nName the security group and select the VPC we created in step 1, then click create. Continue creating the SG for RDS, select the VPC from step 1. In the inbound rules section, for type select PostgreSQL, and for Source select the SG created for Lambda. "},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1” Event Objectives Share experience working in a business. Introducing AWS pre-trained models. Introducing bedrock agent core. Strengthen knowledge for proposal. Speakers Dinh Le Hoang Anh - Cloud Engineer Trainee First Cloud AI Journey Danh Hoang Hieu Nghi - AI Engineer Renova Cloud Lam Tuan Kiet - Sr DevOps Engineer FPT Software Key Highlights 1. AWS AI/ML Services Overview The session began with an introduction to AWS Pre-trained AI Services. These are \u0026ldquo;ready-to-use\u0026rdquo; solutions that allow developers to add intelligence to applications without needing deep machine learning expertise or building models from scratch.\nKey Service Categories: Computer Vision: Amazon Rekognition: Image and video analysis (facial recognition, object detection, text detection). Amazon Lookout (Vision \u0026amp; Equipment): Industrial solutions for anomaly detection and predictive maintenance. Language \u0026amp; Speech: Amazon Translate: Automated multilingual translation. Amazon Textract: Intelligent OCR to extract text and data from scanned documents. Amazon Transcribe: Speech-to-Text conversion. Amazon Polly: Text-to-Speech conversion with lifelike voices. Amazon Comprehend: NLP service for sentiment analysis, keyword extraction, and topic modeling. Search \u0026amp; Personalization: Amazon Kendra: Intelligent enterprise search powered by ML. Amazon Personalize: Real-time recommendation system service. Additional Note: Introduction to Pinecone (Vector Database). This is a critical component often used with AI services to store embeddings for semantic search.\n2. Generative AI with Amazon Bedrock This section shifted focus from traditional ML to Generative AI (GenAI) and the Amazon Bedrock platform.\n2.1. GenAI Fundamentals What is GenAI? Distinguished between Traditional ML (focused on analysis/prediction) and GenML (focused on generating new content). Foundation Models (FMs): Bedrock provides API access to leading foundation models. 2.2. Prompt Engineering Techniques to optimize model outputs:\nZero-Shot Prompting: Making requests without providing examples. Few-Shot Prompting: Providing a few examples to guide the model\u0026rsquo;s context and format. Chain of Thought (CoT): Encouraging the model to reason step-by-step to solve complex logic. 2.3. Retrieval-Augmented Generation (RAG) Combining LLMs with proprietary business data:\nEmbedding: Using the Amazon Titan Embedding model to vectorize text data. Knowledge Base: The process of creating a Knowledge Base on AWS allows chatbots to retrieve accurate information from internal documents, reducing hallucinations. 3. Advanced Workflows: Bedrock Agents Expanding GenAI capabilities from \u0026ldquo;answering\u0026rdquo; to \u0026ldquo;acting\u0026rdquo;.\nBedrock Agent Core: The core component that connects LLMs to backend APIs to execute tasks. Frameworks: Tools and frameworks available for building agents. Key Features: The ability to plan, decompose complex queries, and execute multi-step workflows automatically. 4. Conclusion The workshop provided a comprehensive roadmap for adopting AI on AWS:\nStart quickly with Pre-trained Services (Rekognition, Polly, etc.). Leverage the creative power of GenAI via Amazon Bedrock. Enhance accuracy using RAG and Vector DBs (Pinecone). Automate complex business processes with Bedrock Agents. Some event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “BUILDING AGENTIC AI Context Optimization with Amazon Bedrock” Event Objectives Technical deep-dive into Amazon Bedrock\u0026rsquo;s agent capabilities, covering core concepts, architecture, and key features for building AI agents on AWS. Real-world use case presentation by Diaflow\u0026rsquo;s CEO, showcasing practical implementation of agentic workflows using AWS services. Brief company overview and product introduction by CloudThinker\u0026rsquo;s co-founder, highlighting their AI orchestration platform. Advanced-level (L300) technical session exploring CloudThinker\u0026rsquo;s orchestration framework and context optimization techniques on Amazon Bedrock. Interactive coding workshop with CloudThinker engineers, focusing on practical implementations. Speakers Kien Nguyen - Solutions Architect Amazon Web Services Viet Pham -Founder cum CEO Diaflow Thang Ton - Co-founder \u0026amp; COO CloudThinker Henry Bui - Head of Engineering CloudThinker Kha Van - Community Leader Amazon Web Services Key Highlights 1. Deep Dive: Amazon Bedrock Agents The session began with an in-depth analysis of Amazon Bedrock Agents, marking the shift from passive LLM usage to proactive AI Agents.\nCore Concepts: Agents go beyond answering queries; they possess planning capabilities and can utilize tools to achieve objectives. Architecture: Connects Foundation Models (FMs) with enterprise data sources and APIs. Utilizes \u0026ldquo;Chain-of-Thought\u0026rdquo; reasoning to decompose complex user requests into executable steps. Key Features: Memory management to maintain conversational context. Integrated Retrieval-Augmented Generation (RAG). Tracing capabilities to monitor agent reasoning for debugging and optimization. 2. Real-World Case Study: Diaflow A presentation showcasing how Diaflow implemented \u0026ldquo;Agentic Workflows\u0026rdquo; in a production environment:\nImplementation: Leveraged AWS services to build automated workflows where Agents act as virtual employees handling specific tasks. Key Takeaways: Insights into overcoming challenges when deploying Agents to production, focusing on reliability and accuracy in real-world scenarios. 3. AI Orchestration Solution: CloudThinker Overview An introduction to CloudThinker and the critical role of an Orchestration Platform in the AI ecosystem:\nThe Challenge: Managing multiple models and agents becomes increasingly difficult as AI systems grow in complexity. The Solution: CloudThinker provides an orchestration layer to manage, monitor, and optimize interactions between AI components, allowing businesses to focus on logic rather than infrastructure. 4. Advanced Technical Session (L300): Orchestration \u0026amp; Context Optimization An advanced technical deep dive (Level 300) exploring the integration of CloudThinker with Amazon Bedrock:\nOrchestration Framework: Designing systems for multi-agent collaboration. Intelligent routing logic to direct tasks to the appropriate specialized Agent. Context Optimization: Techniques for managing LLM Context Windows on Bedrock. Strategies for selecting critical information for prompts to reduce token costs and improve response accuracy. 5. Workshop: Interactive Coding A hands-on session focused on practical implementation:\nActivity: Coding to integrate CloudThinker’s framework with Amazon Bedrock. Outcome: Successfully built a basic Agent workflow, covering request handling, context processing, and API execution. 6. Conclusion The event highlighted critical directions for modern AI application development:\nAgentic AI is the future: Moving from Q\u0026amp;A Chatbots to Action-Oriented Agents. Orchestration is essential: Robust orchestration layers (like CloudThinker) are necessary to manage complex AI systems. Optimization is key: Effective context management on Bedrock directly impacts project performance and cost efficiency. Some event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/4-frontend/4.1/","title":"Hosting website with S3","tags":[],"description":"","content":" Step 1: Create an S3 Bucket Go to the S3 service. Click Create bucket. Enter a unique Bucket name (for example: flyora-shop). Uncheck “Block all public access” Acknowledge the warning about public access. Click Create bucket. Step 2: Upload Website Files Open your newly created bucket. Click Upload → Add files → select your website files (e.g, index.html) Click Upload. Step 3: Enable Static Website Hosting Go to the Properties tab of your bucket. Scroll down to Static website hosting. Click Edit → Enable Static website hosting Enter: Index document: index.html Error document: index.html (optional) Click Save changes. *Go to the Permission tab of your bucket. Edit Bucket Policy Paste the following JSON policy (replace flyora-shop with your bucket name): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; } ] } Step 4: Test Your Website Click the Bucket website endpoint URL. http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com\nMake sure it looks like this "},{"uri":"https://workshop-sample.fcjuni.com/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Trinh Quoc Bao\nPhone Number: 0327835351\nEmail: baotqse182782@fpt.edu.vn\nUniversity: FPT University\nMajor: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/1-introduction/","title":"Introduction","tags":[],"description":"","content":"Introduction What is Flyora? Flyora is a modern e-commerce platform designed to demonstrate cloud-native architecture using AWS serverless services. This workshop guides you through building a fully functional online store with product browsing, AI-powered chatbot assistance, and automated data management.\nWorkshop Objectives By completing this workshop, you will:\nDeploy a serverless e-commerce system on AWS Implement automated data pipelines using S3 and Lambda triggers Build RESTful APIs with API Gateway and Lambda Create a scalable database using DynamoDB Integrate AI chatbot functionality for customer support Deploy a static frontend with S3 and CloudFront Set up CI/CD pipelines for automated deployments Architecture Overview The Flyora platform uses a fully serverless architecture:\nFrontend Layer:\nStatic website hosted on Amazon S3 Global content delivery via Amazon CloudFront Responsive UI for product browsing and shopping Backend Layer:\nAPI Gateway for RESTful API endpoints AWS Lambda functions for business logic Amazon DynamoDB for product and order data Amazon S3 for data import and storage AI Layer:\nAI-powered chatbot for product recommendations Integrated into the frontend UI Provides real-time customer assistance Security \u0026amp; Authentication:\nAmazon Cognito for user authentication IAM roles for secure service access Workshop Structure This workshop is organized into team-based modules:\nBackend Team - API development and data pipeline AI Team - Chatbot integration and AI features Frontend Team - UI development and deployment CI/CD - Automated deployment pipeline Testing - System validation and performance testing Cleanup - Resource management and cost optimization Expected Outcomes After completing this workshop, you will have:\nA fully functional e-commerce website running on AWS Hands-on experience with serverless architecture Understanding of AWS best practices for scalability and security Knowledge of CI/CD implementation for cloud applications A portfolio project demonstrating cloud engineering skills Cost Considerations This workshop is designed to run within the AWS Free Tier. All services used have free tier options, and the architecture avoids costly resources like EC2 instances. Estimated cost for running this workshop: $0-5 USD if completed within a few hours.\nPrerequisites Before starting, ensure you have:\nAn AWS account with administrative access Basic understanding of cloud computing concepts Familiarity with REST APIs and JSON Basic knowledge of HTML/CSS/JavaScript (for frontend work) Git installed on your local machine "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/2-backend/2.1/","title":"Prepare &amp; Configure Lambda Trigger for S3","tags":[],"description":"","content":"Prepare \u0026amp; Configure Lambda Trigger for S3 Steps to Perform 1. Access the IAM Service Go to the AWS Management Console → search for IAM. Select Roles → Create Role. Choose Trusted entity type: AWS service. Choose Use case: Lambda, then click Next. 2. Attach Permissions to the Role Attach the following policies:\nAmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Click Next, then name the role LambdaS3DynamoDBRole.\nThis role allows Lambda to read files from S3 and write data to DynamoDB.\nCreate an S3 Bucket Go to the S3 service. In the S3 interface, select Create bucket. On the Create bucket screen:\nBucket name: Enter a name, for example:\nflyora-bucket-database (If the name already exists, add a number at the end.)\nKeep all other default settings unchanged.\nReview your configuration and click Create bucket to finish. Expected Results The flyora-bucket (or your chosen name) is successfully created. The LambdaS3DynamoDBRole role is ready to be assigned to Lambda in the next step. Configure Lambda Trigger for S3 In this step, you will configure AWS Lambda to automatically import CSV files into DynamoDB whenever a new file is uploaded to the S3 Bucket.\nCreate a Lambda Function Go to Lambda → Create function. Select Author from scratch. Function name: AutoImportCSVtoDynamoDB. Runtime: Python 3.13. Role: select LambdaS3DynamoDBRole created in the previous step. Add a Trigger In the Configuration → Triggers tab, click Add trigger. Choose S3. Select Bucket flyora-bucket. Event type: All object create events. Click Add to save. Paste the Lambda Code\nPaste the following code: import boto3 import csv import io import json from botocore.exceptions import ClientError from decimal import Decimal dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) # ------------------------- # Hàm kiểm tra kiểu dữ liệu của mẫu (Detect Type) # ------------------------- def detect_type(value): val_str = str(value).strip() # Check Int/Float try: float(val_str) return \u0026#39;N\u0026#39; # Number except ValueError: pass return \u0026#39;S\u0026#39; # String # ------------------------- # Hàm chuyển đổi dữ liệu (Convert) # ------------------------- def convert_value(value): if value is None: return None val_str = str(value).strip() if val_str == \u0026#34;\u0026#34;: return None # Int check try: if float(val_str).is_integer(): return int(float(val_str)) except ValueError: pass # Decimal check (cho Float) try: return Decimal(val_str) except Exception: pass # Boolean if val_str.lower() == \u0026#34;true\u0026#34;: return True if val_str.lower() == \u0026#34;false\u0026#34;: return False return val_str # ------------------------- # Tạo bảng Dynamic dựa trên kiểu dữ liệu phát hiện được # ------------------------- def create_table_if_not_exists(table_name, pk_name, pk_type): existing_tables = dynamodb.meta.client.list_tables()[\u0026#39;TableNames\u0026#39;] if table_name in existing_tables: print(f\u0026#34;Table \u0026#39;{table_name}\u0026#39; already exists.\u0026#34;) return print(f\u0026#34;Creating table: {table_name} | PK: {pk_name} | Type: {pk_type}\u0026#34;) table = dynamodb.create_table( TableName=table_name, KeySchema=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;KeyType\u0026#39;: \u0026#39;HASH\u0026#39;}], AttributeDefinitions=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;AttributeType\u0026#39;: pk_type}], BillingMode=\u0026#39;PAY_PER_REQUEST\u0026#39; ) table.wait_until_exists() print(\u0026#34;Table created successfully.\u0026#34;) # ------------------------- # Main Handler # ------------------------- def lambda_handler(event, context): try: for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] print(f\u0026#34;Processing: {key}\u0026#34;) response = s3.get_object(Bucket=bucket, Key=key) # 1. QUAN TRỌNG: Dùng \u0026#39;utf-8-sig\u0026#39; để xóa BOM, giúp nhận diện số chính xác body = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8-sig\u0026#39;) reader = csv.DictReader(io.StringIO(body)) # Clean headers reader.fieldnames = [name.strip() for name in reader.fieldnames] items = list(reader) if not items: continue # Lấy thông tin Partition Key (PK) pk_name = reader.fieldnames[0] table_name = key.split(\u0026#39;.\u0026#39;)[0] # 2. QUAN TRỌNG: Phát hiện kiểu dữ liệu dựa trên dòng đầu tiên first_pk_val = items[0].get(pk_name) pk_type = detect_type(first_pk_val) # Sẽ trả về \u0026#39;N\u0026#39; nếu là số, \u0026#39;S\u0026#39; nếu là chữ # Tạo bảng đúng kiểu (N hoặc S) create_table_if_not_exists(table_name, pk_name, pk_type) table = dynamodb.Table(table_name) count = 0 with table.batch_writer() as batch: for row in items: clean_item = {} is_valid = True for k, v in row.items(): if not k or k.strip() == \u0026#34;\u0026#34;: continue clean_k = k.strip() val = convert_value(v) # Chuyển đổi sang Int/Decimal/Bool if val is None: continue # 3. QUAN TRỌNG: Xử lý Partition Key theo đúng kiểu của Bảng if clean_k == pk_name: if pk_type == \u0026#39;N\u0026#39;: # Nếu bảng là Number, bắt buộc Key phải là Number if not isinstance(val, (int, Decimal)): print(f\u0026#34;SKIPPING ROW: Key \u0026#39;{val}\u0026#39; is not a number but table requires Number.\u0026#34;) is_valid = False break else: # Nếu bảng là String, ép kiểu sang String val = str(val) clean_item[clean_k] = val if is_valid and pk_name in clean_item: batch.put_item(Item=clean_item) count += 1 print(f\u0026#34;Success: Imported {count} items into {table_name} (PK Type: {pk_type})\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;OK\u0026#39;)} except Exception as e: print(f\u0026#34;ERROR: {str(e)}\u0026#34;) import traceback traceback.print_exc() return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} Click Deploy and confirm it shows Successfully. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with the members of the First Cloud Journey. Understand the basic AWS services. Read and clearly understand the rules and regulations of the internship and the First Cloud Journey. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 07/09/2025 08/09/2025 3 - Learn what AWS is and its purpose - Watch introductory videos about AWS on YouTube and AWS Skill Builder. - Get a basic understanding of the First Cloud Journey program. 08/09/2025 09/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Create AWS Free Tier account - Get familiar with the AWS Management Console interface - Practice: + Create an AWS account\nSet up a Virtual MFA Device 09/09/2025 10/09/2025 https://000001.awsstudygroup.com/vi/ 5 - Learn about IAM (Identity and Access Management).: - Practice:\n+ Create an admin group and an admin user. + Set up basic access permissions (policies) for users. + Enable MFA (Multi-Factor Authentication) to enhance security. 10/09/2025 11/09/2025 https://000001.awsstudygroup.com/vi/ 6 - Learn how to set up AWS Budgets to track monthly costs. - Explore AWS Support Cases to understand how to submit support requests - Review and practice the knowledge learned. 11/08/2025 12/08/2025 https://000001.awsstudygroup.com/vi/ Week 1 Achievements: Get acquainted with the members of FCJ, understand the working environment, and the organizational structure at the internship unit.\nRead, remember, and comply with all the rules and regulations of the organization during the internship.\nUnderstand what AWS (Amazon Web Services) is, its purpose, and the benefits of cloud computing in real-world applications.\nGain knowledge of AWS’s basic service categories, including:\nCompute (EC2, Lambda) Storage (S3, EBS) Networking (VPC, Route 53, CloudFront) Database (RDS, DynamoDB) Successfully create an AWS Free Tier account, including:\nVerifying email, phone number, and payment information. Setting up a Virtual MFA Device for the root account to enhance security. Getting familiar with the AWS Management Console interface and learning how to find and access services. Learn and practice IAM (Identity and Access Management):\nCreate a separate Admin group and Admin user. Assign appropriate access policies (Permissions) to users. Enable MFA (Multi-Factor Authentication) for IAM user accounts. Understand how IAM helps manage permissions and protect AWS resources. Set up AWS Budgets to manage and monitor usage costs:\nTạo Cost Budget giới hạn chi tiêu trong Free Tier (ví dụ 5 USD/tháng). Enable email alerts when spending reaches 80% of the limit. Learn about AWS Support Case:\nUnderstand how to access the Support Center and create support cases when issues occur. Differentiate between Support plans: Basic (free), Developer, and Business. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand the concept and role of VPC (Virtual Private Cloud) in AWS infrastructure. Learn how to create and configure a VPC (subnet, route table, internet gateway, security group). Deploy, connect, and manage EC2 instances within the created VPC. Practice managing EC2 instances using both the AWS Management Console and AWS CLI. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Understand what a VPC is and its role in the AWS system. - Learn about the following concepts: + VPC, Subnet, Route Table, Internet Gateway, NAT Gateway. + Differentiate between public subnet and private subnet. 14/09/2025 15/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i/ 3 - Create and configure a VPC. Practice: + Create a new VPC from scratch. + Configure subnet, route table, internet gateway, security group, and NAT gateway. 15/09/2025 16/09/2025 https://000003.awsstudygroup.com/vi/ 4 - Learn about EC2. - Launch and explore different EC2 instance types, AMI, key pair, and security group. - Practice: 1. Go to EC2 Dashboard → Launch Instance. 2. Choose: - AMI: Amazon Linux 2 - Instance type: t2.micro (Free Tier)\n- Network: select the newly created VPC.\n- Subnet: select public subnet.\n- Security group: allow SSH (port 22).\n3.Create or reuse a key pair to SSH into the instance. 4. Launch the EC2 instance and verify its status. 16/09/2025 17/09/2025 https://000004.awsstudygroup.com/vi/ 5 - Connect to the EC2 instance via SSH. Perform some basic EC2 operations using AWS CLI. 17/09/2025 18/09/2025 https://000004.awsstudygroup.com/vi/ 6 - Reinforce knowledge and security practices for VPC and EC2. Learn about Security Groups and Network ACLs. + Practice connecting to a private subnet via SSH. 18/09/2025 19/09/2025 https://000004.awsstudygroup.com/vi/ Week 2 Achievements: Theoretical Understanding\nUnderstand the concept of VPC (Virtual Private Cloud) and its role in isolating and controlling networks within AWS. Understand the main components of a VPC:\nSubnet (Public \u0026amp; Private) Route Table Internet Gateway (IGW) NAT Gateway Security Group và Network ACL Understand how EC2 (Elastic Compute Cloud) works — a service that provides virtual servers in AWS.\nDifferentiate between the basic concepts:\nAMI (Amazon Machine Image) Instance type (t2.micro, t3.small, v.v.) Key Pair Elastic IP Security Group VPC Hands-on Practice\nAccess the VPC Dashboard to view the default VPC configuration of the AWS account. Create a new custom VPC (CIDR: 10.0.0.0/16). Create two subnets: public-subnet (10.0.1.0/24) private-subnet (10.0.2.0/24) Create an Internet Gateway (IGW) and attach it to the VPC. Create a Route Table and add a route to the IGW for the public subnet. Test network connectivity between subnets and to the Internet. EC2 Hands-on Practice\nSelect and learn about: Amazon Linux 2, Instance type: t2.micro (Free Tier). Launch an EC2 Instance in the public-subnet of the newly created VPC. Configure: Security Group: allow SSH (port 22) and HTTP (port 80). Key Pair: create a new key file aws-keypair.pem for SSH connection. Successfully connect to the EC2 instance via SSH using the Public IP. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Gain a deeper understanding of EC2 and S3 services. Learn how to create an EC2 instance for configuring Storage Gateway. Explore and practice CloudFront for global content delivery. Study S3 Cross-Region Replication (CRR) to ensure data availability and redundancy. Improve skills in managing AWS resources via both Console and CLI. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Improve skills in managing AWS resources via both Console and CLI. - Learn about EBS Volume and Snapshot, including how to create, attach, and detach a volume. 21/09/2025 22/09/2025 3 - Learn the fundamentals of Amazon S3 (Simple Storage Service). - Understand key concepts: Bucket, Object, Region, Versioning, Storage Class. - Practice: + Create an S3 bucket. + Upload/download files via AWS Console. + Enable Versioning to track data changes.\n22/09/2025 23/09/2025 https://000057.awsstudygroup.com/vi/ 4 - Learn about AWS Storage Gateway. - Differentiate between the three types: File Gateway, Volume Gateway, Tape Gateway. Tape. - Practice: + Launch an EC2 instance to host the Storage Gateway. + Configure the Storage Gateway in AWS Console. + Connect to S3 for data synchronization. 23/09/2025 24/09/2025 https://000024.awsstudygroup.com/vi/ 5 - Learn about AWS CloudFront. - Understand how Edge Locations and Distributions work.\n- Practice: + Create a CloudFront Distribution using data from an S3 bucket.\n+ Test access speed from multiple regions. 24/09/2025 25/09/2025 https://000057.awsstudygroup.com/vi/ 6 - Learn about S3 Cross-Region Replication (CRR). - Practice: + Create two buckets in different regions (e.g., Singapore and Tokyo). + Enable CRR between the two buckets. + Upload a file and verify automatic replication. 25/09/2025 26/09/2025 https://000057.awsstudygroup.com/vi/ Week 3 Achievements: Clearly understand the relationship between EC2, S3, Storage Gateway, and CloudFront.\nIndependently create and manage EC2 instances for various purposes.\nBecome proficient in creating, uploading, and managing data within S3.\nLearn how to implement Storage Gateway to synchronize data between on-premises systems and AWS.\nSuccessfully set up and configure a CloudFront Distribution to optimize content delivery speed.\nConfigure S3 Cross-Region Replication (CRR) to ensure high data availability across multiple regions.\nStrengthen AWS resource management skills through both the Console and CLI.\nAmazon S3 Practice\nCreate the first S3 bucket: my-first-s3-demo. Upload files (image, text, CSV) and verify public URLs. Configure Public/Private access, enable Versioning to track file changes. Create a Bucket Policy that allows access only from specific AWS accounts. Cross-Region Replication (CRR) Practice\nCreate two buckets: my-bucket-sg (region: Singapore) my-bucket-tokyo (region: Tokyo) Enable Cross-Region Replication between them. Upload a file to my-bucket-sg and confirm it automatically replicates to my-bucket-tokyo. Understand the multi-region replication process, which enhances reliability and data availability. EC2 for Storage Gateway Practice\nLaunch an EC2 Instance (Amazon Linux 2 – t2.micro) to install Storage Gateway. Configuration steps: Attach an Elastic IP for stable access. Open required ports: 80, 443, 3260. Install Storage Gateway Appliance on EC2. Connect the EC2 instance to the AWS Storage Gateway Console, select File Gateway type. Create a File Share linked to an existing S3 bucket. Test read/write operations from the Gateway to S3 to verify synchronization. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand the concept of VM Import/Export and why businesses need this feature. Learn how to import virtual machines (VMs) from on-premises environments to AWS and export them back when necessary. Get familiar with Amazon FSx for Windows File Server, understand its architecture, benefits, and real-world use cases. Practice creating, configuring, and managing an FSx File System. Enhance skills in using both AWS CLI and AWS Management Console to work with storage-related services. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Learn about the concept of VM Import/Export in AWS. - Understand the roles of S3 and IAM Role in the import/export process. - Understand the purposes:: + Migrate virtual machines from on-premises to EC2. + Use for backup or disaster recovery between local and cloud environments. 28/09/2025 29/09/2025 https://000014.awsstudygroup.com/ 3 - Prepare the environment for VM Import/Export. - Practice: + Create an S3 bucket to store virtual machine files. + Configure IAM Role (vmimport) with necessary permissions. + Install AWS CLI and verify account credentials. 29/09/2025 30/09/2025 https://000014.awsstudygroup.com/ 4 - Practice importing a virtual machine to EC2. - After finish, launch an instance from the created AMI. - Practice exporting the EC2 instance back to a local VM format. 30/09/2025 01/10/2025 https://000014.awsstudygroup.com/ 5 - Learn about Amazon FSx for Windows File Server. - Understand Multi-AZ deployment. - Distinguish between the two types of file systems: + SSD Multi-AZ: High performance, ideal for fast-access workloads. + HDD Multi-AZ: Cost-effective, suitable for backups or infrequently accessed data. - Explore advanced FSx features. 01/10/2025 02/10/2025 https://000025.awsstudygroup.com/ 6 - Hands-on practice with FSx management: + Create and configure File Systems (SSD \u0026amp; HDD). + Manage and scale FSx as needed. + Monitor performance of the system. năng 02/10/2025 03/10/2025 https://000025.awsstudygroup.com/ Week 4 Achievements: Clearly understand the VM Import/Export process and how to migrate virtual machines between on-premises and AWS Cloud. Have a strong understanding of Amazon FSx for Windows File Server, including its architecture, storage types, and Multi-AZ features. Successfully practiced creating and managing SSD Multi-AZ and HDD Multi-AZ File Systems. Learned how to create new file shares, enable deduplication and shadow copies, and set up user quotas. Monitored and analyzed system performance using CloudWatch metrics and FSx performance reports. Efficiently managed user sessions, enabled Continuous Access share, and performed throughput and storage scaling as needed. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand AWS Security Hub and how it centralizes security findings. Learn how to enable, configure, and interpret Security Hub security standards (CIS, PCI-DSS, Foundational Best Practices). Understand AWS Key Management Service (KMS), its encryption mechanisms, key policies, CMKs, and use cases. Practice creating, managing, and rotating encryption keys. Learn AWS Identity Center (Successor to AWS SSO), including permission sets, identity sources, and user/group access management. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Learn about AWS Security Hub: + Understand what Security Hub does and how it aggregates security findings. +Understand the concept of Insights, Findings, and Integrations 05/10/2025 06/10/2025 https://000018.awsstudygroup.com/2-enable-sec-hub/ 3 - Practice hands-on with Security Hub: + Enable CIS standard \u0026amp; interpret common failed controls. + View and analyze Findings. + Compare Security Hub vs GuardDuty vs Inspector 06/10/2025 07/10/2025 https://000018.awsstudygroup.com/2-enable-sec-hub/ 4 - Learn about AWS Key Management Service (KMS): + Understand CMK, symmetric vs asymmetric keys. + Grant, Key Policy, IAM Permission interaction. + Envelope Encryption \u0026amp; how AWS Services integrate with KMS. 07/10/2025 08/10/2025 https://000033.awsstudygroup.com/ 5 - Hands-on with KMS operations: + Encrypt/Decrypt data locally using AWS CLI. + Use KMS with S3, EBS, RDS, Lambda environment variables. + Enable Key Rotation. + Test disabling \u0026amp; scheduling key deletion. 08/10/2025 09/10/2025 https://000033.awsstudygroup.com/ 6 - Learn \u0026amp; practice AWS Identity Center (AWS SSO): + Understand identity sources + Learn Permission Sets + Assign users/groups to AWS accounts with Permission Sets. +Test login flow and switching roles. 09/10/2025 10/10/2025 https://000012.awsstudygroup.com/vi/ Week 5 Achievements: Gained a solid understanding of AWS Security Hub’s architecture, security standards, and how to interpret aggregated findings. Successfully enabled and configured Security Hub Understood the roles of KMS in encrypting data across AWS services Gained skills in AWS Identity Center: managing users, groups, permission sets, and cross-account access. Improved overall knowledge about AWS security best practices. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn the basics of Amazon RDS, supported engines, backups, Multi-AZ, and read replicas. Understand Amazon Aurora, its architecture, benefits, and differences from RDS. Learn the core concepts of Amazon Redshift for data warehousing. Understand Amazon ElastiCache (Redis/Memcached) and why caching improves performance. Practice simple hands-on tasks with each service. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Learn the concept of Amazon RDS. - Understand DB engines (MySQL, PostgreSQL, MariaDB). - Learn backups, Multi-AZ, and read replicas. 12/10/2025 13/10/2025 3 - Hands-on with RDS: + Create a DB instance + Connect via client(DBeaver) + Test snapshot backup \u0026amp; restore. 13/10/2025 14/10/2025 https://000005.awsstudygroup.com/ 4 - Learn about Amazon Aurora: + Aurora architecture vs RDS. + High availability \u0026amp; scaling. + Writer/Reader endpoints. 14/10/2025 15/10/2025 https://000005.awsstudygroup.com/ 5 - Learn about Amazon Redshift: + Data warehouse concept. + Clusters, nodes, and basic SQL usage. + Use cases for analytics. 15/10/2025 16/10/2025 https://000005.awsstudygroup.com/ 6 - Understand ElastiCache + What caching is and why it improves performance. + Differences between Redis vs Memcached. + Create a simple Redis cluster. volume 16/10/2025 17/10/2025 https://000005.awsstudygroup.com/ Week 6 Achievements: Understood the basics of RDS and created a working database instance. Learned Aurora’s high-availability design and scaling model. Gained foundation knowledge of Redshift for analytics and data warehousing. Understood caching concepts and practiced creating an ElastiCache Redis cluster. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand what Amazon Lightsail is and when to use it. Learn how to deploy simple applications (WordPress, Web server, container) on Lightsail. Understand basic Lightsail components: Instances, Networking, Snapshots, Databases. Learn the fundamentals of Amazon CloudWatch for monitoring AWS services. Practice creating alarms, dashboards, and logs monitoring. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Learn the concept of Amazon Lightsail. - Compare Lightsail vs EC2. -Explore Lightsail instances, pricing, and blueprints. 19/10/2025 20/10/2025 https://000045.awsstudygroup.com/ 3 - Hands-on with Lightsail: + Create a Lightsail instance + Connect via browser SSH. + Explore networking 20/10/2025 21/10/2025 https://000045.awsstudygroup.com/ 4 - Learn Amazon CloudWatch basics: + Metrics, Logs, Events, Alarms. + Understand how CloudWatch collects data. + CloudWatch namespaces 21/10/2025 22/10/2025 https://000008.awsstudygroup.com/ 5 - Hands-on with CloudWatch: + Create CloudWatch Alarms for EC2 CPU usage. + Create a Log Group and test Log Insights queries. + Build a simple CloudWatch Dashboard. 22/10/2025 23/10/2025 https://000008.awsstudygroup.com/ 6 - Explore CloudWatch + Lightsail integration. + Enable Lightsail metrics in CloudWatch. + Monitor instance CPU, network, and storage. 23/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Understood the purpose and use cases of Lightsail. Successfully created and managed a Lightsail instance. Learned CloudWatch concepts: metrics, logs, alarms, dashboards. Built alarms and dashboards for monitoring usage. Integrated Lightsail metrics with CloudWatch. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Understand the basic concept of a Data Lake on AWS. Learn how Amazon S3 is used as the storage foundation for Data Lakes. Explore AWS tools commonly used with Data Lakes: Glue, Lake Formation, Athena. Learn the fundamentals of Amazon DynamoDB and NoSQL database concepts. Practice simple tasks with S3, Athena, and DynamoDB. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Learn what a Data Lake is and why businesses use it. - Understand S3 as the main Data Lake storage layer. -Explore Data Lake architecture 26/10/2025 27/10/2025 https://000035.awsstudygroup.com/ 3 - Hands-on Data Lake : + Create S3 buckets to simulate raw \u0026amp; processed layers. + Upload sample CSV/JSON files. + Query data using Amazon Athena. 27/10/2025 28/10/2025 https://000035.awsstudygroup.com/ 4 - Learn AWS Glue \u0026amp; Lake Formation + Glue Data Catalog basics. + Crawlers to auto-detect schema. + Simple ETL overview. 28/10/2025 29/10/2025 https://000035.awsstudygroup.com/ 5 - Learn Amazon DynamoDB concepts: + NoSQL vs SQL. + Partition key, Sort key. + Read/Write Capacity Modes 29/10/2025 30/10/2025 https://000039.awsstudygroup.com/ 6 - Hands-on with DynamoDB: + Create a table with partition key + Insert, update, delete items. + Query \u0026amp; Scan operations. 30/10/2025 31/10/2025 https://000039.awsstudygroup.com/ Week 8 Achievements: Understood Data Lake fundamentals and S3’s role in Data Lakes. Practiced querying S3 data with Athena. Learned the basics of Glue and Lake Formation for metadata and ETL. Gained a solid foundation in DynamoDB concepts and operations. Successfully created and interacted with a DynamoDB table. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn the fundamentals of AWS Bedrock and generative AI on AWS. Understand different Foundation Models (Claude, Llama, Titan, Mistral). Learn how to call Bedrock using the AWS Console \u0026amp; SDK. Understand basic concepts of embeddings \u0026amp; RAG workflow. Learn the basics of AWS Lambda and how to trigger functions. Build a simple serverless combining Lambda + Bedrock for proposal Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Introduction to AWS Bedrock. - Explore available Foundation Models. - Try generating text using Bedrock Console. 02/11/2025 03/11/2025 3 - Practice calling Bedrock via AWS SDK (Python or Node): + Simple text-generation API call. + Understand pricing + Set up IAM Role for Bedrock access. 03/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn embeddings \u0026amp; RAG basics: + What is embedding? + Generate Titan Embeddings. + Build a small RAG pipeline 04/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Introduction to AWS Lambda: + What is serverless? + Create first Lambda function. + Test using “Invoke” on Console. 05/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on with Lambda: + Lambda triggers: API Gateway, S3, EventBridge. + Deploy new Lambda versions. + Intro to Lambda VPC networking. 06/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood AWS Bedrock fundamentals and tried multiple foundation models. Practiced calling Bedrock through SDK with IAM permissions. Learned embedding concepts and built a mini RAG flow. Built and tested Lambda functions with different triggers. Gained confidence combining Lambda + Bedrock for basic serverless AI workloads. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Practice and get familiar with VPC and EC2\nWeek 3: Learn about S3 and EC2\nWeek 4: Learn about VM Import/Export and get familiar with Amazon FSx service\nWeek 5: Explore Security Hub and AWS Key Management Service (KMS)\nWeek 6: Learn about RDS and database types\nWeek 7: Explore Amazon Lightsail and Amazon CloudWatch\nWeek 8: Learn the Basics of Data Lake on AWS\nWeek 9: Explore AWS Bedrock and GenAI on AWS\nWeek 10: Learn about serverless chatbot architecture on AWS and PostgreSQL\nWeek 11: Practice making chatbot project\nWeek 12: Prepare proposal for chatbot project and synthesize architecture\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/2-backend/","title":"Backend Workshop (API &amp; Data Layer)","tags":[],"description":"","content":"Backend: API \u0026amp; Data Pipeline In this workshop, you will:\nUse Amazon S3 to store input data (CSV files). Configure AWS Lambda Trigger to automatically import data into DynamoDB. Create Lambda (API Handler) and expose it through API Gateway to access DynamoDB data. Test REST API endpoints via Postman or API Gateway Console. Clean up all resources after completion. "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/3-ai/3.2/","title":"Configure RDS and Connect with DBeaver","tags":[],"description":"","content":"Configure RDS and Connect with DBeaver Implementation Steps 1. Access RDS Service Go to AWS Management Console → search for RDS. Before creating the RDS instance, we will create a subnet group. Name the subnet group and select the VPC you created. For AZ select ap-southeast-1a and ap-southeast-1b. For Subnets, select the 2 private subnets, then click Create. 2. Create RDS Go to databases → Create database. In the database configuration section, select Full configuration, select PostgreSQL. In Templates select Sandbox. In Settings, name the DB and set a password. Configure the remaining parts as follows: In the Connectivity section, select the VPC created and the DB subnet group. Set Public access to No. In the VPC security group section, select the Security group created for RDS. Keep the rest as default and click Create. 1. Store Data and Connect to PostgreSQL using DBeaver You can download DBeaver here: https://dbeaver.io/ Download the knowledge_base file from here.\nTo connect from the local machine to DBeaver, we need to create an EC2 instance to act as a bridge.\nAccess EC2 → Launch instance.\nName the EC2, select Instance type t2.micro, create a key pair, and save it to your machine. In Network settings, select the VPC created, select a public subnet, and create a Security group. In Inbound Security Group Rules, select \u0026lsquo;My IP\u0026rsquo;, then Launch instance. Next, go to the RDS Security group section, edit the inbound rules, and add a new rule. Type: PostgreSQL and Source: the newly created EC2. Open DBeaver and click on the connection section. Select PostgreSQL. In the Host section, copy the RDS Endpoint. Fill in the information you used when creating the RDS. Click on the SSH tab. In Host/IP, copy the Public IP of the EC2. For User Name, enter ec2-user. In Authentication Method, select Public Key and choose the key pair created with the EC2. Then click Test connection and Finish. After successfully connecting, open an SQL script and paste this code to create the knowledge_base table. After creating the table, refresh the database to show the knowledge_base table.\n-- 1. Enable vector extension (run only once) CREATE EXTENSION IF NOT EXISTS vector; -- 2. Create knowledge base table (Knowledge Base) CREATE TABLE knowledge_base ( id bigserial PRIMARY KEY, content text, -- Original text content (chunked text) metadata jsonb, -- Store extra info: image link, filename, page number... embedding vector(1024) -- IMPORTANT: Must be 1024 for Cohere Multilingual ); -- 3. Create index for faster search CREATE INDEX ON knowledge_base USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64); To import data into DBeaver using Python, we need to SSH via CMD. Open CMD in the folder containing the keypair and copy this command:\nssh -i \u0026#34;my-key.pem\u0026#34; -L 5433:RDS endpoint port:5432 ec2-user@public IP EC2 -N Then, run this Python script to import data into DBeaver.\nimport pandas as pd import json import boto3 import psycopg2 import time import glob import os import numpy as np from dotenv import load_dotenv # ========================================== # 1. CONFIGURATION \u0026amp; SECURITY # ========================================== current_dir = os.path.dirname(os.path.abspath(__file__)) env_path = os.path.join(current_dir, \u0026#39;pass.env\u0026#39;) load_dotenv(env_path) CSV_FOLDER = \u0026#39;./database\u0026#39; DB_HOST = os.getenv(\u0026#34;DB_HOST\u0026#34;) DB_NAME = os.getenv(\u0026#34;DB_NAME\u0026#34;) DB_USER = os.getenv(\u0026#34;DB_USER\u0026#34;) DB_PASS = os.getenv(\u0026#34;DB_PASS\u0026#34;) # AWS Connection bedrock = boto3.client( service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;, aws_access_key_id=os.getenv(\u0026#34;aws_access_key_id\u0026#34;), aws_secret_access_key=os.getenv(\u0026#34;aws_secret_access_key\u0026#34;) ) # ========================================== # 2. TRANSLATION DICTIONARY (MOST IMPORTANT) # ========================================== # A. Translate Column Names (For AI context) COLUMN_MAP = { \u0026#34;price\u0026#34;: \u0026#34;Price\u0026#34;, \u0026#34;gia\u0026#34;: \u0026#34;Price\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;Cost\u0026#34;, \u0026#34;fee\u0026#34;: \u0026#34;Fee\u0026#34;, \u0026#34;stock\u0026#34;: \u0026#34;Stock\u0026#34;, \u0026#34;so_luong\u0026#34;: \u0026#34;Stock\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Description\u0026#34;, \u0026#34;mo_ta\u0026#34;: \u0026#34;Description\u0026#34;, \u0026#34;chi_tiet\u0026#34;: \u0026#34;Details\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;Origin\u0026#34;, \u0026#34;xuat_xu\u0026#34;: \u0026#34;Origin\u0026#34;, \u0026#34;material\u0026#34;: \u0026#34;Material\u0026#34;, \u0026#34;chat_lieu\u0026#34;: \u0026#34;Material\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;Color\u0026#34;, \u0026#34;mau_sac\u0026#34;: \u0026#34;Color\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;Weight\u0026#34;, \u0026#34;trong_luong\u0026#34;: \u0026#34;Weight\u0026#34;, \u0026#34;food_type\u0026#34;: \u0026#34;Food Type\u0026#34;, \u0026#34;usage_target\u0026#34;: \u0026#34;Suitable for bird species\u0026#34;, \u0026#34;furniture_type\u0026#34;: \u0026#34;Furniture Type\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;Processing Time\u0026#34;, \u0026#34;thoi_gian\u0026#34;: \u0026#34;Processing Time\u0026#34;, \u0026#34;method_name\u0026#34;: \u0026#34;Method Name\u0026#34; } # B. Translate Values (For Website display) \u0026lt;--- PART YOU NEED VALUE_TRANSLATIONS = { \u0026#34;FOODS\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;Foods\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;foods\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;TOYS\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;Toys\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;toys\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;FURNITURE\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;Furniture\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;furniture\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;Bird\u0026#34;: \u0026#34;Pet Bird\u0026#34;, \u0026#34;bird\u0026#34;: \u0026#34;Pet Bird\u0026#34; } # --- AUXILIARY FUNCTIONS --- def get_embedding(text): try: if not text or len(str(text)) \u0026lt; 5: return None body = json.dumps({\u0026#34;texts\u0026#34;: [str(text)], \u0026#34;input_type\u0026#34;: \u0026#34;search_document\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34;}) response = bedrock.invoke_model(body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34;) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except: return None def clean(val): if pd.isna(val) or str(val).lower() in [\u0026#39;nan\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;null\u0026#39;]: return \u0026#34;\u0026#34; val_str = str(val).strip() # --- AUTO TRANSLATE HERE --- # If the value exists in the translation dictionary, replace it immediately if val_str in VALUE_TRANSLATIONS: return VALUE_TRANSLATIONS[val_str] return val_str def main(): try: conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS, port=5433) # Note the SSH port 5433 cur = conn.cursor() print(\u0026#34;✅ Database connection successful!\u0026#34;) except Exception as e: print(f\u0026#34;❌ DB Connection Error: {e}\u0026#34;); return csv_files = glob.glob(os.path.join(CSV_FOLDER, \u0026#34;*.csv\u0026#34;)) print(f\u0026#34;📂 Found {len(csv_files)} CSV files.\u0026#34;) # Statistics variables stats = {\u0026#34;bird\u0026#34;: 0, \u0026#34;food\u0026#34;: 0, \u0026#34;toy\u0026#34;: 0, \u0026#34;furniture\u0026#34;: 0, \u0026#34;best_sellers\u0026#34;: []} total_success = 0 for file_path in csv_files: filename = os.path.basename(file_path).lower() print(f\u0026#34;\\n--- Processing file: {filename} ---\u0026#34;) try: df = pd.read_csv(file_path) df = df.replace({np.nan: None}) # Auto-detect category (Prefix) category_prefix = \u0026#34;Product\u0026#34; if \u0026#34;bird\u0026#34; in filename: category_prefix = \u0026#34;Bird Species\u0026#34; elif \u0026#34;food\u0026#34; in filename: category_prefix = \u0026#34;Bird Food\u0026#34; elif \u0026#34;toy\u0026#34; in filename or \u0026#34;do_choi\u0026#34; in filename: category_prefix = \u0026#34;Bird Toy\u0026#34; elif \u0026#34;furniture\u0026#34; in filename: category_prefix = \u0026#34;Cage Furniture\u0026#34; elif \u0026#34;ship\u0026#34; in filename or \u0026#34;delivery\u0026#34; in filename: category_prefix = \u0026#34;Shipping Method\u0026#34; elif \u0026#34;payment\u0026#34; in filename: category_prefix = \u0026#34;Payment Method\u0026#34; for index, row in df.iterrows(): # Statistics if \u0026#34;bird\u0026#34; in filename: stats[\u0026#34;bird\u0026#34;] += 1 elif \u0026#34;food\u0026#34; in filename: stats[\u0026#34;food\u0026#34;] += 1 elif \u0026#34;toy\u0026#34; in filename: stats[\u0026#34;toy\u0026#34;] += 1 elif \u0026#34;furniture\u0026#34; in filename: stats[\u0026#34;furniture\u0026#34;] += 1 # A. IDENTITY p_id = clean(row.get(\u0026#39;id\u0026#39;) or row.get(\u0026#39;product_id\u0026#39;) or row.get(\u0026#39;payment_id\u0026#39;)) name = clean(row.get(\u0026#39;name\u0026#39;) or row.get(\u0026#39;product_name\u0026#39;) or row.get(\u0026#39;title\u0026#39;) or row.get(\u0026#39;method_name\u0026#39;)) if not name: if p_id: name = f\u0026#34;Code {p_id}\u0026#34; else: continue # B. AUTO SCAN COLUMNS AND TRANSLATE content_parts = [f\u0026#34;{category_prefix}: {name}\u0026#34;] # Scan all columns, if exists in COLUMN_MAP then add for col_key, col_val in row.items(): val_clean = clean(col_val) # clean function will auto translate FOODS -\u0026gt; Food if val_clean and col_key in COLUMN_MAP: content_parts.append(f\u0026#34;{COLUMN_MAP[col_key]}: {val_clean}\u0026#34;) # C. HANDLE PRICE \u0026amp; STOCK \u0026amp; BEST SELLERS SEPARATELY price = clean(row.get(\u0026#39;price\u0026#39;) or row.get(\u0026#39;gia\u0026#39;) or row.get(\u0026#39;fee\u0026#39;)) if price: content_parts.append(f\u0026#34;Price: {price}\u0026#34;) stock = clean(row.get(\u0026#39;stock\u0026#39;) or row.get(\u0026#39;so_luong\u0026#39;)) if stock: content_parts.append(f\u0026#34;Stock: {stock}\u0026#34;) sold = clean(row.get(\u0026#39;sold\u0026#39;) or row.get(\u0026#39;da_ban\u0026#39;)) if sold: content_parts.append(f\u0026#34;Sold: {sold}\u0026#34;) try: if float(sold) \u0026gt; 0: stats[\u0026#34;best_sellers\u0026#34;].append((float(sold), name, category_prefix)) except: pass content_to_embed = \u0026#34;. \u0026#34;.join(content_parts) + \u0026#34;.\u0026#34; # D. CREATE METADATA (Also use translated values) # Note: The clean() function above already translated, so we call clean() again for each field metadata = {} for k, v in row.items(): metadata[k] = clean(v) # Save to metadata in English # Overwrite standard fields metadata[\u0026#39;id\u0026#39;] = p_id metadata[\u0026#39;name\u0026#39;] = name metadata[\u0026#39;price\u0026#39;] = price if price else \u0026#34;Contact\u0026#34; metadata[\u0026#39;type\u0026#39;] = category_prefix metadata[\u0026#39;image\u0026#39;] = clean(row.get(\u0026#39;image_url\u0026#39;) or row.get(\u0026#39;link_anh\u0026#39;)) metadata[\u0026#39;sold\u0026#39;] = sold # E. INSERT vector = get_embedding(content_to_embed) if vector: cur.execute( \u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (content_to_embed, json.dumps(vector), json.dumps(metadata, default=str)) ) total_success += 1 if total_success % 10 == 0: print(f\u0026#34; -\u0026gt; Loaded {total_success} rows...\u0026#34;) conn.commit() time.sleep(0.1) except Exception as e: print(f\u0026#34;⚠️ Error processing file {filename}: {e}\u0026#34;); continue # --- CREATE STATISTICS REPORT --- print(\u0026#34;\\n--- Creating statistics report... ---\u0026#34;) top_products = sorted(stats[\u0026#34;best_sellers\u0026#34;], key=lambda x: x[0], reverse=True)[:5] top_names = \u0026#34;, \u0026#34;.join([f\u0026#34;{p[1]} ({int(p[0])} purchases)\u0026#34; for p in top_products]) summary_content = ( f\u0026#34;BIRD SHOP STATISTICS REPORT: \u0026#34; f\u0026#34;Total birds: {stats[\u0026#39;bird\u0026#39;]}. Food: {stats[\u0026#39;food\u0026#39;]}. \u0026#34; f\u0026#34;Toys: {stats[\u0026#39;toy\u0026#39;]}. Furniture: {stats[\u0026#39;furniture\u0026#39;]}. \u0026#34; f\u0026#34;TOP 5 BEST SELLING PRODUCTS: {top_names}.\u0026#34; ) summary_vector = get_embedding(summary_content) if summary_vector: cur.execute(\u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (summary_content, json.dumps(summary_vector), json.dumps({\u0026#34;id\u0026#34;:\u0026#34;STATS\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Statistics\u0026#34;}, default=str))) conn.commit() cur.close(); conn.close() print(f\u0026#34;\\n🎉 COMPLETED! Total imported: {total_success + 1} rows.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() After importing, refresh the knowledge_base table to see the results.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/4-frontend/4.2/","title":"Distribute with CloudFront ","tags":[],"description":"","content":" Create a CloudFront Distribution pointing to the S3 bucket Go to the Cloudfront service. Create Cloudfront Distribution Enter a Cloudfront name (for example: flyora-shop). Select your S3 you have created before Turn on Website endpoint make sure you get a url like this http://Your-S3-name.s3-website.ap-southeast-1.amazonaws.com/ Configure: Viewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP method: GET, HEAD , OPTION Cache policy: CatchingOptimized Response headers policy: CORS-with-preflight-and-SecurityHeadersPolicy Choose do not enable securiy protections Review and click Create Distribution After Create Distribution you have to wait for 5 or 10 minutes to deploy and if it deploy successfully it will show you date you have deploy "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/2-backend/2.2/","title":"Verify Data in DynamoDB","tags":[],"description":"","content":"Verify Data in DynamoDB In this step, you will verify that the data from the CSV file has been successfully imported into DynamoDB.\nSteps to Perform Upload the CSV File to the Bucket Download the sample CSV file from here.\nIn the newly created Bucket:\nGo to the Objects tab → click Upload. Extract file zip. Drag and drop the files, then click Upload. [!TIP] After uploading, please wait about 3–5 minutes for the Lambda function to import the data.\nAccess the DynamoDB Service Go to AWS Management Console → search for DynamoDB. Select Tables → click on the products table. View the Data Open the Explore items tab. Check the list of products that have been imported. If you don\u0026rsquo;t see any data, please check the following:\nThe DynamoDB table name must match the CSV file name. The CSV file must have a valid header. The Lambda function must have sufficient permissions to access both S3 and DynamoDB. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Understand the architecture of a serverless chatbot. Learn how embedding models (Cohere Embed v3) work. Understand vector search logic and how RDS PostgreSQL stores embeddings. Learn Claude Haiku 3 for LLM generation. Prepare AWS infrastructure: Lambda, VPC, NAT Gateway. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Learn chatbot architecture - Understand the RAG workflow 09/11/2025 10/11/2025 3 - Study Cohere Embedding Model (Embed v3): + How embedding vectors work + Chunking content + Cosine similarity overview 10/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Hands-on with embeddings: + Generate embeddings from sample text + Store vectors inside RDS PostgreSQL table + Test vector similarity query 11/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Claude 3 Haiku basics: + Prompting + System message + Temperature \u0026amp; max tokens 12/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Set up AWS environment: + Create Lambda inside VPC + NAT Gateway for outbound internet + Connect Lambda to RDS PostgreSQL 13/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood chatbot and RAG architecture. Learned Cohere embeddings and vector similarity. Successfully stored and queried embeddings in PostgreSQL. Learned how Claude Haiku 3 handles generation. Deployed Lambda + NAT Gateway + VPC environment. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Build Chatbot backend logic inside Lambda. Integrate embeddings + vector search + LLM response. Publish chatbot endpoint using API Gateway. Test entire pipeline end-to-end. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Build Lambda function: + Load embedding from Cohere + Query PostgreSQL for top matches 16/11/2025 17/11/2025 3 - Integrate Claude Haiku 3: + Use search results to generate final answer + Add safety prompts + response formatting 17/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Build API Gateway endpoint: + Create REST API + Connect to Lambda + Add basic authentication 18/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - End-to-end testing: + Send user query → Lambda → RDS → Cohere → Claude + Log responses + Fix latency issues 19/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Optimization: + Add caching + Improve prompt quality + Clean database structure 20/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Built full Lambda backend. Completed embedding → retrieval → generation pipeline. Successfully deployed API Gateway endpoint. Full chatbot running end-to-end with Claude + Cohere + RDS. Improved latency \u0026amp; prompt engineering. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Prepare project for proposal Summarize architecture: Embedding (Cohere) + LLM (Claude Haiku 3) + RDS PostgreSQL + Lambda + API Gateway + NAT Gateway. Define project scope, objectives, and milestones. Identify technical challenges and solutions. Plan next steps for testing and production deployment. Tasks to be carried out this week: Day Task Start .Date Completion .Date Reference Material 2 - Review all implemented components: + Cohere embedding pipeline + Claude Haiku 3 integration + Lambda functions +RDS PostgreSQL vector storage 23/11/2025 24/11/2025 3 - Document architecture diagram: + API Gateway → Lambda → RDS → Cohere → Claude → Lambda → Response + Include NAT Gateway \u0026amp; VPC design 24/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Define proposal content: + Project objectives \u0026amp; scope + Technology stack + Workflow \u0026amp; RAG logic 25/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Identify potential challenges: + Latency + Embedding storage \u0026amp; query optimization + LLM prompt design \u0026amp; safety 26/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Finalize proposal draft: + Include architecture diagram, component description, milestones, and technical considerations + Prepare for review \u0026amp; feedback 27/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Completed a full proposal draft for chatbot project. Documented architecture and workflow clearly. Identified technical challenges and proposed solutions. Defined milestones and next steps for deployment and testing. "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/3-ai/","title":"AI Workshop (Chatbot)","tags":[],"description":"","content":"This workshop provides a detailed guide on how to build a Product Consultation Chatbot using RAG (Retrieval-Augmented Generation) architecture on the AWS platform.\n1. System Architecture The system utilizes the following AWS services:\nAmazon Bedrock: Provides AI models (LLMs). Generation Model: Amazon Nova Lite (anthropic.claude-3-haiku-20240307-v1:0) for answering questions. Embedding Model: Cohere Embed Multilingual (cohere.embed-multilingual-v3) for data vectorization. Amazon RDS (PostgreSQL): Stores product data and vectors (using the Dbeaver extension). AWS Lambda: Intermediate logic processing function (Serverless backend). "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/4-frontend/4.3/","title":"API Integration","tags":[],"description":"","content":"Step 1: Get Your API Gateway URL First of all you have to receive API GateWay url https://uwbxj9wfq6.execute-api.ap-southeast-1.amazonaws.com/dev Open your project folder in VS Code. Create a new file named api.js inside your src directory. Add the following JavaScript code: Step 2: Test the API in Postman Take the data from postman You should receive a status 200 response with JSON data Step 3: Deploy and Verify on S3 Visit your http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com Using data from Postman with https://uwbxj9wfq6.execute-api.ap-southeast-1.amazonaws.com/dev If you login and it show a notification like this the API Integration was successfully "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/2-backend/2.3/","title":"Create Lambda Function to Handle DynamoDB Requests","tags":[],"description":"","content":"Objective Create a new Lambda Function to process requests to DynamoDB through API Gateway.\nSteps Download the backend file from here.\nStep 1: Create IAM Role Open IAM Console → Roles → Create role Select\nTrusted entity: AWS Service → Lambda Attach the following permissions:\nAmazonDynamoDBFullAccess CloudWatchLogsFullAccess Set the role name: LambdaAPIAccessRole\nStep 2: Create Lambda Function Go to AWS Lambda → Create function Select Author from scratch Function name: DynamoDB_API_Handler Runtime: Java 17 Choose IAM Role: LambdaAPIAccessRole Step 3: Deploy the JAR File Go to S3 → Upload → Add files\nUpload the jar file, then copy the Object URL Go to AWS Lambda → Upload from S3\nPaste the object URL you copied Go to AWS Lambda → Code → Runtime settings → Edit Set Handler:\norg.example.flyora_backend.handler.StreamLambdaHandler::handleRequest Step 4: Configure Lambda Function Go to AWS Lambda → Configuration → General configuration → Edit Set Timeout: 1 min Go to AWS Lambda → Configuration → Environment variables → Edit Add: Key: APP_JWT_SECRET Value: huntrotflyorateam!@ky5group5member Key: GHN_TOKEN; Value: 445c659d-5586-11f0-8c19-5aba781b9b65 "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/3-ai/3.3/","title":"Create Logic for Lambda Function","tags":[],"description":"","content":"Create Logic for Lambda Function Implementation Steps 1. Access Lambda Service Go to AWS Management Console → search for Lambda. First, go to the Layers section to create a layer so the Lambda function has the library to connect to PostgreSQL (psycopg2).\nAccess the GitHub link to download the appropriate Python version, here we use psycopg2-3.11: https://github.com/jkehler/awslambda-psycopg2\nAfter downloading, put the entire psycopg2-3.11 folder into a folder named (python), then zip that folder and name it (postgres-layer-3.11).\nClick Create layer, name the layer, upload the postgres-layer-3.11 zip file → Create. 2. Create Lambda Functions → Create function. Name the Lambda function and select runtime Python 3.11. In Additional configurations, check VPC, select the created VPC, select 1 private subnet for Subnet, and select Lambda-SG for Security group. After the Lambda function is created, we will go to the layer section to add the created layer. Select Custom layers, choose the created layer, select version 1 → Add. Go to the Configuration tab, in General configuration, increase the timeout to 1 minute. In Permissions, add AmazonBedrockFullAccess and AWSLambdaVPCAccessExecutionRole. In Environment variables, add the following variables: DB_HOST, DB_NAME, DB_USER, DB_PASS (Enter your RDS information). After configuring, paste this Python code into the Lambda function and click Deploy.\nimport json import boto3 import psycopg2 import os # --- CONFIGURATION --- DB_HOST = os.environ.get(\u0026#39;DB_HOST\u0026#39;) DB_NAME = os.environ.get(\u0026#39;DB_NAME\u0026#39;) DB_USER = os.environ.get(\u0026#39;DB_USER\u0026#39;) DB_PASS = os.environ.get(\u0026#39;DB_PASS\u0026#39;) # Use ap-southeast-1 region bedrock = boto3.client(service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # --- 1. EMBEDDING FUNCTION (COHERE) --- def get_embedding(text): try: body = json.dumps({ \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34; }) response = bedrock.invoke_model( body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34; ) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except Exception as e: print(f\u0026#34;Embed Error: {e}\u0026#34;) return None # --- 2. SAFE METADATA HANDLING FUNCTION --- def format_product_info(metadata): \u0026#34;\u0026#34;\u0026#34; This function helps normalize data even if the CSV file is missing columns \u0026#34;\u0026#34;\u0026#34; if not metadata: return None # Get name (Prioritize common keys) name = metadata.get(\u0026#39;name\u0026#39;) or metadata.get(\u0026#39;product_name\u0026#39;) or metadata.get(\u0026#39;title\u0026#39;) or \u0026#34;Unnamed Product\u0026#34; # Get price (If not present, leave empty or \u0026#39;Contact\u0026#39;) price = metadata.get(\u0026#39;price\u0026#39;) or metadata.get(\u0026#39;gia\u0026#39;) or metadata.get(\u0026#39;display_price\u0026#39;) price_str = f\u0026#34;- Price: {price}\u0026#34; if price else \u0026#34;\u0026#34; # Get type (if available from smart import script) category = metadata.get(\u0026#39;type\u0026#39;) or \u0026#34;Product\u0026#34; # Create description string for AI to read # Example: \u0026#34;Bird Species: Parrot. - Price: 500k\u0026#34; ai_context = f\u0026#34;Category/Product: {name} ({category}) {price_str}\u0026#34; # Create object for Frontend display (Product Card) frontend_card = { \u0026#34;id\u0026#34;: metadata.get(\u0026#39;id\u0026#39;) or metadata.get(\u0026#39;product_id\u0026#39;), \u0026#34;name\u0026#34;: name, \u0026#34;price\u0026#34;: price if price else \u0026#34;Contact\u0026#34;, # Frontend will show \u0026#34;Contact\u0026#34; if no price \u0026#34;image\u0026#34;: metadata.get(\u0026#39;image_url\u0026#39;) or metadata.get(\u0026#39;link_anh\u0026#39;) or \u0026#34;\u0026#34;, # Image can be empty \u0026#34;type\u0026#34;: category # To let frontend know if this is a bird or a toy } return ai_context, frontend_card # --- 3. MAIN HANDLER --- def lambda_handler(event, context): print(\u0026#34;Event:\u0026#34;, event) try: # Parse Input if \u0026#39;body\u0026#39; in event: try: body_data = json.loads(event[\u0026#39;body\u0026#39;]) if isinstance(event[\u0026#39;body\u0026#39;], str) else event[\u0026#39;body\u0026#39;] except: body_data = {} else: body_data = event user_question = body_data.get(\u0026#39;question\u0026#39;, \u0026#39;\u0026#39;) if not user_question: return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Missing question\u0026#39;)} # A. Create Vector q_vector = get_embedding(user_question) if not q_vector: return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Vector creation error\u0026#39;)} # B. Search DB conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS) cur = conn.cursor() # Retrieve metadata for processing sql = \u0026#34;\u0026#34;\u0026#34; SELECT content, metadata FROM knowledge_base ORDER BY embedding \u0026lt;=\u0026gt; %s LIMIT 3 \u0026#34;\u0026#34;\u0026#34; cur.execute(sql, (json.dumps(q_vector),)) results = cur.fetchall() cur.close(); conn.close() # C. Process Results (MOST IMPORTANT) ai_contexts = [] frontend_products = [] for row in results: raw_content = row[0] # Original content at import raw_metadata = row[1] # JSON metadata # Call normalization function ai_text, card_data = format_product_info(raw_metadata) if ai_text: # Merge original content + identification content to be sure ai_contexts.append(f\u0026#34;{ai_text}. Details: {raw_content}\u0026#34;) frontend_products.append(card_data) # If nothing is found if not ai_contexts: final_answer = \u0026#34;Sorry, the shop currently cannot find matching information in the database.\u0026#34; else: # D. Send to LLM (Claude 3 Haiku) context_str = \u0026#34;\\n---\\n\u0026#34;.join(ai_contexts) system_prompt = ( \u0026#34;You are a consultant for the Bird Shop, named \u0026#39;Smart Parrot\u0026#39;. \u0026#34; \u0026#34;Style: Friendly, Polite but Concise.\\n\\n\u0026#34; \u0026#34;HANDLING INSTRUCTIONS (PRIORITIZE IN ORDER):\\n\u0026#34; \u0026#34;1. SOCIAL INTERACTION: If the customer just says hello (Hello, Hi...) or thanks: \u0026#34; \u0026#34;-\u0026gt; Greeting back warmly and ask what they are looking for. DO NOT list products unless asked.\\n\u0026#34; \u0026#34;2. PRODUCT CONSULTATION: When customer asks about goods:\\n\u0026#34; \u0026#34;-\u0026gt; Answer concisely: Product Name + Price + Stock status (if any).\\n\u0026#34; \u0026#34;-\u0026gt; Do not describe in flowery detail unless asked specifically \u0026#39;how is it\u0026#39;.\\n\u0026#34; \u0026#34;-\u0026gt; If information is not in Context, say \u0026#39;Sorry, the shop doesn\u0026#39;t have this item yet\u0026#39;.\\n\u0026#34; ) user_msg = f\u0026#34;Reference Information:\\n{context_str}\\n\\nQuestion: {user_question}\u0026#34; # Call Claude 3 Haiku claude_body = json.dumps({ \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: 300, \u0026#34;temperature\u0026#34;: 0.1, \u0026#34;system\u0026#34;: system_prompt, \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_msg}] }) try: model_res = bedrock.invoke_model( body=claude_body, modelId=\u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34; ) res_json = json.loads(model_res[\u0026#39;body\u0026#39;].read()) final_answer = res_json[\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;] except Exception as e: print(f\u0026#34;LLM Call Error: {e}\u0026#34;) final_answer = \u0026#34;Sorry, the AI system is busy, please try again later.\u0026#34; # E. Return Result return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;POST\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#34;answer\u0026#34;: final_answer, \u0026#34;products\u0026#34;: frontend_products # Frontend uses this to draw UI }) } except Exception as e: print(f\u0026#34;System Error: {e}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} 3. Integrate into Team\u0026rsquo;s API Gateway Access API Gateway Service. Select the API created by the Backend. Select Create resource. Resource name chatbot and check CORS. Select the chatbot resource and click Create method. Method type select POST, check Lambda proxy integration. Select the created Lambda Function (or VPC based on original context), then click Create method. After configuration is complete, click Deploy API. "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/2-backend/2.4/","title":"Create API Gateway and Integrate with Lambda","tags":[],"description":"","content":"Objective Connect AWS API Gateway with a Lambda Function to create a RESTful endpoint that allows accessing data stored in DynamoDB.\nImplementation Steps 1. Access API Gateway Go to AWS Console → API Gateway Click Create API Select REST API (Build) Configure: Create new API: New API API name: FlyoraAPI Endpoint type: Regional Click Create API 2. Create Resources and Methods In the sidebar, select\nActions → Create Resource Resource Name: api Click Create Resource Select /api → Actions → Create Resource Configure the resource: Resource path: /api/ Resource Name: v1 Click Create Resource Create a proxy resource under /api Check Proxy resource Resource path: /api/ Resource Name: {proxy+} Click Create Resource Select /v1 → Actions → Create Resource Configure:\nCheck Proxy resource Resource path: /api/v1/ Resource Name: {myProxy+} Click Create Resource Enable CORS for all resources Under OPTIONS → Integration response → Header Mappings, ensure the headers below exist:\nAccess-Control-Allow-Origin: * Access-Control-Allow-Headers:\nContent-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token Access-Control-Allow-Methods:\nDELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT 3. Integrate with Lambda After creating /api/v1/{myProxy+}, the ANY method appears: Select ANY → Integration request → Edit Attach Lambda: Integration type: Lambda Function Check Lambda proxy integration Lambda Region: ap-southeast-1 (Singapore) Lambda Function: select your Lambda_API_Handler 4. Deploy API Select Actions → Deploy API Deployment stage: New stage Stage name: dev Description: Development stage for Lambda API Click Deploy After deployment, you will receive an Invoke URL in the format:\nhttps://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev\n"},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1\n.Date \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: BUILDING AGENTIC AI Context Optimization with Amazon Bedrock\n.Date \u0026amp; Time: 09:00, December 05, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/4-frontend/","title":"Frontend Workshop (UI)","tags":[],"description":"","content":"Frontend Hosting and API Integration on AWS In this workshop, you will learn how to deploy a frontend web application on AWS and connect it to a backend API hosted via Amazon API Gateway.\nThis activity combines frontend hosting and API integration, showing how AWS services can support interactive, serverless web applications.\nAmazon S3 – Stores and serves your static web assets (HTML, CSS, JS).\nAmazon CloudFront – Distributes your website globally with HTTPS and low latency.\nAmazon API Gateway – Exposes backend API endpoints that your frontend can call.\nThis workshop demonstrates how to connect a static website to a backend API through API Gateway, creating a complete serverless web architecture that enables real-time data interaction between frontend and backend.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5-cicd/","title":"CI/CD Automation","tags":[],"description":"","content":"AWS CodeBuild Setup Guide for Flyora Frontend This guide covers CI/CD setup for the Frontend Repository (React).\nThis guide walks you through setting up AWS CodeBuild and CodePipeline for the Flyora React frontend application.\nPrerequisites AWS Account with appropriate permissions GitHub repository: QuangHieu-lab/Flyora-shop buildspec.yml file in the repository root (see below) AWS Region: Singapore (ap-southeast-1) (or your preferred region) S3 bucket for hosting static files CloudFront distribution (optional, for CDN) Required: buildspec.yml File Create a buildspec.yml file in the root of your repository with this content:\nversion: 0.2 phases: pre_build: commands: - echo \u0026#34;Installing dependencies on `date`\u0026#34; - npm ci - echo \u0026#34;Running linter...\u0026#34; - npm run lint --if-present || echo \u0026#34;No lint script configured\u0026#34; build: commands: - echo \u0026#34;Running tests on `date`\u0026#34; - npm test -- --watchAll=false --passWithNoTests --coverage || echo \u0026#34;Tests completed\u0026#34; - echo \u0026#34;Building application on `date`\u0026#34; - npm run build - echo \u0026#34;Build completed on `date`\u0026#34; post_build: commands: - echo \u0026#34;Post-build phase started on `date`\u0026#34; - echo \u0026#34;Checking if build directory exists...\u0026#34; - ls -la build/ - echo \u0026#34;Build artifacts created successfully\u0026#34; - echo \u0026#34;Build completed successfully - artifacts ready for deployment\u0026#34; - echo \u0026#34;Use CodePipeline Deploy stage for S3 deployment\u0026#34; - echo \u0026#34;CloudFront invalidation will be handled by CodePipeline\u0026#34; artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: build name: BuildArtifact discard-paths: yes cache: paths: - \u0026#39;/root/.npm/**/*\u0026#39; - \u0026#39;node_modules/**/*\u0026#39; Key Points:\nUses npm ci for faster, reliable installs Runs linter if configured Runs tests with coverage Builds React application Deployment handled by CodePipeline Deploy stage (not in buildspec) Caches npm and node_modules Step 1: Create S3 Bucket for Hosting Before setting up CodeBuild, create an S3 bucket to host your React application:\nGo to S3 console Click \u0026ldquo;Create bucket\u0026rdquo; Bucket name: flyora-frontend-hosting Region: Singapore (ap-southeast-1) Uncheck \u0026ldquo;Block all public access\u0026rdquo; (for static website hosting) Enable \u0026ldquo;Static website hosting\u0026rdquo; in bucket properties Set Index document: index.html Set Error document: index.html Step 2: Navigate to CodeBuild Open your browser and go to: https://console.aws.amazon.com/codesuite/codebuild/projects Sign in to your AWS account Ensure you\u0026rsquo;re in the Singapore (ap-southeast-1) region Click \u0026ldquo;Create build project\u0026rdquo; Step 3: Project Configuration Field Value Project name flyora-frontend-build Description Build project for Flyora React frontend Build badge Leave unchecked Step 4: Source Configuration Field Value Source provider GitHub Repository Repository in my GitHub account GitHub repository https://github.com/QuangHieu-lab/Flyora-shop | Source version | Leave blank | | Git clone depth | 1 | | Primary source webhook events | ⚠️ UNCHECK this |\nStep 5: Environment Configuration Section Field Value Provisioning Provisioning model On-demand Environment image Managed image Operating system Amazon Linux Runtime(s) Standard Image Latest (e.g., aws/codebuild/amazonlinux2-x86_64-standard:5.0) Service role Service role New service role Step 6: Environment Variables Add these environment variables in CodeBuild:\nName Value Type AWS_S3_BUCKET flyora-frontend-hosting Plaintext CLOUDFRONT_DISTRIBUTION_ID Your CloudFront distribution ID (if using) Plaintext REACT_APP_API_URL Your backend API URL Plaintext Step 7: Buildspec and Logs Buildspec:\nBuild specifications: Use a buildspec file Buildspec name: Leave blank Logs:\nCloudWatch logs: ✅ Enable Group name: /aws/codebuild/flyora-frontend Step 8: IAM Permissions The CodeBuild service role needs permissions to access S3 and CloudFront. Add this policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::flyora-frontend-hosting\u0026#34;, \u0026#34;arn:aws:s3:::flyora-frontend-hosting/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 9: Create CodePipeline Go to CodePipeline console Click \u0026ldquo;Create pipeline\u0026rdquo; Pipeline name: flyora-frontend-pipeline Source stage: GitHub (Version 2) Build stage: Select flyora-frontend-build Deploy stage: Add S3 deploy action Action provider: Amazon S3 Bucket: flyora-frontend-hosting Extract file before deploy: ✅ Checked Testing After pipeline creation:\nMake a change in your React repository Commit and push to GitHub Pipeline automatically triggers Check S3 bucket for updated files Access your website via S3 endpoint or CloudFront URL Troubleshooting Issue: Build fails with \u0026ldquo;npm: command not found\u0026rdquo; Solution: Ensure runtime-versions: nodejs: 18 is set in buildspec.yml\nIssue: S3 sync permission denied Solution: Check IAM role has S3 permissions (see Step 8)\nIssue: Website shows old content Solution:\nClear browser cache Invalidate CloudFront cache if using CDN Check S3 bucket has latest files Quick Reference Resource Value Pipeline Name flyora-frontend-pipeline Build Project Name flyora-frontend-build S3 Bucket flyora-frontend-hosting Region Singapore (ap-southeast-1) Source GitHub (QuangHieu-lab/Flyora-shop) Buildspec buildspec.yml (in repo root) Logs CloudWatch: /aws/codebuild/flyora-frontend Cost Estimate Item Cost CodePipeline $1/month per active pipeline CodeBuild (Free Tier) 100 build minutes/month for 12 months S3 Storage ~$0.023/GB/month S3 Requests Minimal for static hosting CloudFront Free tier: 1TB data transfer/month Monthly (estimated) $1-3/month Summary ✅ Frontend CI/CD Pipeline Configured!\nAccomplished:\n✅ CodeBuild project for React application ✅ CodePipeline for automated workflow ✅ GitHub integration with automatic triggers ✅ Automatic deployment to S3 ✅ Optional CloudFront CDN integration Your pipeline now:\nAutomatically detects code changes in GitHub Builds React application with npm Deploys static files to S3 Serves website via S3 or CloudFront "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/2-backend/2.5/","title":"Testing API using Postman","tags":[],"description":"","content":"Objective Test the API Gateway REST endpoint integrated with the Lambda Function to verify the data operations performed on DynamoDB.\nDownload and install Postman before starting this section.\n1. Update Authorization Settings Go to AWS Console → API Gateway Select FlyoraAPI Navigate to\n/api/v1/{myProxy+} → ANY → Method request → Edit Set Authorization to AWS_IAM(Note: Only enable when testing with Postman, then remember to turn it off afterwards) 2. Create an Access Key Go to AWS Console → IAM → Users Click Create User Set username: test Confirm user creation Open the test user → Security credentials → Create access key Choose Local code Copy the Access key and Secret access key Testing GET Request Open Postman\nChoose GET\nEnter URL:https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/product/1\nHeaders tab:\nKey: Content-Type | Value: application/json Authorization tab:\nType: AWS Signature Enter AccessKey Enter SecretKey AWS Region: ap-southeast-1 Service Name: execute-api Click Send\nResult: Returns the list of Items from the reviews table.\nTesting POST Request Choose POST\nURL:https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/submit\nBody → raw → JSON\n{ \u0026#34;customerId\u0026#34;: 2, \u0026#34;rating\u0026#34;: 4, \u0026#34;comment\u0026#34;: \u0026#34;Chim ăn ngon và vui vẻ!\u0026#34;, \u0026#34;customerName\u0026#34;: \u0026#34;Nguyễn Văn B\u0026#34; } Click Send\nResult: Inserts a new Item into the Review table.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploying the Flyora E-commerce System on AWS Overview In this workshop, you will deploy the core components of the Flyora platform using a Serverless architecture on AWS.\nThe objective is to build a system that is scalable, cost-efficient, and easy to maintain.\nComponents to be deployed:\nFrontend: Store \u0026amp; deliver UI via S3 + CloudFront Backend API: Handle business logic with API Gateway + AWS Lambda Database: Manage product / order data using DynamoDB + S3 User Authentication: Implemented via Amazon Cognito Chatbot: Product consultation assistant integrated into UI (handled by AI Team) The workshop is divided into group roles for parallel development: Backend (BE), AI (Chatbot), and Frontend (FE).\nSystem Architecture Workshop Content Introduction: Objectives \u0026amp; Expected Outcomes\nBackend Workshop (BE) — Build API + Automated Data Import Pipeline\nPrepare \u0026amp; upload CSV data to S3 Create Lambda to automatically write CSV data to DynamoDB (S3 Trigger) Create API Gateway and integrate Lambda as Backend API Test API via Postman / API Gateway Console AI Workshop (Chatbot) — Product Consultation Support\n(Create VPC \u0026amp; Configure Security Groups for RDS and Lambda) (Configure RDS and connect to Dbeaver) (Creating logic for lambda function) Frontend Workshop (FE) — Display Data \u0026amp; Hosting website\nHosting website with S3 Distribute with CloudFront API Integration Set Up CI/CD for Automatic Deployment\nResource Cleanup to Avoid Unnecessary Charges\nThis workshop is designed to run within the AWS Free Tier,\nusing no EC2, no SSH, and no paid services beyond free tier limits.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/6-cleanup/","title":"Clean Up Resources to Avoid AWS Charges","tags":[],"description":"","content":"Clean Up AWS Resources To avoid unexpected charges, you need to delete the AWS resources created during this workshop in the following order:\n1. Delete EventBridge Rule Go to AWS Console → EventBridge Select Rules Select the DatabaseBackup rule Click Delete Confirm deletion 2. Delete API Gateway Go to AWS Console → API Gateway Select FlyoraAPI Choose Actions → Delete API Confirm deletion by entering the API name Click Delete 3. Delete Lambda Functions Go to AWS Console → Lambda Delete the following Lambda functions: DynamoDB_API_Handler AutoImportCSVtoDynamoDB DatabaseBackupFunction For each function: Select the function → Actions → Delete Confirm deletion 4. Delete DynamoDB Tables Go to AWS Console → DynamoDB Select Tables Delete all tables created from CSV files: Select a table → Delete Confirm deletion by typing delete Repeat for all tables (Products, Orders, Customers, Reviews, etc.) 5. Delete S3 Buckets and Objects 5.1. Delete Database Bucket Go to AWS Console → S3 Select the flyora-bucket-database bucket Delete all objects in the bucket: Click Empty bucket Confirm by typing permanently delete After the bucket is empty: Select the bucket → Delete bucket Confirm by entering the bucket name 5.2. Delete Backup Bucket Select the flyora-bucket-backup bucket Delete all objects: Click Empty bucket Confirm deletion Delete the bucket: Click Delete bucket Confirm by entering the bucket name 6. Delete IAM User and Access Key Go to AWS Console → IAM → Users Select the test user Go to the Security credentials tab Delete the Access Key you created: Select the Access Key → Actions → Delete Return to the Users list Select the test user → Delete user Confirm deletion 7. Delete IAM Roles 7.1. Delete LambdaAPIAccessRole Go to AWS Console → IAM → Roles Select LambdaAPIAccessRole Detach the attached policies: AmazonDynamoDBFullAccess CloudWatchLogsFullAccess AWSXRayDaemonWriteAccess Click Delete role Confirm deletion 7.2. Delete LambdaS3DynamoDBRole Select LambdaS3DynamoDBRole Detach the policies: AmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Click Delete role Confirm deletion 7.3. Delete LambdaDynamoDBBackupRole Select LambdaDynamoDBBackupRole Detach the policies: AmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Click Delete role Confirm deletion 8. Delete CloudWatch Logs Go to AWS Console → CloudWatch Select Logs → Log groups Find and delete the related log groups: /aws/lambda/DynamoDB_API_Handler /aws/lambda/AutoImportCSVtoDynamoDB /aws/lambda/DatabaseBackupFunction /aws/apigateway/FlyoraAPI Select log group → Actions → Delete log group(s) Confirm deletion 9. Delete X-Ray Traces (Optional) X-Ray traces automatically expire after 30 days and don\u0026rsquo;t incur storage charges, but you can manually delete them if desired.\nGo to AWS Console → X-Ray Select Traces Traces will be automatically deleted after the default retention period 10. Delete RDS and Subnet groups Go to Subnet groups, select the created subnet group, and click Delete. Go to Databases, select the created database → Actions → Delete. 11. Delete BirdShopChatBot Lambda and Layer Go to Functions, select BirdShopChatBot → Actions → Delete. Go to Layers, select the created layer, and click Delete. 12. Delete VPC, NAT Gateway, Elastic IP, and EC2 Go to VPC, select the created NAT gateway → Actions → Delete NAT gateway. Select Elastic IPs → Actions → Release Elastic IP addresses. After deleting the NAT gateway and Elastic IP, go to Your VPCs, select the created VPC → Actions → Delete VPC. Go to EC2, select Instances, choose the created EC2 instance → Instance state → Terminate instance. 13. Delete Cloudfront Go to CloudFront, select the created distribution → Actions → Disable. Wait until the status changes to Disabled. Select the checkbox for the disabled distribution again. Choose Delete and confirm the deletion. The distribution cannot be recovered once deleted. Final Verification After completing the steps above, verify the following services to ensure no resources remain:\n✅ EventBridge: No rules remaining ✅ API Gateway: No APIs remaining ✅ Lambda: No functions remaining (3 functions) ✅ DynamoDB: No tables remaining ✅ S3: No buckets remaining (2 buckets) ✅ Cloudfront: No more distribution ✅ IAM Users: No test user remaining ✅ IAM Roles: No created roles remaining (3 roles) ✅ CloudWatch Logs: No related log groups remaining ✅ X-Ray: Traces will expire automatically ✅ RDS: Successfully deleted ✅ NAT gateway: No longer exists ✅ Elastic IP: No longer exists ✅ EC2 : Terminated Make sure you\u0026rsquo;ve deleted all resources to avoid unexpected charges. Pay special attention to S3 buckets as they can accumulate data over time.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/2-backend/2.6/","title":"S3 CSV Backup","tags":[],"description":"","content":"S3 CSV Backup Create IAM Role for Lambda Go to AWS Management Console → search for IAM. Select Roles → Create Role. Choose Trusted entity type: AWS service. Choose Use case: Lambda, then click Next. Attach Permissions to the Role Attach the following policies:\nAmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Click Next, then name the role: LambdaDynamoDBBackupRole.\nThis role allows the Lambda function to scan all DynamoDB tables and store the backup as CSV files in an S3 bucket.\nCreate S3 Bucket Open the S3 service. In the S3 dashboard, click Create bucket. In the Create bucket screen:\nBucket name: Enter a name, for example:\nflyora-bucket-backup (If the name is already taken, add a number at the end.)\nLeave all other configuration settings as default.\nReview the configuration and click Create bucket to finish. Configure Lambda Trigger for S3 Create Lambda Function Go to Lambda → Create function. Choose Author from scratch Name: DatabaseBackupFunction Runtime: Python 3.14 Role: select LambdaDynamoDBBackupRole created earlier Go to Configuration → Environment variables Click Edit Add environment variable: Key: BUCKET_NAME Value: flyora-bucket-backup Click Save Code\nimport boto3 import csv import io import os from datetime import datetime from boto3.dynamodb.conditions import Key s3 = boto3.client(\u0026#39;s3\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) def scan_all(table): \u0026#34;\u0026#34;\u0026#34;Scan toàn bộ bảng DynamoDB, xử lý paging.\u0026#34;\u0026#34;\u0026#34; items = [] response = table.scan() items.extend(response.get(\u0026#39;Items\u0026#39;, [])) while \u0026#39;LastEvaluatedKey\u0026#39; in response: response = table.scan(ExclusiveStartKey=response[\u0026#39;LastEvaluatedKey\u0026#39;]) items.extend(response.get(\u0026#39;Items\u0026#39;, [])) return items def lambda_handler(event, context): bucket = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] tables = [t.strip() for t in os.environ[\u0026#34;TABLE_LIST\u0026#34;].split(\u0026#34;,\u0026#34;)] timestamp = datetime.utcnow().strftime(\u0026#34;%Y-%m-%d-%H-%M-%S\u0026#34;) for table_name in tables: try: table = dynamodb.Table(table_name) data = scan_all(table) if not data: print(f\u0026#34;Table {table_name} EMPTY → skip\u0026#34;) continue # Lấy danh sách tất cả fields all_keys = sorted({key for item in data for key in item.keys()}) # Convert to CSV csv_buffer = io.StringIO() writer = csv.DictWriter(csv_buffer, fieldnames=all_keys) writer.writeheader() for item in data: writer.writerow({k: item.get(k, \u0026#34;\u0026#34;) for k in all_keys}) key = f\u0026#34;dynamo_backup/{table_name}/{table_name}_{timestamp}.csv\u0026#34; s3.put_object( Bucket=bucket, Key=key, Body=csv_buffer.getvalue().encode(\u0026#34;utf-8\u0026#34;) ) print(f\u0026#34;Backup xong bảng {table_name} → {key}\u0026#34;) except Exception as e: print(f\u0026#34;Lỗi khi backup bảng {table_name}: {e}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tables\u0026#34;: tables } Click Deploy in the Lambda console. Configure Automatic Schedule Go to Lambda → Triggers → Add Trigger → EventBridge (Schedule)\nRule name: DatabaseBackup\nRule description: AutoBackup in 4 days\nRule type: Schedule expression\nSchedule expression: rate(4 days)\n"},{"uri":"https://workshop-sample.fcjuni.com/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services (AWS) from 07/09 to 28/11, I had the opportunity to learn, practice, and apply the knowledge I gained in school to real-world cloud environments.\nI participated in building a chatbot system using AWS services such as Lambda, API Gateway, RDS PostgreSQL, Amazon Bedrock, Cohere Embeddings, and VPC networking components. Through this project, I improved my skills in cloud architecture design, programming, system analysis, technical documentation, teamwork, and communication.\nRegarding work ethic, I always strived to complete assigned tasks on time, follow team workflows, and proactively communicate with my mentor to ensure effective collaboration.\nBelow is my self-evaluation based on key performance criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Need to improve discipline, particularly in time management and consistency with workflow standards. Need to enhance ability to evaluate problems more thoroughly before choosing a technical solution. Need to strengthen technical communication skills to explain ideas more clearly during discussions. "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/2-backend/2.7/","title":"Integrating AWS X-Ray","tags":[],"description":"","content":"Objective Use AWS X-Ray to trace and inspect the entire processing flow of API Gateway → Lambda → DynamoDB.\nX-Ray helps visualize traces, latency, errors, and segments/subsegments to ensure the API behaves correctly and efficiently.\nImplementation Steps 1. Access IAM Service Go to AWS Console → IAM Select Roles → LambdaAPIAccessRole Choose Add permissions → Attach policies → AWSXRayDaemonWriteAccess 2. Access Lambda Go to AWS Console → Lambda Select Functions → DynamoDB_API_Handler Go to\nConfiguration → Monitoring and operations tools → Additional monitoring tools → Edit Under Lambda service traces, enable Enable 3. Access API Gateway Go to AWS Console → API Gateway Select APIs → FlyoraAPI Go to\nStages → Logs and tracing → Edit Check X-Ray tracing 4. Testing Go to AWS Console → Lambda In the Test tab, click Create new event\nEvent name: test\nPaste the JSON below into the event:\n{ \u0026#34;resource\u0026#34;: \u0026#34;/{myProxy+}\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/api/v1/bird-types\u0026#34;, \u0026#34;httpMethod\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;headers\u0026#34;: {}, \u0026#34;multiValueHeaders\u0026#34;: {}, \u0026#34;queryStringParameters\u0026#34;: {}, \u0026#34;multiValueQueryStringParameters\u0026#34;: {}, \u0026#34;pathParameters\u0026#34;: {}, \u0026#34;stageVariables\u0026#34;: {}, \u0026#34;requestContext\u0026#34;: { \u0026#34;identity\u0026#34;: {} }, \u0026#34;body\u0026#34;: null, \u0026#34;isBase64Encoded\u0026#34;: false } Save -\u0026gt; Test\nNext, go to AWS Console → X-ray In tab Traces, a new trace ID will appear Click it to view detailed trace information\n"},{"uri":"https://workshop-sample.fcjuni.com/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working atmosphere at FCJ is incredibly welcoming and collaborative. I felt supported not just during office hours but whenever I faced challenges. The workspace is well-organized and conducive to productivity.\n2. Support from Mentor / Team Admin\nhighly appreciate my mentor\u0026rsquo;s coaching style. Instead of micromanaging, they encouraged me to brainstorm and troubleshoot problems independently before providing detailed feedback. This approach significantly improved my problem-solving skills. The Team Admin was also very helpful with documentation and onboarding procedures.\n3. Relevance of Work to Academic Major\nThe assignments were highly relevant to my academic background while introducing me to new technological domains not covered in university. This balance allowed me to solidify my foundational knowledge while acquiring up-to-date practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nBeyond technical skills, I gained valuable experience in project management tools, teamwork, and professional communication standards. The career advice and industry insights shared by my mentor were particularly beneficial for my future path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is positive and respectful. Everyone works with a high level of professionalism but maintains a cheerful attitude. I was particularly impressed by the team spirit during deadlines—everyone supports each other regardless of their level, making me feel like a valued member of the team.\n6. Internship Policies / Benefits\nThe internship allowance and flexible scheduling demonstrated the company\u0026rsquo;s consideration for students. Additionally, access to internal training sessions was a significant advantage that added great value to my internship.\nAdditional Questions Most Satisfied With: The autonomy I was given to handle tasks and the constructive feedback culture.\nRecommendation: Yes, definitely. It is an excellent environment for fresh graduates to build a strong foundation for their careers.\nSuggestions \u0026amp; Expectations Future Plans: I am very interested in continuing my journey with the company and contributing to future projects as a full-time employee if the opportunity arises. "},{"uri":"https://workshop-sample.fcjuni.com/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://workshop-sample.fcjuni.com/tags/","title":"Tags","tags":[],"description":"","content":""}]