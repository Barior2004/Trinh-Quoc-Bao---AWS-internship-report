[{"uri":"https://workshop-sample.fcjuni.com/vi/","title":"Báo cáo thực tập","tags":[],"description":"","content":"Báo cáo thực tập Thông tin sinh viên: Họ và tên: Trịnh Quốc Bảo\nSố điện thoại: 0327835351\nEmail: baotqse182782@fpt.edu.vn\nTrường: Đại học FPT HCM\nNgành: Trí tuệ nhân tạo\nLớp: FCJ 2025\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 08/09/2025 đến ngày 08/12/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "},{"uri":"https://workshop-sample.fcjuni.com/vi/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/vi/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/vi/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/vi/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/vi/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/vi/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/2-backend/2.1/","title":"Chuẩn bị &amp; Cấu hình Lambda Trigger cho S3","tags":[],"description":"","content":"Chuẩn bị \u0026amp; Cấu hình Lambda Trigger cho S3 Các bước thực hiện 1. Truy cập dịch vụ IAM Vào AWS Management Console → tìm IAM. Chọn Roles → Create Role. Chọn Trusted entity type: AWS service. Chọn Use case: Lambda, sau đó nhấn Next. 2. Gán quyền truy cập cho Role Gắn các policy sau:\nAmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Nhấn Next, đặt tên role là LambdaS3DynamoDBRole.\nRole này giúp Lambda có thể đọc file từ S3 và ghi dữ liệu vào DynamoDB.\nTạo S3 Bucket Truy cập dịch vụ S3. Trong giao diện S3, chọn Create bucket. Trong màn hình Create bucket:\nBucket name: Nhập tên, ví dụ:\nflyora-bucket-database (Nếu tên đã tồn tại, hãy thêm số phía sau.)\nGiữ nguyên các thiết lập mặc định còn lại.\nXem lại cấu hình và chọn Create bucket để hoàn tất. Kết quả mong đợi Bucket flyora-bucket (hoặc tên bạn đã đặt) được tạo thành công. Role LambdaS3DynamoDBRole sẵn sàng để gán cho Lambda trong bước kế tiếp. Cấu hình Lambda Trigger cho S3 Trong bước này, bạn sẽ cấu hình AWS Lambda để tự động import file CSV vào DynamoDB mỗi khi có file mới trong S3 Bucket.\nTạo Lambda Function Truy cập Lambda → Create function. Chọn Author from scratch. Đặt tên: AutoImportCSVtoDynamoDB. Runtime: Python 3.13. Role: chọn LambdaS3DynamoDBRole đã tạo ở bước trước. Thêm Trigger Trong tab Configuration → Triggers, nhấn Add trigger. Chọn S3. Chọn Bucket flyora-bucket. Event type: All object create events. Nhấn Add để lưu. Dán code Lambda Dán đoạn code dưới đây: import boto3 import csv import io import json from botocore.exceptions import ClientError from decimal import Decimal dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) # ------------------------- # Hàm kiểm tra kiểu dữ liệu của mẫu (Detect Type) # ------------------------- def detect_type(value): val_str = str(value).strip() # Check Int/Float try: float(val_str) return \u0026#39;N\u0026#39; # Number except ValueError: pass return \u0026#39;S\u0026#39; # String # ------------------------- # Hàm chuyển đổi dữ liệu (Convert) # ------------------------- def convert_value(value): if value is None: return None val_str = str(value).strip() if val_str == \u0026#34;\u0026#34;: return None # Int check try: if float(val_str).is_integer(): return int(float(val_str)) except ValueError: pass # Decimal check (cho Float) try: return Decimal(val_str) except Exception: pass # Boolean if val_str.lower() == \u0026#34;true\u0026#34;: return True if val_str.lower() == \u0026#34;false\u0026#34;: return False return val_str # ------------------------- # Tạo bảng Dynamic dựa trên kiểu dữ liệu phát hiện được # ------------------------- def create_table_if_not_exists(table_name, pk_name, pk_type): existing_tables = dynamodb.meta.client.list_tables()[\u0026#39;TableNames\u0026#39;] if table_name in existing_tables: print(f\u0026#34;Table \u0026#39;{table_name}\u0026#39; already exists.\u0026#34;) return print(f\u0026#34;Creating table: {table_name} | PK: {pk_name} | Type: {pk_type}\u0026#34;) table = dynamodb.create_table( TableName=table_name, KeySchema=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;KeyType\u0026#39;: \u0026#39;HASH\u0026#39;}], AttributeDefinitions=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;AttributeType\u0026#39;: pk_type}], BillingMode=\u0026#39;PAY_PER_REQUEST\u0026#39; ) table.wait_until_exists() print(\u0026#34;Table created successfully.\u0026#34;) # ------------------------- # Main Handler # ------------------------- def lambda_handler(event, context): try: for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] print(f\u0026#34;Processing: {key}\u0026#34;) response = s3.get_object(Bucket=bucket, Key=key) # 1. QUAN TRỌNG: Dùng \u0026#39;utf-8-sig\u0026#39; để xóa BOM, giúp nhận diện số chính xác body = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8-sig\u0026#39;) reader = csv.DictReader(io.StringIO(body)) # Clean headers reader.fieldnames = [name.strip() for name in reader.fieldnames] items = list(reader) if not items: continue # Lấy thông tin Partition Key (PK) pk_name = reader.fieldnames[0] table_name = key.split(\u0026#39;.\u0026#39;)[0] # 2. QUAN TRỌNG: Phát hiện kiểu dữ liệu dựa trên dòng đầu tiên first_pk_val = items[0].get(pk_name) pk_type = detect_type(first_pk_val) # Sẽ trả về \u0026#39;N\u0026#39; nếu là số, \u0026#39;S\u0026#39; nếu là chữ # Tạo bảng đúng kiểu (N hoặc S) create_table_if_not_exists(table_name, pk_name, pk_type) table = dynamodb.Table(table_name) count = 0 with table.batch_writer() as batch: for row in items: clean_item = {} is_valid = True for k, v in row.items(): if not k or k.strip() == \u0026#34;\u0026#34;: continue clean_k = k.strip() val = convert_value(v) # Chuyển đổi sang Int/Decimal/Bool if val is None: continue # 3. QUAN TRỌNG: Xử lý Partition Key theo đúng kiểu của Bảng if clean_k == pk_name: if pk_type == \u0026#39;N\u0026#39;: # Nếu bảng là Number, bắt buộc Key phải là Number if not isinstance(val, (int, Decimal)): print(f\u0026#34;SKIPPING ROW: Key \u0026#39;{val}\u0026#39; is not a number but table requires Number.\u0026#34;) is_valid = False break else: # Nếu bảng là String, ép kiểu sang String val = str(val) clean_item[clean_k] = val if is_valid and pk_name in clean_item: batch.put_item(Item=clean_item) count += 1 print(f\u0026#34;Success: Imported {count} items into {table_name} (PK Type: {pk_type})\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;OK\u0026#39;)} except Exception as e: print(f\u0026#34;ERROR: {str(e)}\u0026#34;) import traceback traceback.print_exc() return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} Nhấn Deploy và xác nhận Successfully. "},{"uri":"https://workshop-sample.fcjuni.com/vi/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Bài thu hoạch “AWS Cloud Mastery Series #1” Mục Đích Của Sự Kiện Chia sẻ kinh nghiệm đi làm ở doanh nghiệp. Giới thiệu các mô hình được huấn luyện sẵn của AWS. Giới thiệu về bedrock agent core. Củng cố kiến thức cho proposal. Danh Sách Diễn Giả Dinh Le Hoang Anh - Cloud Engineer Trainee First Cloud AI Journey Danh Hoang Hieu Nghi - AI Engineer Renova Cloud Lam Tuan Kiet - Sr DevOps Engineer FPT Software Nội Dung Nổi Bật 1. Tổng quan về AWS AI/ML Services Phần mở đầu tập trung vào các dịch vụ AI được huấn luyện trước (Pre-trained AI Services) của AWS. Đây là các giải pháp \u0026ldquo;Ready-to-use\u0026rdquo;, cho phép tích hợp trí tuệ nhân tạo vào ứng dụng mà không cần kiến thức sâu về Data Science hay xây dựng mô hình từ đầu.\nCác nhóm dịch vụ chính: Computer Vision (Thị giác máy tính): Amazon Rekognition: Phân tích hình ảnh và video (nhận diện khuôn mặt, vật thể, văn bản). Amazon Lookout (Vision \u0026amp; Equipment): Các giải pháp cho công nghiệp, giúp phát hiện lỗi sản phẩm và bảo trì dự đoán cho thiết bị. Language \u0026amp; Speech (Ngôn ngữ \u0026amp; Giọng nói): Amazon Translate: Dịch thuật đa ngôn ngữ tự động. Amazon Textract: OCR nâng cao, trích xuất văn bản và dữ liệu có cấu trúc từ tài liệu scan. Amazon Transcribe: Chuyển đổi giọng nói thành văn bản (Speech-to-Text). Amazon Polly: Chuyển đổi văn bản thành giọng nói tự nhiên (Text-to-Speech). Amazon Comprehend: NLP service giúp phân tích cảm xúc, từ khóa và ngữ nghĩa văn bản. Search \u0026amp; Personalization: Amazon Kendra: Công cụ tìm kiếm thông minh cho doanh nghiệp. Amazon Personalize: Xây dựng hệ thống gợi ý (recommendation) theo thời gian thực. Ghi chú bổ sung: Giới thiệu về Pinecone (trong note là Pinecat) - Đây là một Vector Database quan trọng, thường được sử dụng kết hợp với các dịch vụ AI để lưu trữ embedding.\n2. Generative AI với Amazon Bedrock Phần này chuyển trọng tâm từ ML truyền thống sang làn sóng Generative AI (GenAI) và nền tảng Amazon Bedrock.\n2.1. Kiến thức nền tảng GenAI là gì? Phân biệt giữa ML truyền thống (tập trung phân tích/dự đoán) và GenML (tập trung tạo sinh nội dung mới). Foundation Models (FMs): Bedrock cung cấp quyền truy cập vào các mô hình nền tảng hàng đầu thông qua API. 2.2. Prompt Engineering (Kỹ thuật Prompt) Các kỹ thuật để tối ưu hóa kết quả đầu ra của mô hình:\nZero-Shot Prompting: Đưa yêu cầu không cần ví dụ mẫu. Few-Shot Prompting: Cung cấp vài ví dụ (shots) để hướng dẫn mô hình hiểu ngữ cảnh. Chain of Thought (CoT): Yêu cầu mô hình suy luận từng bước (step-by-step) để giải quyết vấn đề phức tạp. 2.3. Retrieval-Augmented Generation (RAG) Giải pháp kết hợp LLM với dữ liệu riêng của doanh nghiệp:\nEmbedding: Sử dụng mô hình Amazon Titan Embedding để vector hóa dữ liệu. Knowledge Base: Quy trình tạo Knowledge Base trên AWS để chatbot có thể tra cứu thông tin chính xác, giảm thiểu ảo giác (hallucination). 3. Ứng dụng nâng cao: Bedrock Agents Mở rộng khả năng của GenAI từ \u0026ldquo;trả lời\u0026rdquo; sang \u0026ldquo;hành động\u0026rdquo;.\nBedrock Agent Core: Cốt lõi của Agent giúp kết nối LLM với các API backend để thực thi tác vụ. Frameworks: Các công cụ hỗ trợ xây dựng Agent. Tính năng nổi bật: Khả năng lập kế hoạch (planning), phân rã tác vụ và thực hiện quy trình nghiệp vụ nhiều bước (multi-step workflows). 4. Kết luận Buổi hội thảo cung cấp cái nhìn toàn diện về lộ trình ứng dụng AI trên AWS:\nBắt đầu nhanh với các Pre-trained Services (Rekognition, Polly\u0026hellip;). Tận dụng sức mạnh sáng tạo của GenAI qua Bedrock. Tối ưu hóa độ chính xác với RAG và Vector DB (Pinecone). Tự động hóa quy trình phức tạp với Bedrock Agents. Một số hình ảnh khi tham gia sự kiện Tổng thể, sự kiện không chỉ cung cấp kiến thức kỹ thuật mà còn giúp tôi thay đổi cách tư duy về thiết kế ứng dụng, hiện đại hóa hệ thống và phối hợp hiệu quả hơn giữa các team.\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Bài thu hoạch “BUILDING AGENTIC AI Context Optimization with Amazon Bedrock” Mục Đích Của Sự Kiện Phân tích chuyên sâu về khả năng của tác nhân Amazon Bedrock, bao gồm các khái niệm cốt lõi, kiến ​​trúc và các tính năng chính để xây dựng tác nhân AI trên AWS. Bài thuyết trình về trường hợp sử dụng thực tế của CEO Diaflow, giới thiệu cách triển khai thực tế quy trình làm việc của agentic bằng cách sử dụng dịch vụ AWS. Tổng quan ngắn gọn về công ty và giới thiệu sản phẩm của người đồng sáng lập CloudThinker, nêu bật nền tảng điều phối AI của họ. Phiên họp kỹ thuật nâng cao (L300) khám phá khuôn khổ phối hợp của CloudThinker và các kỹ thuật tối ưu hóa ngữ cảnh trên Amazon Bedrock. Workshop lập trình tương tác với các kỹ sư CloudThinker, tập trung vào việc triển khai thực tế. Danh Sách Diễn Giả Kien Nguyen - Solutions Architect Amazon Web Services Viet Pham -Founder cum CEO Diaflow Thang Ton - Co-founder \u0026amp; COO CloudThinker Henry Bui - Head of Engineering CloudThinker Kha Van - Community Leader Amazon Web Services Nội Dung Nổi Bật 1. Phân tích chuyên sâu: Amazon Bedrock Agents Phần đầu tiên tập trung vào việc mổ xẻ khả năng của Amazon Bedrock Agents, chuyển dịch từ việc sử dụng LLM thụ động sang các tác nhân AI chủ động.\nKhái niệm cốt lõi (Core Concepts): Agent không chỉ trả lời câu hỏi mà còn có khả năng lập kế hoạch (planning) và sử dụng công cụ (tools) để hoàn thành mục tiêu. Kiến trúc (Architecture): Kết nối Foundation Models (FMs) với các nguồn dữ liệu doanh nghiệp và API. Cơ chế \u0026ldquo;Chain-of-Thought\u0026rdquo; giúp Agent tự động phân rã yêu cầu phức tạp thành các bước nhỏ để thực thi. Tính năng chính: Quản lý bộ nhớ (Memory management) để duy trì ngữ cảnh hội thoại. Khả năng truy xuất kiến thức (RAG) tích hợp sẵn. Tracing: Theo dõi quá trình suy luận của Agent để debug và tối ưu hóa. 2. Case Study thực tế: Diaflow Bài tham luận chia sẻ về cách Diaflow đã hiện thực hóa khái niệm \u0026ldquo;Agentic Workflow\u0026rdquo; vào sản phẩm thực tế:\nTriển khai: Sử dụng các dịch vụ AWS để xây dựng luồng công việc tự động hóa, nơi các Agent đóng vai trò như những nhân viên ảo xử lý tác vụ cụ thể. Bài học kinh nghiệm: Cách xử lý các thách thức khi đưa Agent vào môi trường production, đảm bảo độ tin cậy và phản hồi chính xác trong các tình huống thực tế. 3. Giải pháp Điều phối AI: CloudThinker Overview Giới thiệu về CloudThinker và vai trò của nền tảng điều phối (Orchestration Platform) trong hệ sinh thái AI:\nVấn đề: Khi hệ thống AI phức tạp lên, việc quản lý nhiều model và nhiều agent trở nên khó khăn. Giải pháp: CloudThinker cung cấp lớp điều phối giúp quản lý, giám sát và tối ưu hóa sự tương tác giữa các thành phần AI, giúp doanh nghiệp tập trung vào logic nghiệp vụ thay vì hạ tầng. 4. Phiên kỹ thuật nâng cao (L300): Orchestration \u0026amp; Context Optimization Khuôn khổ phối hợp (Orchestration Framework): Cách thiết kế hệ thống để nhiều Agent có thể cộng tác (Multi-agent collaboration). Điều hướng luồng xử lý (Routing) thông minh đến đúng Agent chuyên trách. Tối ưu hóa ngữ cảnh (Context Optimization): Kỹ thuật quản lý cửa sổ ngữ cảnh (Context Window) của LLM trên Bedrock. Cách chọn lọc thông tin quan trọng để đưa vào prompt, giúp giảm chi phí token và tăng độ chính xác cho câu trả lời. 5. Workshop: Lập trình tương tác (Interactive Coding) Phần thực hành tập trung vào việc triển khai thực tế (\u0026ldquo;Hands-on\u0026rdquo;):\nNội dung: Viết code để tích hợp framework của CloudThinker với Amazon Bedrock. Kết quả: Xây dựng thành công một quy trình Agent cơ bản, từ việc nhận yêu cầu, xử lý ngữ cảnh đến việc gọi API thực thi tác vụ. 6. Tổng kết Sự kiện đã mở ra hướng đi mới trong việc phát triển ứng dụng AI:\nAgentic AI là tương lai: Chuyển từ Chatbot hỏi-đáp sang Agent thực thi hành động. Tầm quan trọng của Orchestration: Cần một lớp điều phối mạnh mẽ (như CloudThinker) để quản lý các hệ thống AI phức tạp. Tối ưu hóa là chìa khóa: Việc quản lý ngữ cảnh hiệu quả trên Bedrock quyết định trực tiếp đến hiệu năng và chi phí của dự án. Một số hình ảnh khi tham gia sự kiện Tổng thể, sự kiện không chỉ cung cấp kiến thức kỹ thuật mà còn giúp tôi thay đổi cách tư duy về thiết kế ứng dụng, hiện đại hóa hệ thống và phối hợp hiệu quả hơn giữa các team.\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/1-introduction/","title":"Giới thiệu","tags":[],"description":"","content":"Giới thiệu Workshop Flyora là gì? Flyora là nền tảng thương mại điện tử hiện đại được thiết kế để minh họa kiến trúc cloud-native sử dụng các dịch vụ serverless của AWS. Workshop này hướng dẫn bạn xây dựng một cửa hàng trực tuyến hoàn chỉnh với tính năng duyệt sản phẩm, chatbot hỗ trợ AI, và quản lý dữ liệu tự động.\nMục tiêu Workshop Sau khi hoàn thành workshop, bạn sẽ:\nTriển khai hệ thống thương mại điện tử serverless trên AWS Xây dựng pipeline dữ liệu tự động sử dụng S3 và Lambda triggers Phát triển RESTful APIs với API Gateway và Lambda Tạo cơ sở dữ liệu có khả năng mở rộng bằng DynamoDB Tích hợp chatbot AI cho hỗ trợ khách hàng Triển khai frontend tĩnh với S3 và CloudFront Thiết lập CI/CD pipeline cho triển khai tự động Tổng quan Kiến trúc Nền tảng Flyora sử dụng kiến trúc serverless hoàn toàn:\nTầng Frontend:\nWebsite tĩnh được host trên Amazon S3 Phân phối nội dung toàn cầu qua Amazon CloudFront Giao diện responsive cho duyệt và mua sắm sản phẩm Tầng Backend:\nAPI Gateway cho các RESTful API endpoints AWS Lambda functions xử lý business logic Amazon DynamoDB lưu trữ dữ liệu sản phẩm và đơn hàng Amazon S3 cho import và lưu trữ dữ liệu Tầng AI:\nChatbot hỗ trợ AI cho gợi ý sản phẩm Tích hợp vào giao diện frontend Hỗ trợ khách hàng theo thời gian thực Bảo mật \u0026amp; Xác thực:\nAmazon Cognito cho xác thực người dùng IAM roles cho truy cập dịch vụ an toàn Cấu trúc Workshop Workshop được tổ chức theo các module theo nhóm:\nBackend Team - Phát triển API và data pipeline AI Team - Tích hợp chatbot và tính năng AI Frontend Team - Phát triển và triển khai UI CI/CD - Pipeline triển khai tự động Testing - Kiểm thử hệ thống và hiệu năng Cleanup - Quản lý tài nguyên và tối ưu chi phí Kết quả Mong đợi Sau khi hoàn thành workshop, bạn sẽ có:\nWebsite thương mại điện tử hoạt động hoàn chỉnh trên AWS Kinh nghiệm thực tế với kiến trúc serverless Hiểu biết về AWS best practices cho khả năng mở rộng và bảo mật Kiến thức triển khai CI/CD cho ứng dụng cloud Dự án portfolio thể hiện kỹ năng cloud engineering Chi phí Workshop này được thiết kế để chạy trong AWS Free Tier. Tất cả dịch vụ sử dụng đều có tùy chọn free tier, và kiến trúc tránh các tài nguyên tốn kém như EC2 instances. Chi phí ước tính: $0-5 USD nếu hoàn thành trong vài giờ.\nYêu cầu Trước khi Bắt đầu Trước khi bắt đầu, đảm bảo bạn có:\nTài khoản AWS với quyền quản trị Hiểu biết cơ bản về cloud computing Quen thuộc với REST APIs và JSON Kiến thức cơ bản về HTML/CSS/JavaScript (cho frontend) Git đã cài đặt trên máy local "},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/4-frontend/4.1/","title":"Hosting website với S3","tags":[],"description":"","content":" Bước 1: Tạo một S3 Bucket Truy cập vào dịch vụ S3. Nhấn Create bucket. Nhập một Tên Bucket duy nhất (ví dụ: flyora-shop).\nBỏ chọn “Block all public access” Xác nhận cảnh báo về quyền truy cập công khai. Nhấn Create bucket. Bước 2: Tải lên các tệp website Mở bucket vừa tạo. Nhấn Upload → Add files → chọn các tệp website của bạn (ví dụ: index.html) Nhấn Upload. Bước 3: Bật tính năng Static Website Hosting Chuyển đến tab Properties của bucket. Kéo xuống Static website hosting. Nhấn Edit → Bật Static website hosting Nhập: Index document: index.html Error document: index.html (tùy chọn) Nhấn Save changes. Chuyển sang tab Permission của bucket. Chỉnh sửa Bucket Policy Dán JSON policy sau (thay flyora-shop bằng tên bucket của bạn): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; } ] } Bước 4: Kiểm tra Website Nhấn vào Bucket website endpoint URL http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com Đảm bảo website hiển thị như hình dưới: "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/","title":"Nhật ký công việc","tags":[],"description":"","content":"Tuần 1: Làm quen với AWS và các dịch vụ cơ bản trong AWS\nTuần 2: Thực hành và làm quen với VPC và EC2\nTuần 3: TÌm hiểu về S3 và EC2\nTuần 4: Tìm hiểu về VM Import/Export và làm quen với dịch vụ Amazon FSx\nTuần 5: Khám phá dịch vụ Security Hub và AWS Key Management Service (KMS)\nTuần 6: Học về RDS và các loại cơ sở dữ liệu Tuần 7: Khám phá về Amazon Lightsail và Amazon CloudWatch\nTuần 8: Tìm hiểu cơ bản về Data Lake trên AWS\nTuần 9: Khám phá về AWS Bedrock và GenAI trên AWS\nTuần 10: Tìm hiều về kiến trúc chatbot serverless trên AWS và PostgreSQL\nTuần 11: Thực hành làm dự án chatbot Tuần 12: Chuẩn bị proposal cho dự án chatbot và tổng hợp kiến trúc\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/3-ai/3.1/","title":"Tạo VPC  &amp; Cấu hình Sercurity Group cho RDS và Lambda","tags":[],"description":"","content":"Tạo VPC \u0026amp; Cấu hình Sercurity Group cho RDS và Lambda Các bước thực hiện 1. Truy cập dịch vụ VPC Vào AWS Management Console → tìm VPC.\nChọn Your VPCs → Create VPC. Chọn VPC and more và đặt tên cho project Ở phần cấu hình cho VPC Number of Availability Zones chọn 1 Number of public subnets chọn 1 và Number of private subnets chọn 2 và NAT gateways chọn Zonal NAT gateways chọn In 1 Az VPC endpoints chọn None → Create VPC\nQuá trình này sẽ diễn ra vài phút để có thể tạo NAT gateway.\nSau khi tạo xong chúng ta có thể coi cách Vpc and more tạo ra các tài nguyên thông qua Resource map Chúng ta sẽ tạo thêm một private subnet đặt ở một AZ khác để có thể tạo RDS được 2. Thiết lập Security Groups Chúng ta sẽ tạo 2 Security Group (SG) riêng biệt để bảo mật tối đa.\nVào phần Security Groups bấm Create security group chúng ta sẽ tạo 2 security group cho Lambda và RDS\nĐặt tên cho security group và chọn VPC chúng ta đã tạo ở bước 1 sau đó bấm create Tiếp tục tạo SG cho RDS chọn VPC ở bước 1 ở phần inbound rules ở phần type chọn PostgreSQL ở phần Source chọn SG đã tạo cho Lambda "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.1-week1/","title":"Worklog Tuần 1","tags":[],"description":"","content":"Mục tiêu tuần 1: Kết nối, làm quen với các thành viên trong First Cloud Journey. Hiểu dịch vụ AWS cơ bản. Đọc và nắm rõ các quy định cũng như nội quy của thực tập và First Cloud Journey. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Làm quen với các thành viên FCJ - Đọc và lưu ý các nội quy, quy định tại đơn vị thực tập 07/09/2025 08/09/2025 3 - Tìm hiểu AWS là gì, mục đích sử dụng - Xem video giới thiệu AWS trên YouTube và AWS Skill Builder - Tìm hiểu sơ qua về chương trình First Cloud Journey; 08/09/2025 09/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Tạo AWS Free Tier account - Làm quen với giao diện AWS Management Console - Thực hành: + Tạo AWS account\nThiết lập Virtual MFA Device 09/09/2025 10/09/2025 https://000001.awsstudygroup.com/vi/ 5 - Học về IAM (Identity and Access Management).: - Thực hành:\n+ Tạo admin group và admin user + Thiết lập quyền truy cập (policy) cơ bản cho user. + Kích hoạt MFA (Multi-Factor Authentication) để tăng bảo mật. 10/09/2025 11/09/2025 https://000001.awsstudygroup.com/vi/ 6 - Học cách thiết lập AWS Budgets để theo dõi chi phí hàng tháng. - Tìm hiểu về AWS Support case để biết cách gửi yêu cầu hỗ trợ. - Ôn lại các kiến thức vừa học và thực hành lại. 11/08/2025 12/08/2025 https://000001.awsstudygroup.com/vi/ Kết quả đạt được tuần 1: Làm quen với các thành viên trong FCJ, hiểu rõ môi trường làm việc và cơ cấu tổ chức tại đơn vị thực tập.\nĐọc, ghi nhớ và tuân thủ các nội quy, quy định của đơn vị trong quá trình thực tập.\nHiểu AWS (Amazon Web Services) là gì, nắm được mục đích sử dụng và lợi ích của điện toán đám mây trong thực tế.\nNắm được các nhóm dịch vụ cơ bản của AWS, bao gồm:\nCompute (EC2, Lambda) Storage (S3, EBS) Networking (VPC, Route 53, CloudFront) Database (RDS, DynamoDB) Tạo thành công tài khoản AWS Free Tier, bao gồm:\nXác minh email, số điện thoại và thông tin thanh toán. Thiết lập Virtual MFA Device cho tài khoản root nhằm tăng cường bảo mật. Làm quen với giao diện AWS Management Console, biết cách tìm và truy cập dịch vụ. Tìm hiểu và thực hành IAM (Identity and Access Management):\nTạo Admin group và Admin user riêng biệt. Gán chính sách quyền truy cập (Policy) phù hợp cho user. Kích hoạt MFA (Multi-Factor Authentication) cho tài khoản IAM user. Hiểu cách IAM giúp phân quyền và bảo vệ tài nguyên trong AWS. Thiết lập AWS Budgets để quản lý và giám sát chi phí sử dụng:\nTạo Cost Budget giới hạn chi tiêu trong Free Tier (ví dụ 5 USD/tháng). Bật cảnh báo email khi chi phí đạt 80% giới hạn. Tìm hiểu về AWS Support Case:\nBiết cách truy cập Support Center và tạo case yêu cầu hỗ trợ khi gặp sự cố. Phân biệt các gói Support: Basic (miễn phí), Developer, Business. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.2-week2/","title":"Worklog Tuần 2","tags":[],"description":"","content":"Mục tiêu tuần 2: Hiểu khái niệm và vai trò của VPC trong hạ tầng AWS. Biết cách tạo và cấu hình VPC (subnet, route table, internet gateway, security group). Triển khai, kết nối và quản lý EC2 instance trong VPC vừa tạo. Thực hành quản lý EC2 qua AWS Console và AWS CLI. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Hiểu rõ VPC là gì, vai trò của nó trong hệ thống AWS - Tìm hiểu về khái niệm: + VPC, Subnet, Route Table, Internet Gateway, NAT Gateway. + Phân biệt giữa public subnet và private subnet. 14/09/2025 15/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i/ 3 - Tạo và cấu hình VPC Thực hành: + Tạo một VPC mới từ đầu. + Cấu hình subnet, route table, internet gateway, sercurity group, NAT gateway. 15/09/2025 16/09/2025 https://000003.awsstudygroup.com/vi/ 4 - Tìm hiểu về EC2 - Launch và tìm hiểu các loại EC2 instance, AMI, key pair, security group - Thực hành: 1. Vào EC2 Dashboard → Launch Instance. 2. Chọn: - AMI: Amazon Linux 2 - Instance type: t2.micro (Free Tier)\n- Network: chọn VPC vừa tạo.\n- Subnet: chọn public-subnet.\n- Security group: cho phép SSH (port 22).\n3.Tạo hoặc sử dụng lại key pair để SSH vào máy. 4. Khởi tạo EC2 và kiểm tra trạng thái. 16/09/2025 17/09/2025 https://000004.awsstudygroup.com/vi/ 5 - Kết nối EC2 instance qua SSH. Thực hiện một số thao tác cơ bản với EC2 qua AWS CLI. 17/09/2025 18/09/2025 https://000004.awsstudygroup.com/vi/ 6 - Củng cố kiến thức và bảo mật VPC + EC2. Tìm hiểu Security Group và Network ACL. + Kết nối SSH private subnet 18/09/2025 19/09/2025 https://000004.awsstudygroup.com/vi/ Kết quả đạt được tuần 2: Hiểu biết lý thuyết\nNắm được khái niệm VPC (Virtual Private Cloud) và vai trò trong việc cô lập, kiểm soát mạng trong AWS. Hiểu được các thành phần chính của một VPC:\nSubnet (Public \u0026amp; Private) Route Table Internet Gateway (IGW) NAT Gateway Security Group và Network ACL Nắm được cơ chế hoạt động của EC2 (Elastic Compute Cloud) — dịch vụ cung cấp máy chủ ảo trong AWS.\nPhân biệt các khái niệm cơ bản:\nAMI (Amazon Machine Image) Instance type (t2.micro, t3.small, v.v.) Key Pair Elastic IP Security Group Thực hành với VPC\nTruy cập VPC Dashboard để xem cấu hình VPC mặc định của tài khoản AWS. Tạo mới một VPC riêng (CIDR: 10.0.0.0/16). Tạo 2 subnet: public-subnet (10.0.1.0/24) private-subnet (10.0.2.0/24) Tạo Internet Gateway và gắn vào VPC. Tạo Route Table, gán route tới IGW cho subnet public. Kiểm tra khả năng kết nối mạng giữa các subnet và ra Internet. Thực hành với EC2\nTìm hiểu và chọn AMI: Amazon Linux 2, Instance type: t2.micro (Free Tier). Tạo EC2 Instance nằm trong public-subnet thuộc VPC mới. Cấu hình: Security Group: cho phép SSH (port 22) và HTTP (port 80). Key Pair: tạo mới aws-keypair.pem để kết nối SSH. Kết nối SSH thành công tới EC2 qua Public IP "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.3-week3/","title":"Worklog Tuần 3","tags":[],"description":"","content":"Mục tiêu tuần 3: Hiểu sâu hơn về dịch vụ EC2 và S3. Biết cách tạo EC2 instance phục vụ cho việc cấu hình Storage Gateway. Tìm hiểu và thực hành CloudFront để phân phối nội dung toàn cầu. Tìm hiểu tính năng Replication Multi-Region trong S3 để đảm bảo tính sẵn sàng và dự phòng dữ liệu. Rèn luyện kỹ năng quản lý tài nguyên AWS trên cả Console và CLI. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Ôn lại các khái niệm EC2: Instance type, AMI, Key Pair, Security Group, EBS. - Tìm hiểu EBS Volume và Snapshot, cách tạo, attach và detach volume. 21/09/2025 22/09/2025 3 - Tìm hiểu tổng quan về Amazon S3 (Simple Storage Service). - Nắm rõ các khái niệm: Bucket, Object, Region, Versioning, Storage Class. - Thực hành: + Tạo một S3 Bucket. + Upload/Download file qua AWS Console. + Bật Versioning để lưu lại lịch sử thay đổi dữ liệu.\n22/09/2025 23/09/2025 https://000057.awsstudygroup.com/vi/ 4 - Tìm hiểu AWS Storage Gateway - Phân biệt 3 loại Gateway: File, Volume, Tape. - Thực hành: + Tạo EC2 instance để chạy Storage Gateway. + Cấu hình Storage Gateway trong AWS Console. + Kết nối với S3 để đồng bộ dữ liệu. 23/09/2025 24/09/2025 https://000024.awsstudygroup.com/vi/ 5 - Tìm hiểu AWS CloudFront: - Hiểu cơ chế hoạt động của Edge Locations và Distribution.\n- Thực hành: + Tạo CloudFront Distribution sử dụng dữ liệu từ S3 Bucket.\n+ Kiểm tra tốc độ truy cập từ nhiều khu vực. 24/09/2025 25/09/2025 https://000057.awsstudygroup.com/vi/ 6 - Tìm hiểu về Cross-Region Replication (CRR) trong S3. - Thực hành: + Tạo 2 bucket ở 2 region khác nhau (ví dụ: Singapore và Tokyo). + Kích hoạt CRR giữa 2 bucket. + Upload file và kiểm tra dữ liệu tự động replicate. 25/09/2025 26/09/2025 https://000057.awsstudygroup.com/vi/ Kết quả đạt được tuần 3: Hiểu rõ mối liên kết giữa EC2 – S3 – Storage Gateway – CloudFront.\nTự tạo và quản lý EC2 instance phục vụ các mục đích khác nhau.\nThành thạo việc tạo, upload, quản lý dữ liệu trong S3.\nBiết cách triển khai Storage Gateway để đồng bộ dữ liệu giữa on-premises và AWS.\nTạo và cấu hình CloudFront Distribution để tối ưu tốc độ phân phối nội dung.\nThiết lập thành công Cross-Region Replication trong S3 để đảm bảo tính sẵn sàng dữ liệu trên nhiều vùng.\nNâng cao kỹ năng thao tác song song trên AWS Console và CLI.\nThực hành với Amazon S3\nTạo bucket đầu tiên: my-first-s3-demo. Upload file (ảnh, text, csv) và kiểm tra đường dẫn công khai. Thiết lập quyền truy cập Public/Private, bật Versioning để quản lý thay đổi của file. Tạo Bucket Policy để chỉ cho phép truy cập từ tài khoản cụ thể. Thực hành Cross-Region Replication (CRR)\nTạo hai bucket: my-bucket-sg (region: Singapore) my-bucket-tokyo (region: Tokyo) Bật Cross-Region Replication giữa hai bucket. Upload file lên my-bucket-sg, kiểm tra file tự động được replicate sang my-bucket-tokyo. Hiểu rõ quy trình replicate object đa vùng giúp tăng độ tin cậy và sẵn sàng cho dữ liệu. Thực hành EC2 cho Storage Gateway\nTạo EC2 Instance (Amazon Linux 2 – t2.micro) để cài đặt Storage Gateway. Cấu hình: Gắn Elastic IP để truy cập ổn định. Mở các cổng cần thiết: 80, 443, 3260. Cài đặt Storage Gateway Appliance trên EC2. Kết nối EC2 với AWS Storage Gateway Console, chọn loại File Gateway. Tạo File Share liên kết với S3 bucket đã có. Kiểm tra hoạt động đọc/ghi dữ liệu từ Gateway tới S3. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.4-week4/","title":"Worklog Tuần 4","tags":[],"description":"","content":"Mục tiêu tuần 4: Hiểu rõ khái niệm VM Import/Export và lý do tại sao doanh nghiệp cần tính năng này. Biết cách import máy ảo (VM) từ môi trường on-premises lên AWS và export ngược lại khi cần. Làm quen với dịch vụ Amazon FSx for Windows File Server, hiểu cơ chế hoạt động, ưu điểm, và các tình huống sử dụng thực tế. Thực hành tạo, cấu hình và quản lý FSx File System. Nâng cao khả năng thao tác CLI và Console để xử lý các dịch vụ lưu trữ Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu khái niệm VM Import/Export trong AWS. - Tìm hiểu vai trò của S3 và IAM Role trong quá trình import/export. - Hiểu được mục đích: + Di chuyển máy ảo từ on-premises lên EC2. + Dự phòng hoặc backup VM giữa môi trường local và cloud. 28/09/2025 29/09/2025 https://000014.awsstudygroup.com/ 3 - Chuẩn bị môi trường cho VM Import/Export - Thực hành: + Tạo S3 bucket để lưu trữ file máy ảo. + Cấu hình IAM Role (vmimport) với các quyền cần thiết. + Cài đặt AWS CLI và kiểm tra thông tin tài khoản. 29/09/2025 30/09/2025 https://000014.awsstudygroup.com/ 4 - Thực hành import máy ảo vào EC2 - Sau khi hoàn tất, khởi tạo instance từ AMI được tạo ra. - Thực hành export EC2 instance về dạng máy ảo local 30/09/2025 01/10/2025 https://000014.awsstudygroup.com/ 5 - Tìm hiểu dịch vụ Amazon FSx for Windows File Server - Hiểu về Multi-AZ deployment - Phân biệt hai loại file system: + SSD Multi-AZ: Hiệu năng cao, thích hợp cho workload truy cập nhanh + HDD Multi-AZ: Chi phí thấp, phù hợp cho lưu trữ bản ghi, sao lưu hoặc dữ liệu ít truy cập. - Tìm hiểu về các tính năng nâng cao của FSx 01/10/2025 02/10/2025 https://000025.awsstudygroup.com/ 6 - Thực hành triển khai và quản trị FSx + Tạo và cấu hình File System + Quản lý và mở rộng FSx + Theo dõi hiệu năng 02/10/2025 03/10/2025 https://000025.awsstudygroup.com/ Kết quả đạt được tuần 4: Hiểu rõ quy trình VM Import/Export, có thể di chuyển máy ảo giữa môi trường on-premises và AWS Cloud. Nắm vững kiến thức về Amazon FSx for Windows File Server, bao gồm kiến trúc, loại lưu trữ, và tính năng Multi-AZ. Thực hành tạo và quản lý thành công SSD Multi-AZ và HDD Multi-AZ File Systems. Biết cách tạo file share mới, bật deduplication, shadow copies, và thiết lập user quotas. Theo dõi và đánh giá hiệu năng hệ thống bằng CloudWatch metrics và FSx performance reports. Quản lý hiệu quả các phiên làm việc của người dùng, bật Continuous Access share, và thực hiện scaling throughput \u0026amp; storage khi cần thiết. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.5-week5/","title":"Worklog Tuần 5","tags":[],"description":"","content":"Mục tiêu tuần 5: Hiểu rõ dịch vụ AWS Security Hub và cách nó tổng hợp các phát hiện bảo mật từ nhiều nguồn. Biết cách kích hoạt, cấu hình và phân tích các tiêu chuẩn bảo mật (CIS, PCI-DSS, AWS Foundational Best Practices). Hiểu AWS Key Management Service (KMS), các cơ chế mã hóa, key policy, CMK và các tình huống sử dụng thực tế. Thực hành tạo, quản lý và xoay vòng (rotation) khóa KMS. Tìm hiểu AWS Identity Center, bao gồm Permission Sets, nguồn danh tính (Identity Sources) và cách quản lý người dùng/nhóm. Nâng cao kỹ năng triển khai các dịch vụ bảo mật thông qua cả AWS Console và AWS CLI. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu AWS Security Hub: + Security Hub dùng để làm gì, tổng hợp phát hiện ra sao.. +Khái niệm Insights, Findings và các tích hợp 05/10/2025 06/10/2025 https://000018.awsstudygroup.com/2-enable-sec-hub/ 3 - Thực hành với Security Hub: + Bật CIS standard và phân tích những control failed phổ biến. + Xem và phân tích Findings chi tiết. + Phân biệt Security Hub – GuardDuty – Inspector 06/10/2025 07/10/2025 https://000018.awsstudygroup.com/2-enable-sec-hub/ 4 - Tìm hiểu AWS Key Management Service (KMS): + Khái niệm CMK, khóa đối xứng / bất đối xứng. + Cách hoạt động của Key Policy và quyền IAM. + Envelope Encryption \u0026amp; cách các dịch vụ AWS dùng KMS. 07/10/2025 08/10/2025 https://000033.awsstudygroup.com/ 5 - Thực hành với KMS: + Mã hóa / giải mã dữ liệu bằng AWS CLI. + Tích hợp KMS với S3, EBS, RDS, Lambda environment variables. + Bật Key Rotation. + Kiểm thử disable key \u0026amp; schedule key deletion. deletion. 08/10/2025 09/10/2025 https://000033.awsstudygroup.com/ 6 - Tìm hiểu \u0026amp; thực hành AWS Identity Center: + Hiểu về identity sources + Learn Permission Sets + Gán người dùng/nhóm vào các tài khoản AWS. +Kiểm thử quy trình đăng nhập và chuyển đổi role. 09/10/2025 10/10/2025 https://000012.awsstudygroup.com/vi/ Kết quả đạt được tuần 5: Hiểu rõ cách hoạt động của AWS Security Hub và các tiêu chuẩn bảo mật trong AWS. Đã bật và cấu hình thành công Security Hub Hiểu được vai trò của KMS trong việc mã hóa dữ liệu trên các dịch vụ AWS Rèn luyện kỹ năng trong AWS Identity Center: quản lý người dùng, nhóm, bộ quyền và quyền truy cập liên tài khoản. Cải thiện kiến ​​thức tổng thể về các biện pháp bảo mật tốt nhất của AWS. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.6-week6/","title":"Worklog Tuần 6","tags":[],"description":"","content":"Mục tiêu tuần 6: Hiểu khái niệm cơ bản về Amazon RDS, các loại database engine, backup, Multi-AZ và read replica. Hiểu Amazon Aurora, kiến trúc, lợi ích và khác biệt so với RDS. Nắm kiến thức nền tảng về Amazon Redshift dùng cho phân tích dữ liệu. Tìm hiểu Amazon ElastiCache (Redis/Memcached) và lý do caching cải thiện hiệu năng. Thực hành các thao tác cơ bản với từng dịch vụ. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu Amazon RDS. - Hiểu MySQL, PostgreSQL, MariaDB. - Nắm backup, Multi-AZ và read replica. 12/10/2025 13/10/2025 3 - Thực hành với RDS: + Tạo một DB instance. + Kết nối bằng DBeaver. + Thử snapshot backup \u0026amp; restore. 13/10/2025 14/10/2025 https://000005.awsstudygroup.com/ 4 - Tìm hiểu Amazon Aurora: + Kiến trúc Aurora vs RDS. + Khả năng mở rộng \u0026amp; High Availability. + Writer/Reader endpoints. 14/10/2025 15/10/2025 https://000005.awsstudygroup.com/ 5 - Tìm hiểu Amazon Redshift: + Khái niệm data warehouse. + Cluster, node và cách chạy query. + Các tình huống sử dụng phân tích dữ liệu. 15/10/2025 16/10/2025 https://000005.awsstudygroup.com/ 6 - Tìm hiểu ElastiCache + Bộ nhớ đệm là gì và tại sao nó cải thiện hiệu suất. + Redis vs Memcached khác nhau thế nào. + Tạo thử một Redis cluster đơn giản. volume 16/10/2025 17/10/2025 https://000005.awsstudygroup.com/ Kết quả đạt được tuần 6: Nắm cơ bản về RDS và tự tạo được một DB instance. Hiểu kiến trúc Aurora và cách Aurora đạt hiệu năng cao. Có kiến thức nền về Redshift cho phân tích dữ liệu. Hiểu caching và thực hành tạo một cluster Redis bằng ElastiCache. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.7-week7/","title":"Worklog Tuần 7","tags":[],"description":"","content":"Mục tiêu tuần 7: Hiểu Amazon Lightsail là gì và dùng khi nào. Biết cách triển khai ứng dụng đơn giản (WordPress, web server) trên Lightsail. Nắm các thành phần cơ bản: Instances, Networking, Snapshots, Databases. Hiểu Amazon CloudWatch và cách giám sát tài nguyên AWS. Thực hành tạo alarm, dashboard và xem log. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu về Amazon Lightsail. - So sánh Lightsail và EC2. -Xem các loại instance, blueprint và chi phí. 19/10/2025 20/10/2025 https://000045.awsstudygroup.com/ 3 - Thực hành với Lightsail: + Tạo một Lightsail instance + Kết nối bằng SSH ngay trên trình duyệt. + Tìm hiểu IP tĩnh và firewall rules. 20/10/2025 21/10/2025 https://000045.awsstudygroup.com/ 4 - Tìm hiểu Amazon CloudWatch: + Metrics, Logs, Events, Alarms. + Cách CloudWatch thu thập dữ liệu. + Namespaces như EC2, RDS, Lambda. 21/10/2025 22/10/2025 https://000008.awsstudygroup.com/ 5 - Thực hành CloudWatch: + Tạo Alarm cho CPU EC2. + Tạo Log Group và chạy Log Insights. + Tạo một CloudWatch Dashboard đơn giản. 22/10/2025 23/10/2025 https://000008.awsstudygroup.com/ 6 - Tìm hiểu giám sát Lightsail bằng CloudWatch: + Bật metrics của Lightsail vào CloudWatch. + Theo dõi CPU, network và storage. 23/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 7: Hiểu Lightsail và khi nào nên sử dụng. Tạo và quản lý thành công một Lightsail instance. Nắm các khái niệm CloudWatch: metrics, logs, alarms, dashboards. Tạo alarm và dashboard để giám sát hệ thống. Tích hợp Lightsail với CloudWatch để theo dõi hiệu năng. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.8-week8/","title":"Worklog Tuần 8","tags":[],"description":"","content":"Mục tiêu tuần 8: Hiểu khái niệm Data Lake trên AWS. Nắm vai trò của Amazon S3 trong việc xây dựng Data Lake. Tìm hiểu các dịch vụ dùng chung với Data Lake: Glue, Lake Formation, Athena. Hiểu cơ bản về Amazon DynamoDB và mô hình NoSQL. Thực hành đơn giản với S3, Athena, và DynamoDB. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu Data Lake là gì và lý do doanh nghiệp sử dụng. - Hiểu S3 là lớp lưu trữ chính cho Data Lake. -Tìm hiểu kiến trúc Data Lake architecture 26/10/2025 27/10/2025 https://000035.awsstudygroup.com/ 3 - Thực hành Data Lake : + Tạo S3 bucket cho raw \u0026amp; processed. + Upload dữ liệu mẫu CSV/JSON . + Query dữ liệu bằng Amazon Athena. 27/10/2025 28/10/2025 https://000035.awsstudygroup.com/ 4 - Tìm hiểu AWS Glue \u0026amp; Lake Formation + Glue Data Catalog basics. + Crawler tự nhận diện schema. + Tổng quan ETL. 28/10/2025 29/10/2025 https://000035.awsstudygroup.com/ 5 - Tìm hiểu DynamoDB: + So sánh NoSQL và SQL. + Partition key, Sort key. + Read/Write Capacity Modes 29/10/2025 30/10/2025 https://000039.awsstudygroup.com/ 6 - Thực hành với DynamoDB: + Tạo table với partition key và sort key. + Thêm, sửa, xoá record. + Thực hiện Query \u0026amp; Scan. 30/10/2025 31/10/2025 https://000039.awsstudygroup.com/ Kết quả đạt được tuần 8: Hiểu rõ cơ bản về Data Lake và vai trò của S3. Biết cách query dữ liệu trong S3 bằng Athena. Nắm được Glue \u0026amp; Lake Formation ở mức nền tảng. Có kiến thức vững về DynamoDB và NoSQL. Tự tạo và thao tác thành công một DynamoDB table. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.9-week9/","title":"Worklog Tuần 9","tags":[],"description":"","content":"Mục tiêu tuần 9: Hiểu cơ bản về AWS Bedrock và GenAI trên AWS. Nắm các Foundation Models: Claude, Llama, Titan, Mistral. Biết cách gọi Bedrock bằng Console và AWS SDK. Hiểu embedding \u0026amp; quy trình RAG đơn giản. Học nền tảng về AWS Lambda và các trigger phổ biến. Tạo một demo nhỏ dùng Lambda + Bedrock. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Giới thiệu AWS Bedrock. - Xem danh sách Foundation Models. - Thử tạo văn bản bằng Bedrock Console. 02/11/2025 03/11/2025 3 - Thực hành gọi API Bedrock bằng AWS SDK: + Gọi API sinh văn bản. + Hiểu token pricing. + Tạo IAM Role để sử dụng Bedrock. 03/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Học về embedding \u0026amp; RAG cơ bản: + Embedding là gì? + Tạo embedding bằng Titan. + Xây một pipeline RAG nhỏ 04/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Giới thiệu AWS Lambda: + Khái niệm serverless. + Tạo Lambda function đầu tiên. + Test bằng “Invoke”. 05/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Thực hành với Lambda: + Lambda triggers: API Gateway, S3, EventBridge. + Deploy version mới của Lambda. + Giới thiệu networking khi Lambda nằm trong VPC. networking. 06/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 9: Hiểu rõ cách hoạt động của AWS Bedrock và sử dụng nhiều foundation model. Thực hành gọi API Bedrock bằng SDK + IAM Role. Nắm được embedding và xây 1 pipeline RAG nhỏ. Tạo và test Lambda function với nhiều trigger. Tự tin kết hợp Lambda + Bedrock để tạo ứng dụng AI serverless cơ bản. "},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/2-backend/","title":"Backend Workshop (BE)","tags":[],"description":"","content":"Xây dựng Backend Trong workshop này, bạn sẽ:\nSử dụng Amazon S3 để lưu trữ file dữ liệu đầu vào (CSV). Cấu hình AWS Lambda Trigger để tự động import dữ liệu vào DynamoDB. Tạo Lambda Function (API Handler) và công khai qua API Gateway để truy cập dữ liệu. Kiểm thử API bằng Postman hoặc API Gateway Console. Dọn dẹp toàn bộ tài nguyên sau khi hoàn thành. "},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/3-ai/3.2/","title":"Cấu hình RDS và kết nối với Dbeaver","tags":[],"description":"","content":"Cấu hình RDS và kết nối với Dbeaver Các bước thực hiện 1. Truy cập dịch vụ RDS Vào AWS Management Console → tìm RDS. Trước khi tạo RDS chúng ta sẽ tạo subnet group Đặt tên cho subnet group và chọn VPC đã tạo AZ chọn ap-southeast-1a và ap-southeast-1b Subnet chọn 2 private subnet sau đó ấn Create 2. Tạo RDS Vào database → Create database Ở phần cấu hình database chọn Full configuration chọn PostgreSQL ờ phần Templates chọn Sandbox ở phần Settings đặt tên cho DB và đặt mật khẩu Cấu hình các phần còn lại như sau Ở phần Connectivity chọn VPC đã tạo và DB subnet group và Public access chọn No và phần VPC security group chọn Security group đã tạo cho RDS còn lại giữ nguyên và bấm Create 1. Lưu trữ dữ liệu và kết nối PostgreSQL bằng DBeaver Bạn có thể tải Dbeaver tại đây: https://dbeaver.io/ Tải file knowledge_base từ đây.\nĐể có thể kết nối từ máy local tới Dbeaver chúng ta cần tạo một EC2 để có thể làm cầu nối\nTruy cập vào EC2 → Launch instance\nĐặt tên cho EC2 và chọn Instance type t2.micro tạo một key pair và lưu trong máy Ở phần Network settings chọn VPC đã tạo chọn public subnet và tạo một Security group Inbound Security Group Rules chọn my ip xong Launch instance Tiếp theo chúng ta sẽ qua phần Security group của RDS chỉnh sửa phần inbound rules chúng ta sẽ add rule mới type: PostgreSQL và Source là EC2 vừa tạo Truy cập vào dbeaver ấn vào phần connection Chọn PostgreSQL ở phần Host copy port Endpoint của RDS và các thông tin bạn đã tạo RDS từ đầu Ấn vào dấu + SSH ở phần Host/IP copy public IP của EC2 User Name điền ec2-user ở phần Authentication Method chọn Public Key và chọn key pair đã tạo lúc tạo EC2 sau đó ấn Test connection rồi Finish Sau khi kết nối thành công mở sql script và dán dòng code này để tạo bảng knowledge_base sau khi tạo bảng refresh lại database để hiện bảng knowledge_base\n-- 1. Bật extension vector (chỉ chạy 1 lần) CREATE EXTENSION IF NOT EXISTS vector; -- 2. Tạo bảng kiến thức (Knowledge Base) CREATE TABLE knowledge_base ( id bigserial PRIMARY KEY, content text, -- Nội dung text gốc (đoạn văn đã chia nhỏ) metadata jsonb, -- Lưu thêm info: link ảnh, tên file, page number... embedding vector(1024) -- QUAN TRỌNG: Phải là 1024 cho Cohere Multilingual ); -- 3. Tạo index để tìm kiếm nhanh hơn CREATE INDEX ON knowledge_base USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64); Để có thể import dữ liệu vào Dbeaver bằng python chúng ta cần ssh thông qua cmd chúng ta sẽ mở cmd ở nơi lưu trữ keypair và copy câu lệnh này\nssh -i \u0026#34;my-key.pem\u0026#34; -L 5433:RDS endpoint port:5432 ec2-user@public IP EC2 -N Sau đó chúng ta chạy hàm python này để import dữ liệu vô Dbeaver\nimport pandas as pd import json import boto3 import psycopg2 import time import glob import os import numpy as np from dotenv import load_dotenv # ========================================== # 1. CẤU HÌNH \u0026amp; BẢO MẬT # ========================================== current_dir = os.path.dirname(os.path.abspath(__file__)) env_path = os.path.join(current_dir, \u0026#39;pass.env\u0026#39;) load_dotenv(env_path) CSV_FOLDER = \u0026#39;./database\u0026#39; DB_HOST = os.getenv(\u0026#34;DB_HOST\u0026#34;) DB_NAME = os.getenv(\u0026#34;DB_NAME\u0026#34;) DB_USER = os.getenv(\u0026#34;DB_USER\u0026#34;) DB_PASS = os.getenv(\u0026#34;DB_PASS\u0026#34;) # Kết nối AWS bedrock = boto3.client( service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;, aws_access_key_id=os.getenv(\u0026#34;aws_access_key_id\u0026#34;), aws_secret_access_key=os.getenv(\u0026#34;aws_secret_access_key\u0026#34;) ) # ========================================== # 2. TỪ ĐIỂN DỊCH (QUAN TRỌNG NHẤT) # ========================================== # A. Dịch Tên Cột (Cho AI hiểu ngữ cảnh) COLUMN_MAP = { \u0026#34;price\u0026#34;: \u0026#34;Giá bán\u0026#34;, \u0026#34;gia\u0026#34;: \u0026#34;Giá bán\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;Chi phí\u0026#34;, \u0026#34;fee\u0026#34;: \u0026#34;Phí\u0026#34;, \u0026#34;stock\u0026#34;: \u0026#34;Tồn kho\u0026#34;, \u0026#34;so_luong\u0026#34;: \u0026#34;Tồn kho\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Mô tả\u0026#34;, \u0026#34;mo_ta\u0026#34;: \u0026#34;Mô tả\u0026#34;, \u0026#34;chi_tiet\u0026#34;: \u0026#34;Chi tiết\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;Xuất xứ\u0026#34;, \u0026#34;xuat_xu\u0026#34;: \u0026#34;Xuất xứ\u0026#34;, \u0026#34;material\u0026#34;: \u0026#34;Chất liệu\u0026#34;, \u0026#34;chat_lieu\u0026#34;: \u0026#34;Chất liệu\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;Màu sắc\u0026#34;, \u0026#34;mau_sac\u0026#34;: \u0026#34;Màu sắc\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;Trọng lượng\u0026#34;, \u0026#34;trong_luong\u0026#34;: \u0026#34;Trọng lượng\u0026#34;, \u0026#34;food_type\u0026#34;: \u0026#34;Loại thức ăn\u0026#34;, \u0026#34;usage_target\u0026#34;: \u0026#34;Phù hợp cho loài chim\u0026#34;, \u0026#34;furniture_type\u0026#34;: \u0026#34;Loại nội thất\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;Thời gian xử lý\u0026#34;, \u0026#34;thoi_gian\u0026#34;: \u0026#34;Thời gian xử lý\u0026#34;, \u0026#34;method_name\u0026#34;: \u0026#34;Tên phương thức\u0026#34; } # B. Dịch Giá Trị (Cho Website hiển thị tiếng Việt) \u0026lt;--- PHẦN BẠN CẦN VALUE_TRANSLATIONS = { \u0026#34;FOODS\u0026#34;: \u0026#34;Đồ ăn\u0026#34;, \u0026#34;Foods\u0026#34;: \u0026#34;Đồ ăn\u0026#34;, \u0026#34;foods\u0026#34;: \u0026#34;Đồ ăn\u0026#34;, \u0026#34;TOYS\u0026#34;: \u0026#34;Đồ chơi\u0026#34;, \u0026#34;Toys\u0026#34;: \u0026#34;Đồ chơi\u0026#34;, \u0026#34;toys\u0026#34;: \u0026#34;Đồ chơi\u0026#34;, \u0026#34;FURNITURE\u0026#34;: \u0026#34;Nội thất\u0026#34;, \u0026#34;Furniture\u0026#34;: \u0026#34;Nội thất\u0026#34;, \u0026#34;furniture\u0026#34;: \u0026#34;Nội thất\u0026#34;, \u0026#34;Bird\u0026#34;: \u0026#34;Chim cảnh\u0026#34;, \u0026#34;bird\u0026#34;: \u0026#34;Chim cảnh\u0026#34; } # --- CÁC HÀM PHỤ TRỢ --- def get_embedding(text): try: if not text or len(str(text)) \u0026lt; 5: return None body = json.dumps({\u0026#34;texts\u0026#34;: [str(text)], \u0026#34;input_type\u0026#34;: \u0026#34;search_document\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34;}) response = bedrock.invoke_model(body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34;) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except: return None def clean(val): if pd.isna(val) or str(val).lower() in [\u0026#39;nan\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;null\u0026#39;]: return \u0026#34;\u0026#34; val_str = str(val).strip() # --- DỊCH TỰ ĐỘNG Ở ĐÂY --- # Nếu giá trị có trong từ điển dịch thì thay thế luôn if val_str in VALUE_TRANSLATIONS: return VALUE_TRANSLATIONS[val_str] return val_str def main(): try: conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS, port=5433) # Lưu ý port SSH 5433 cur = conn.cursor() print(\u0026#34;✅ Kết nối Database thành công!\u0026#34;) except Exception as e: print(f\u0026#34;❌ Lỗi kết nối DB: {e}\u0026#34;); return csv_files = glob.glob(os.path.join(CSV_FOLDER, \u0026#34;*.csv\u0026#34;)) print(f\u0026#34;📂 Tìm thấy {len(csv_files)} file CSV.\u0026#34;) # Biến thống kê stats = {\u0026#34;bird\u0026#34;: 0, \u0026#34;food\u0026#34;: 0, \u0026#34;toy\u0026#34;: 0, \u0026#34;furniture\u0026#34;: 0, \u0026#34;best_sellers\u0026#34;: []} total_success = 0 for file_path in csv_files: filename = os.path.basename(file_path).lower() print(f\u0026#34;\\n--- Đang xử lý file: {filename} ---\u0026#34;) try: df = pd.read_csv(file_path) df = df.replace({np.nan: None}) # Tự động nhận diện loại (Prefix) category_prefix = \u0026#34;Sản phẩm\u0026#34; if \u0026#34;bird\u0026#34; in filename: category_prefix = \u0026#34;Loài chim\u0026#34; elif \u0026#34;food\u0026#34; in filename: category_prefix = \u0026#34;Thức ăn chim\u0026#34; elif \u0026#34;toy\u0026#34; in filename or \u0026#34;do_choi\u0026#34; in filename: category_prefix = \u0026#34;Đồ chơi chim\u0026#34; elif \u0026#34;furniture\u0026#34; in filename: category_prefix = \u0026#34;Nội thất lồng chim\u0026#34; elif \u0026#34;ship\u0026#34; in filename or \u0026#34;delivery\u0026#34; in filename: category_prefix = \u0026#34;Phương thức vận chuyển\u0026#34; elif \u0026#34;payment\u0026#34; in filename: category_prefix = \u0026#34;Phương thức thanh toán\u0026#34; for index, row in df.iterrows(): # Thống kê if \u0026#34;bird\u0026#34; in filename: stats[\u0026#34;bird\u0026#34;] += 1 elif \u0026#34;food\u0026#34; in filename: stats[\u0026#34;food\u0026#34;] += 1 elif \u0026#34;toy\u0026#34; in filename: stats[\u0026#34;toy\u0026#34;] += 1 elif \u0026#34;furniture\u0026#34; in filename: stats[\u0026#34;furniture\u0026#34;] += 1 # A. ĐỊNH DANH p_id = clean(row.get(\u0026#39;id\u0026#39;) or row.get(\u0026#39;product_id\u0026#39;) or row.get(\u0026#39;payment_id\u0026#39;)) name = clean(row.get(\u0026#39;name\u0026#39;) or row.get(\u0026#39;product_name\u0026#39;) or row.get(\u0026#39;title\u0026#39;) or row.get(\u0026#39;method_name\u0026#39;)) if not name: if p_id: name = f\u0026#34;Mã {p_id}\u0026#34; else: continue # B. QUÉT CỘT TỰ ĐỘNG VÀ DỊCH content_parts = [f\u0026#34;{category_prefix}: {name}\u0026#34;] # Quét toàn bộ cột, nếu có trong COLUMN_MAP thì thêm vào for col_key, col_val in row.items(): val_clean = clean(col_val) # Hàm clean sẽ tự động dịch FOODS -\u0026gt; Đồ ăn if val_clean and col_key in COLUMN_MAP: content_parts.append(f\u0026#34;{COLUMN_MAP[col_key]}: {val_clean}\u0026#34;) # C. XỬ LÝ RIÊNG GIÁ \u0026amp; TỒN KHO \u0026amp; BÁN CHẠY price = clean(row.get(\u0026#39;price\u0026#39;) or row.get(\u0026#39;gia\u0026#39;) or row.get(\u0026#39;fee\u0026#39;)) if price: content_parts.append(f\u0026#34;Giá: {price}\u0026#34;) stock = clean(row.get(\u0026#39;stock\u0026#39;) or row.get(\u0026#39;so_luong\u0026#39;)) if stock: content_parts.append(f\u0026#34;Tồn kho: {stock}\u0026#34;) sold = clean(row.get(\u0026#39;sold\u0026#39;) or row.get(\u0026#39;da_ban\u0026#39;)) if sold: content_parts.append(f\u0026#34;Đã bán: {sold}\u0026#34;) try: if float(sold) \u0026gt; 0: stats[\u0026#34;best_sellers\u0026#34;].append((float(sold), name, category_prefix)) except: pass content_to_embed = \u0026#34;. \u0026#34;.join(content_parts) + \u0026#34;.\u0026#34; # D. TẠO METADATA (Cũng dùng giá trị đã dịch) # Lưu ý: Hàm clean() ở trên đã dịch rồi, nên ta gọi lại clean() cho từng field metadata = {} for k, v in row.items(): metadata[k] = clean(v) # Lưu vào metadata bản tiếng Việt luôn # Ghi đè các trường chuẩn metadata[\u0026#39;id\u0026#39;] = p_id metadata[\u0026#39;name\u0026#39;] = name metadata[\u0026#39;price\u0026#39;] = price if price else \u0026#34;Liên hệ\u0026#34; metadata[\u0026#39;type\u0026#39;] = category_prefix metadata[\u0026#39;image\u0026#39;] = clean(row.get(\u0026#39;image_url\u0026#39;) or row.get(\u0026#39;link_anh\u0026#39;)) metadata[\u0026#39;sold\u0026#39;] = sold # E. INSERT vector = get_embedding(content_to_embed) if vector: cur.execute( \u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (content_to_embed, json.dumps(vector), json.dumps(metadata, default=str)) ) total_success += 1 if total_success % 10 == 0: print(f\u0026#34; -\u0026gt; Đã nạp {total_success} dòng...\u0026#34;) conn.commit() time.sleep(0.1) except Exception as e: print(f\u0026#34;⚠️ Lỗi xử lý file {filename}: {e}\u0026#34;); continue # --- TẠO BẢN TIN THỐNG KÊ --- print(\u0026#34;\\n--- Đang tạo bản tin thống kê... ---\u0026#34;) top_products = sorted(stats[\u0026#34;best_sellers\u0026#34;], key=lambda x: x[0], reverse=True)[:5] top_names = \u0026#34;, \u0026#34;.join([f\u0026#34;{p[1]} ({int(p[0])} lượt mua)\u0026#34; for p in top_products]) summary_content = ( f\u0026#34;BÁO CÁO THỐNG KÊ SHOP CHIM: \u0026#34; f\u0026#34;Tổng số chim: {stats[\u0026#39;bird\u0026#39;]}. Đồ ăn: {stats[\u0026#39;food\u0026#39;]}. \u0026#34; f\u0026#34;Đồ chơi: {stats[\u0026#39;toy\u0026#39;]}. Nội thất: {stats[\u0026#39;furniture\u0026#39;]}. \u0026#34; f\u0026#34;TOP 5 SẢN PHẨM BÁN CHẠY NHẤT: {top_names}.\u0026#34; ) summary_vector = get_embedding(summary_content) if summary_vector: cur.execute(\u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (summary_content, json.dumps(summary_vector), json.dumps({\u0026#34;id\u0026#34;:\u0026#34;STATS\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Thống kê\u0026#34;}, default=str))) conn.commit() cur.close(); conn.close() print(f\u0026#34;\\n🎉 HOÀN TẤT! Tổng cộng đã import: {total_success + 1} dòng.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Sau khi import xong refresh lại bảng knowledge_base để thấy kết quả\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/2-backend/2.2/","title":"Kiểm tra dữ liệu trong DynamoDB","tags":[],"description":"","content":"Kiểm tra dữ liệu trong DynamoDB Trong bước này, bạn sẽ xác minh rằng dữ liệu từ file CSV đã được import thành công vào DynamoDB.\nCác bước thực hiện Upload dữ liệu CSV lên Bucket Tải file mẫu CSV từ đây.\nTrong Bucket vừa tạo:\nChọn tab Objects → Upload. Giải nén file zip. Kéo \u0026amp; thả các file, sau đó nhấn Upload. [!TIP] Sau khi upload xong hãy đợi khoảng 3-5 phút để lambda import dữ liệu\nTruy cập dịch vụ DynamoDB Vào AWS Management Console → tìm DynamoDB. Chọn Tables → click vào bảng ví dụ: Products. Xem dữ liệu Chọn tab Explore items. Kiểm tra danh sách sản phẩm đã được import. Nếu không thấy dữ liệu, kiểm tra:\nTên bảng DynamoDB phải trùng với tên file CSV. File CSV phải có header hợp lệ. Lambda có đủ quyền truy cập S3 và DynamoDB. Tạo GSI cho DynamoDB\nTạo GSI (Global Secondary Index) cho DynamoDB\nĐể backend có thể truy vấn dữ liệu dựa trên các trường không phải khóa chính (ví dụ: tìm Account bằng username, tìm Order bằng customer_id), chúng ta cần tạo các Global Secondary Indexes (GSI) tương ứng với cấu trúc trong mã nguồn Java.\nChúng ta sẽ sử dụng AWS CloudShell để chạy script tự động tạo toàn bộ Index nhằm đảm bảo chính xác và tiết kiệm thời gian.\nTrên thanh điều hướng phía trên cùng (Top Navigation Bar), nhấp vào biểu tượng CloudShell (hình terminal \u0026gt;_). Đợi vài giây để môi trường dòng lệnh khởi động.\nTại dòng lệnh CloudShell, tạo một file script mới tên là create_gsi.sh:\nXóa file (nếu đã tạo trước đó): rm create_all_gsi.sh\nTạo File mới: nano create_all_gsi.sh\nTải file về mở lên rồi copy toàn bộ vào Cloudshell.\nCtrl + O (Để lưu) -\u0026gt; Nhấn Enter\nCtrl + X (Để thoát)\nCấp quyền chạy cho file: chmod +x create_all_gsi.sh\nKhởi chạy file: ./create_all_gsi.sh\nThời gian đợi sẽ tùy thuộc vào việc Indexs được tạo ra trong bao lâu\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/4-frontend/4.2/","title":"Phân phối với CloudFront","tags":[],"description":"","content":"Tạo CloudFront Distribution trỏ đến S3 bucket Truy cập dịch vụ CloudFront. Nhấn Create CloudFront Distribution (Tạo Phân phối CloudFront). Nhập một Tên CloudFront (ví dụ: flyora-shop). Chọn S3 bucket mà bạn đã tạo trước đó. Bật Website endpoint (Điểm cuối Website), đảm bảo bạn nhận được URL có dạng như sau: [http://Tên-S3-của-bạn.s3-website.ap-southeast-1.amazonaws.com/](http://Tên-S3-của-bạn.s3-website.ap-southeast-1.amazonaws.com/) Cấu hình: - Viewer protocol policy (Chính sách giao thức người xem): Redirect HTTP to HTTPS (Chuyển hướng HTTP sang HTTPS) - Allowed HTTP method (Phương thức HTTP được phép): GET, HEAD, OPTION - Cache policy (Chính sách cache): CachingOptimized (Tối ưu hóa cache) - Response headers policy (Chính sách tiêu đề phản hồi): CORS-with-preflight-and-SecurityHeadersPolicy Chọn do not enable security protections (không bật tính năng bảo vệ bảo mật). Review (Xem lại) và nhấn Create Distribution (Tạo Phân phối). Sau khi tạo Distribution, bạn cần chờ 5 đến 10 phút để quá trình triển khai (deploy) hoàn tất. Nếu triển khai thành công, nó sẽ hiển thị ngày bạn đã triển khai. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.10-week10/","title":"Worklog Tuần 10","tags":[],"description":"","content":"Mục tiêu tuần 10: Hiểu kiến trúc chatbot serverless trên AWS. Học cách hoạt động của embedding Cohere (Embed v3). Hiểu vector search và lưu embedding vào PostgreSQL. Học Claude Haiku 3. Chuẩn bị hạ tầng AWS: Lambda, NAT Gateway, VPC. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu kiến trúc chatbot - Hiểu quy trình RAG workflow 09/11/2025 10/11/2025 3 - Học embedding Cohere (Embed v3): + Embedding hoạt động thế nào + Chunk dữ liệu + Tính cosine similarity 10/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Thực hành embedding: + Tạo embedding từ văn bản mẫu + Lưu vector vào PostgreSQL + Test truy vấn similarity 11/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tìm hiểu Claude Haiku 3: + Prompting + System message + Temperature \u0026amp; max tokens 12/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Set up AWS environment: + Tạo Lambda trong VPC + NAT Gateway cho outbound + Kết nối Lambda tới RDS PostgreSQL 13/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 10: Hiểu rõ kiến trúc chatbot \u0026amp; RAG. Thành thạo embedding Cohere và vector similarity. Lưu \u0026amp; truy vấn embedding trong PostgreSQL. Hiểu cơ bản Claude Haiku. Triển khai Lambda + NAT Gateway thành công. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.11-week11/","title":"Worklog Tuần 11","tags":[],"description":"","content":"Mục tiêu tuần 11: Xây backend chatbot trong Lambda. Kết hợp embedding + vector search + Claude Haiku. Public API qua API Gateway. Test chatbot end-to-end. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Xây logic Lambda: + Gọi Cohere để embed query + Query PostgreSQL để lấy tài liệu phù hợp 16/11/2025 17/11/2025 3 - Kết hợp Claude Haiku 3: + Tạo câu trả lời dựa vào tài liệu tìm được + Thêm prompts an toàn + định dạng phản hồi 17/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tạo API Gateway: + REST API + Kết nối Lambda + Thêm auth cơ bản 18/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Test toàn bộ hệ thống: + Send user query → Lambda → RDS → Cohere → Claude + Ghi log + Fix lỗi độ trễ 19/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Tối ưu hóa: + Add caching + Cải thiện prompt + Tối ưu database 20/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 11: Xây hoàn chỉnh backend chatbot bằng Lambda. Tích hợp thành công Cohere + PostgreSQL + Claude. Triển khai API Gateway hoạt động ổn định. Chatbot chạy end-to-end. Tối ưu tốc độ và chất lượng phản hồi. "},{"uri":"https://workshop-sample.fcjuni.com/vi/1-worklog/1.12-week12/","title":"Worklog Tuần 12","tags":[],"description":"","content":"Mục tiêu tuần 12: Chuẩn bị proposal cho dự án chatbot. Tổng hợp kiến trúc: Embedding (Cohere) + LLM (Claude Haiku 3) + RDS PostgreSQL + Lambda + API Gateway + NAT Gateway. Xác định phạm vi dự án, mục tiêu, milestone. Liệt kê các thách thức kỹ thuật và phương án giải quyết. Lên kế hoạch bước tiếp theo cho testing và triển khai production. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Review các thành phần đã triển khai: + Pipeline embedding Cohere + Tích hợp Claude Haiku 3 + Lambda functions +Lưu trữ vector trong RDS PostgreSQL storage 23/11/2025 24/11/2025 3 - Vẽ sơ đồ kiến trúc: + API Gateway → Lambda → RDS → Cohere → Claude → Lambda → Response + Bao gồm NAT Gateway \u0026amp; thiết kế VPC 24/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Xác định nội dung proposal: + Mục tiêu \u0026amp; phạm vi dự án + Technology stack + Workflow \u0026amp; RAG logic 25/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Liệt kê thách thức tiềm ẩn: + Latency + Lưu trữ embedding \u0026amp; tối ưu truy vấn + LLM prompt design \u0026amp; safety 26/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hoàn thiện draft proposal: + Bao gồm sơ đồ kiến trúc, mô tả thành phần, milestone, các lưu ý kỹ thuật + Chuẩn bị review \u0026amp; feedback 27/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 12: Hoàn tất draft proposal dự án chatbot. Ghi chép rõ ràng kiến trúc \u0026amp; workflow. Nhận diện thách thức kỹ thuật \u0026amp; đề xuất giải pháp. Xác định milestone \u0026amp; bước tiếp theo cho triển khai và testing. "},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/3-ai/","title":"AI Workshop (Chatbot)","tags":[],"description":"","content":"Workshop này hướng dẫn chi tiết cách xây dựng một Chatbot tư vấn sản phẩm sử dụng kiến trúc RAG (Retrieval-Augmented Generation) trên nền tảng AWS.\n1. Kiến trúc hệ thống Hệ thống sử dụng các dịch vụ sau của AWS:\nAmazon Bedrock: Cung cấp các mô hình AI (LLM). Generation Model: Amazon Nova Lite (anthropic.claude-3-haiku-20240307-v1:0) để trả lời câu hỏi. Embedding Model: Cohere Embed Multilingual (cohere.embed-multilingual-v3) để vector hóa dữ liệu. Amazon RDS (PostgreSQL): Lưu trữ dữ liệu sản phẩm và vector (sử dụng extension Dbeaver). AWS Lambda: Hàm xử lý logic trung gian (Serverless backend). "},{"uri":"https://workshop-sample.fcjuni.com/vi/3-blogstranslated/","title":"Các bài blogs đã dịch","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTại đây sẽ là phần liệt kê, giới thiệu các blogs mà các bạn đã dịch. Ví dụ:\nBlog 1 - Getting started with healthcare data lakes: Using microservices Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 2 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 3 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 4 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 5 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 6 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/4-frontend/4.3/","title":"Kết nối với API","tags":[],"description":"","content":"Bước 1: Lấy URL của API Gateway Trước hết, bạn phải nhận được URL của API Gateway: Mở thư mục dự án của bạn trong VS Code. Tạo một tệp mới có tên là api.js bên trong thư mục src. Thêm mã JavaScript sau: Bước 2: Kiểm tra API trong Postman Lấy dữ liệu từ Postman. Bạn sẽ nhận được phản hồi với mã trạng thái 200 kèm theo dữ liệu JSON: Bước 3: Triển khai và Xác minh trên S3 Truy cập trang web của bạn: http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com Sử dụng dữ liệu từ Postman với URL: Nếu bạn đăng nhập và thấy thông báo như thế này, thì việc Tích hợp API đã thành công: "},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/2-backend/2.3/","title":"Tạo API Gateway và tích hợp Lambda","tags":[],"description":"","content":"Mục tiêu Tạo Lambda Function mới để xử lý các yêu cầu đến DynamoDB thông qua API Gateway.\nCác bước thực hiện Tải file backend từ đây.\nBước 1: Tạo IAM Role Mở IAM Console → Roles → Create role Chọn Trusted entity: AWS Service → Lambda Gán quyền: AmazonDynamoDBFullAccess CloudWatchLogsFullAccess Đặt tên role: LambdaAPIAccessRole Bước 2: Tạo Lambda Function Vào AWS Lambda → Create function Chọn Author from scratch Nhập tên: DynamoDB_API_Handler Runtime: Java 17 Chọn IAM Role: LambdaAPIAccessRole Bước 3: Deploy file jar Vào S3 → Upload → Add files: Chọn file jar. Sau đó copy object Url Vào AWS Lambda → Upload from: dan s3 url vua tai len Vào AWS Lambda → Code → Runtime settings -\u0026gt; Edit\nHandler: org.example.flyora_backend.handler.StreamLambdaHandler::handleRequest Vào AWS Lambda → Code -\u0026gt; Configuration → Edit:\nTimeout: 1 min Vào AWS Lambda → Configuration → Environment variables → Edit\nKey: APP_JWT_SECRET; Value: huntrotflyorateam!@ky5group5member Tương tự: Key: GHN_TOKEN; Value: 445c659d-5586-11f0-8c19-5aba781b9b65\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/3-ai/3.3/","title":"Tạo logic cho hàm lambda","tags":[],"description":"","content":"Tạo logic cho hàm lambda Các bước thực hiện 1. Truy cập dịch vụ lambda Vào AWS Management Console → tìm Lambda. Đầu tiên qua phần layer tạo một layer để hàm lambda có thư viện kết nối với PostgreSQL (psycopg2).\nTruy cập vào link github để có thể tải phiên bản python phù hợp ở đây là sử dụng psycopg2-3.11 https://github.com/jkehler/awslambda-psycopg2\nSau khi tải xong bỏ tất cả folder psycopg2-3.11 vào một folder đặt tên là (python) sau đó zip folder đó lại và đặt tên (postgres-layer-3.11)\nBấm create layer đặt tên cho layer xong tải file zip postgres-layer-3.11 lên → create 2. Tạo Lambda Functions → Create function Đặt tên cho hàm lambda và chọn runtime Python 3.11 Additional configurations tích chọn VPC chọn VPC đã được tạo Subnet chọn 1 private subnet Security group chọn Lambda-SG Sau khi hàm lambda được tạo chúng ta sẽ đến phần layer add layer đã được tạo Custom layer chọn layer đã tạo version chọn 1 → Add Qua phần Configuration ở phần General configuration chỉnh thời gian lên 1p Permissions thêm AmazonBedrockFullAccess và AWSLambdaVPCAccessExecutionRole Environment variables thêm các biến: DB_HOST, DB_NAME, DB_USER, DB_PASS (Điền thông tin RDS của bạn vào). Sau khi cấu hình xong dán dòng code python này vô hàm lambda và nhấn nút deploy import json import boto3 import psycopg2 import os # --- CẤU HÌNH --- DB_HOST = os.environ.get(\u0026#39;DB_HOST\u0026#39;) DB_NAME = os.environ.get(\u0026#39;DB_NAME\u0026#39;) DB_USER = os.environ.get(\u0026#39;DB_USER\u0026#39;) DB_PASS = os.environ.get(\u0026#39;DB_PASS\u0026#39;) # Dùng vùng US East 1 cho ổn định bedrock = boto3.client(service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # --- 1. HÀM EMBEDDING (COHERE) --- def get_embedding(text): try: body = json.dumps({ \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34; }) response = bedrock.invoke_model( body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34; ) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except Exception as e: print(f\u0026#34;Lỗi Embed: {e}\u0026#34;) return None # --- 2. HÀM XỬ LÝ METADATA AN TOÀN --- def format_product_info(metadata): \u0026#34;\u0026#34;\u0026#34; Hàm này giúp chuẩn hóa dữ liệu dù file csv thiếu cột \u0026#34;\u0026#34;\u0026#34; if not metadata: return None # Lấy tên (Ưu tiên các key phổ biến) name = metadata.get(\u0026#39;name\u0026#39;) or metadata.get(\u0026#39;product_name\u0026#39;) or metadata.get(\u0026#39;title\u0026#39;) or \u0026#34;Sản phẩm không tên\u0026#34; # Lấy giá (Nếu không có thì để rỗng hoặc \u0026#39;Liên hệ\u0026#39;) price = metadata.get(\u0026#39;price\u0026#39;) or metadata.get(\u0026#39;gia\u0026#39;) or metadata.get(\u0026#39;display_price\u0026#39;) price_str = f\u0026#34;- Giá: {price}\u0026#34; if price else \u0026#34;\u0026#34; # Lấy loại (nếu có từ script import thông minh) category = metadata.get(\u0026#39;type\u0026#39;) or \u0026#34;Sản phẩm\u0026#34; # Tạo chuỗi mô tả cho AI đọc # Ví dụ: \u0026#34;Loài chim: Vẹt. - Giá: 500k\u0026#34; ai_context = f\u0026#34;Danh mục/Sản phẩm: {name} ({category}) {price_str}\u0026#34; # Tạo object cho Frontend hiển thị (Product Card) frontend_card = { \u0026#34;id\u0026#34;: metadata.get(\u0026#39;id\u0026#39;) or metadata.get(\u0026#39;product_id\u0026#39;), \u0026#34;name\u0026#34;: name, \u0026#34;price\u0026#34;: price if price else \u0026#34;Liên hệ\u0026#34;, # Frontend sẽ hiện chữ \u0026#34;Liên hệ\u0026#34; nếu không có giá \u0026#34;image\u0026#34;: metadata.get(\u0026#39;image_url\u0026#39;) or metadata.get(\u0026#39;link_anh\u0026#39;) or \u0026#34;\u0026#34;, # Ảnh có thể rỗng \u0026#34;type\u0026#34;: category # Để frontend biết đây là chim hay đồ chơi } return ai_context, frontend_card # --- 3. MAIN HANDLER --- def lambda_handler(event, context): print(\u0026#34;Event:\u0026#34;, event) try: # Parse Input if \u0026#39;body\u0026#39; in event: try: body_data = json.loads(event[\u0026#39;body\u0026#39;]) if isinstance(event[\u0026#39;body\u0026#39;], str) else event[\u0026#39;body\u0026#39;] except: body_data = {} else: body_data = event user_question = body_data.get(\u0026#39;question\u0026#39;, \u0026#39;\u0026#39;) if not user_question: return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Thiếu câu hỏi\u0026#39;)} # A. Tạo Vector q_vector = get_embedding(user_question) if not q_vector: return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Lỗi tạo vector\u0026#39;)} # B. Tìm kiếm DB conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS) cur = conn.cursor() # Lấy metadata lên để xử lý sql = \u0026#34;\u0026#34;\u0026#34; SELECT content, metadata FROM knowledge_base ORDER BY embedding \u0026lt;=\u0026gt; %s LIMIT 3 \u0026#34;\u0026#34;\u0026#34; cur.execute(sql, (json.dumps(q_vector),)) results = cur.fetchall() cur.close(); conn.close() # C. Xử lý kết quả (QUAN TRỌNG NHẤT) ai_contexts = [] frontend_products = [] for row in results: raw_content = row[0] # Nội dung gốc lúc import raw_metadata = row[1] # JSON metadata # Gọi hàm chuẩn hóa ai_text, card_data = format_product_info(raw_metadata) if ai_text: # Gộp nội dung gốc + nội dung định danh lại cho chắc ăn ai_contexts.append(f\u0026#34;{ai_text}. Chi tiết: {raw_content}\u0026#34;) frontend_products.append(card_data) # Nếu không tìm thấy gì if not ai_contexts: final_answer = \u0026#34;Xin lỗi, hiện tại shop chưa tìm thấy thông tin phù hợp trong kho dữ liệu ạ.\u0026#34; else: # D. Gửi cho LLM (Claude 3 Haiku) context_str = \u0026#34;\\n---\\n\u0026#34;.join(ai_contexts) system_prompt = ( \u0026#34;Bạn là nhân viên tư vấn của Shop Chim, tên là \u0026#39;Vẹt Tinh Thông\u0026#39;. \u0026#34; \u0026#34;Phong cách: Thân thiện, Lịch sự nhưng Ngắn gọn.\\n\\n\u0026#34; \u0026#34;HƯỚNG DẪN XỬ LÝ (ƯU TIÊN THEO THỨ TỰ):\\n\u0026#34; \u0026#34;1. GIAO TIẾP XÃ GIAO: Nếu khách chỉ chào hỏi (Xin chào, Hi...) hoặc cảm ơn: \u0026#34; \u0026#34;-\u0026gt; Hãy chào lại thân mật và hỏi khách cần tìm gì. TUYỆT ĐỐI KHÔNG tự ý liệt kê sản phẩm nếu khách chưa hỏi.\\n\u0026#34; \u0026#34; (Lưu ý: Đừng nhầm từ \u0026#39;Chào\u0026#39; trong \u0026#39;Xin chào\u0026#39; với chim \u0026#39;Chào mào\u0026#39;. Nếu khách chỉ chào, hãy lờ đi dữ liệu về chim Chào mào).\\n\\n\u0026#34; \u0026#34;2. TƯ VẤN SẢN PHẨM: Khi khách hỏi về hàng hóa:\\n\u0026#34; \u0026#34;-\u0026gt; Trả lời súc tích: Tên sản phẩm + Giá + Tình trạng kho (nếu có).\\n\u0026#34; \u0026#34;-\u0026gt; Không mô tả dài dòng văn hoa trừ khi khách hỏi chi tiết \u0026#39;như thế nào\u0026#39;, \u0026#39;ra sao\u0026#39;.\\n\u0026#34; \u0026#34;-\u0026gt; Nếu không có thông tin trong Context, hãy nói \u0026#39;Dạ shop chưa có món này ạ\u0026#39;.\\n\u0026#34; ) user_msg = f\u0026#34;Thông tin tham khảo:\\n{context_str}\\n\\nCâu hỏi: {user_question}\u0026#34; # Gọi Claude 3 Haiku claude_body = json.dumps({ \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: 300, \u0026#34;temperature\u0026#34;: 0.1, \u0026#34;system\u0026#34;: system_prompt, \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_msg}] }) try: model_res = bedrock.invoke_model( body=claude_body, modelId=\u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34; ) res_json = json.loads(model_res[\u0026#39;body\u0026#39;].read()) final_answer = res_json[\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;] except Exception as e: print(f\u0026#34;Lỗi gọi LLM: {e}\u0026#34;) final_answer = \u0026#34;Xin lỗi, hệ thống AI đang bận, vui lòng thử lại sau.\u0026#34; # E. Trả về kết quả return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;POST\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#34;answer\u0026#34;: final_answer, \u0026#34;products\u0026#34;: frontend_products # Frontend dùng cái này để vẽ UI }) } except Exception as e: print(f\u0026#34;Lỗi System: {e}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} 3. Tích hợp vào API Gateway của nhóm Truy cập dịch vụ API Gateway Chọn API mà Backend đã tạo Chọn Create resource Resource name chatbot và tích chọn vào CORS Chọn vào chatbot và tích chọn vào Create method Method type chọn POST tích chọn vào Lambda proxy integration Chọn VPC đã tạo xong ấn Create method Sau khi cấu hình xong bấm deploy API "},{"uri":"https://workshop-sample.fcjuni.com/vi/4-eventparticipated/","title":"Các events đã tham gia","tags":[],"description":"","content":"Trong quá trình thực tập, em đã tham gia 2 events, với mỗi event là một trải nghiệm đáng nhớ với những kiến thức mới, hay và bổ ích, cùng với đó là nhứng món quà và những khoảnh khắc rất tuyệt vời.\nEvent 1 Tên sự kiện: AWS Cloud Mastery Series #1\nThời gian: 08:00 ngày 15/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 2 Tên sự kiện: BUILDING AGENTIC AI Context Optimization with Amazon Bedrock\nThời gian: 09:00 ngày 05/12/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/4-frontend/","title":"Frontend Workshop (UI)","tags":[],"description":"","content":"Lưu trữ Frontend và Tích hợp API trên AWS Trong workshop này, bạn sẽ học cách triển khai một ứng dụng web frontend trên AWS và kết nối nó với API backend được lưu trữ thông qua Amazon API Gateway.\nHoạt động này là sự kết hợp giữa hosting frontend và tích hợp API, cho thấy cách các dịch vụ AWS có thể hỗ trợ các ứng dụng web serverless tương tác.\nAmazon S3 – Lưu trữ và phục vụ các tài nguyên web tĩnh của bạn (HTML, CSS, JS).\nAmazon CloudFront – Phân phối website của bạn trên toàn cầu với HTTPS và độ trễ thấp.\nAmazon API Gateway – Cung cấp các endpoint API backend mà frontend của bạn có thể gọi.\nWorkshop này minh họa cách kết nối một website tĩnh với backend API thông qua API Gateway, tạo nên một complete kiến trúc web serverless cho phép tương tác dữ liệu thời gian thực giữa frontend và backend.\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/2-backend/2.4/","title":"Tạo API Gateway và tích hợp với Lambda","tags":[],"description":"","content":"Mục tiêu Kết nối AWS API Gateway với Lambda Function để tạo endpoint RESTful cho phép truy cập dữ liệu trong DynamoDB.\nCác bước thực hiện 1. Truy cập dịch vụ API Gateway Vào AWS Console → API Gateway Chọn Create API Chọn loại REST API (Build) Chọn: Create new API: New API API name: FlyoraAPI Endpoint Type: Regional Chọn Create API 2. Tạo Resource và Method Trong sidebar, chọn Actions → Create Resource Resource Name: api Chọn Create Resource Chọn /api → Actions → Create Resource\nTrong cấu hình resource:\nResource path: /api/ Resource Name: v1 Nhấn Create resource Trong cấu hình resource:\nTick Proxy resource Resource path: /api/ Resource Name: {proxy+} Nhấn Create resource Chọn /v1 → Actions → Create Resource\nTrong cấu hình resource:\nTick Proxy resource Resource path: /api/v1/ Resource Name: {myProxy+} Nhấn Create resource 6. Enable CORS cho tất cả resource Vào method OPTIONS → Integration response → Header Mappings, đảm bảo có cấu hình: Access-Control-Allow-Origin: \u0026lsquo;*\u0026rsquo; Access-Control-Allow-Headers: \u0026lsquo;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026rsquo; Access-Control-Allow-Methods: \u0026lsquo;DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT\u0026rsquo; 3. Gắn Lambda Sau khi tạo thành công /api/vi/{myProxy+}, xuất hiện method ANY: Chọn ANY → Integration request → Edit Gắn Lambda: Integration type: Lambda Function Tick Lambda proxy integration Lambda Region: ap-southeast-1 (Singapore) Lambda Function: chọn hàm Lambda_API_Handler của bạn 4. Deploy API Chọn Actions → Deploy API Deployment stage: New stage Stage name: dev Description: Development stage for Lambda API Nhấn Deploy Sau khi deploy, bạn sẽ nhận được Invoke URL dạng: https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/5-cicd/","title":"CI/CD Automation","tags":[],"description":"","content":"Hướng dẫn thiết lập AWS CodeBuild cho Flyora Frontend Hướng dẫn này chỉ bao gồm thiết lập CI/CD cho Repository Frontend (React).\nHướng dẫn này sẽ giúp bạn thiết lập AWS CodeBuild và CodePipeline cho ứng dụng Flyora React frontend.\nYêu cầu Tài khoản AWS với quyền phù hợp GitHub repository: QuangHieu-lab/Flyora-shop File buildspec.yml trong thư mục gốc của repository (xem bên dưới) AWS Region: Singapore (ap-southeast-1) (hoặc region bạn chọn) S3 bucket để host static files CloudFront distribution (tùy chọn, cho CDN) Bắt buộc: File buildspec.yml Tạo file buildspec.yml trong thư mục gốc của repository với nội dung:\nversion: 0.2 phases: pre_build: commands: - echo \u0026#34;Installing dependencies on `date`\u0026#34; - npm ci - echo \u0026#34;Running linter...\u0026#34; - npm run lint --if-present || echo \u0026#34;No lint script configured\u0026#34; build: commands: - echo \u0026#34;Running tests on `date`\u0026#34; - npm test -- --watchAll=false --passWithNoTests --coverage || echo \u0026#34;Tests completed\u0026#34; - echo \u0026#34;Building application on `date`\u0026#34; - npm run build - echo \u0026#34;Build completed on `date`\u0026#34; post_build: commands: - echo \u0026#34;Post-build phase started on `date`\u0026#34; - echo \u0026#34;Checking if build directory exists...\u0026#34; - ls -la build/ - echo \u0026#34;Build artifacts created successfully\u0026#34; - echo \u0026#34;Build completed successfully - artifacts ready for deployment\u0026#34; - echo \u0026#34;Use CodePipeline Deploy stage for S3 deployment\u0026#34; - echo \u0026#34;CloudFront invalidation will be handled by CodePipeline\u0026#34; artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: build name: BuildArtifact discard-paths: yes cache: paths: - \u0026#39;/root/.npm/**/*\u0026#39; - \u0026#39;node_modules/**/*\u0026#39; Điểm chính:\nSử dụng npm ci để cài đặt nhanh và đáng tin cậy hơn Chạy linter nếu được cấu hình Chạy tests với coverage Build ứng dụng React Deployment được xử lý bởi CodePipeline Deploy stage (không trong buildspec) Cache npm và node_modules Bước 1: Tạo S3 Bucket để Hosting Trước khi thiết lập CodeBuild, tạo S3 bucket để host ứng dụng React:\nVào S3 console Click \u0026ldquo;Create bucket\u0026rdquo; Bucket name: flyora-frontend-hosting Region: Singapore (ap-southeast-1) Bỏ chọn \u0026ldquo;Block all public access\u0026rdquo; (để host static website) Bật \u0026ldquo;Static website hosting\u0026rdquo; trong bucket properties Đặt Index document: index.html Đặt Error document: index.html Bước 2: Truy cập CodeBuild Mở trình duyệt và truy cập: https://console.aws.amazon.com/codesuite/codebuild/projects Đăng nhập vào tài khoản AWS Đảm bảo đang ở region Singapore (ap-southeast-1) Click \u0026ldquo;Create build project\u0026rdquo; Bước 3: Cấu hình Project Trường Giá trị Project name flyora-frontend-build Description Build project for Flyora React frontend Build badge Để trống Bước 4: Cấu hình Source Trường Giá trị Source provider GitHub Repository Repository in my GitHub account GitHub repository https://github.com/QuangHieu-lab/Flyora-shop | Source version | Để trống | | Git clone depth | 1 | | Primary source webhook events | ⚠️ BỎ CHỌN |\nBước 5: Cấu hình Environment Phần Trường Giá trị Provisioning Provisioning model On-demand Environment image Managed image Operating system Amazon Linux Runtime(s) Standard Image Latest (vd: aws/codebuild/amazonlinux2-x86_64-standard:5.0) Service role Service role New service role Bước 6: Biến môi trường Thêm các biến môi trường trong CodeBuild:\nTên Giá trị Loại AWS_S3_BUCKET flyora-frontend-hosting Plaintext CLOUDFRONT_DISTRIBUTION_ID CloudFront distribution ID của bạn (nếu dùng) Plaintext REACT_APP_API_URL URL backend API của bạn Plaintext Bước 7: Buildspec và Logs Buildspec:\nBuild specifications: Use a buildspec file Buildspec name: Để trống Logs:\nCloudWatch logs: ✅ Bật Group name: /aws/codebuild/flyora-frontend Bước 8: Quyền IAM CodeBuild service role cần quyền truy cập S3 và CloudFront. Thêm policy này:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::flyora-frontend-hosting\u0026#34;, \u0026#34;arn:aws:s3:::flyora-frontend-hosting/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Bước 9: Tạo CodePipeline Vào CodePipeline console Click \u0026ldquo;Create pipeline\u0026rdquo; Pipeline name: flyora-frontend-pipeline Source stage: GitHub (Version 2) Build stage: Chọn flyora-frontend-build Deploy stage: Thêm S3 deploy action Action provider: Amazon S3 Bucket: flyora-frontend-hosting Extract file before deploy: ✅ Chọn Kiểm tra Sau khi tạo pipeline:\nThực hiện thay đổi trong React repository Commit và push lên GitHub Pipeline tự động trigger Kiểm tra S3 bucket có files đã cập nhật Truy cập website qua S3 endpoint hoặc CloudFront URL Xử lý sự cố Vấn đề: Build thất bại với \u0026ldquo;npm: command not found\u0026rdquo; Giải pháp: Đảm bảo runtime-versions: nodejs: 18 được đặt trong buildspec.yml\nVấn đề: S3 sync permission denied Giải pháp: Kiểm tra IAM role có quyền S3 (xem Bước 8)\nVấn đề: Website hiển thị nội dung cũ Giải pháp:\nXóa browser cache Invalidate CloudFront cache nếu dùng CDN Kiểm tra S3 bucket có files mới nhất Tham khảo nhanh Tài nguyên Giá trị Pipeline Name flyora-frontend-pipeline Build Project Name flyora-frontend-build S3 Bucket flyora-frontend-hosting Region Singapore (ap-southeast-1) Source GitHub (QuangHieu-lab/Flyora-shop) Buildspec buildspec.yml (trong repo root) Logs CloudWatch: /aws/codebuild/flyora-frontend Ước tính chi phí Hạng mục Chi phí CodePipeline $1/tháng cho mỗi pipeline CodeBuild (Free Tier) 100 phút build/tháng trong 12 tháng S3 Storage ~$0.023/GB/tháng S3 Requests Tối thiểu cho static hosting CloudFront Free tier: 1TB data transfer/tháng Hàng tháng (ước tính) $1-3/tháng Tóm tắt ✅ Đã cấu hình Frontend CI/CD Pipeline!\nĐã hoàn thành:\n✅ CodeBuild project cho ứng dụng React ✅ CodePipeline cho quy trình tự động ✅ Tích hợp GitHub với trigger tự động ✅ Deploy tự động lên S3 ✅ Tích hợp CloudFront CDN (tùy chọn) Pipeline của bạn hiện tại:\nTự động phát hiện thay đổi code trong GitHub Build ứng dụng React với npm Deploy static files lên S3 Phục vụ website qua S3 hoặc CloudFront "},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/2-backend/2.5/","title":"Kiểm thử API bằng Postman","tags":[],"description":"","content":"Mục tiêu Kiểm thử API Gateway REST endpoint tích hợp với Lambda Function để xác nhận hoạt động dữ liệu DynamoDB.\nTải và cài đặt Postman trước khi bắt đầu phần này.\nChỉnh lại Authorization Vào AWS Console → API Gateway Chọn FlyoraAPI Chọn /api/v1/{myProxy+} → ANY → Method request → Edit \u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; Updated upstream Authorization: AWS_IAM ======= Authorization: AWS_IAM (Lưu ý chỉ bật khi dùng postman để kiểm thử, sau đó phải tắt đi) Stashed changes Tạo access key Vào AWS Console → IAM → Users Nhấn Create User Đặt tên: test Xác nhận tạo iam user Vào test → Security credentiala → Create access key Chọn Local code Copy access key và Secret access key Kiểm thử GET Mở Postman\nChọn GET\nNhập URL: https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/product/1\nTab Headers: Key: Content-Type | Value: application/json\nTab Authorization:\nType: AWS Signature Nhap AccessKey Nhap SecretKey AWS Region: ap-southeast-1 Service Name: execute-api Nhấn Send\nKết quả: Trả về danh sách Items trong bảng reviews\nKiểm thử POST Chọn POST\nURL: https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/submit\nBody → raw → JSON\n{ \u0026#34;customerId\u0026#34;: 2, \u0026#34;rating\u0026#34;: 4, \u0026#34;comment\u0026#34;: \u0026#34;Chim ăn ngon và vui vẻ!\u0026#34;, \u0026#34;customerName\u0026#34;: \u0026#34;Nguyễn Văn B\u0026#34; } Nhấn Send\nKết quả: Thêm Items trong bảng Review\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Triển khai hệ thống thương mại điện tử Flyora trên AWS Tổng quan Trong workshop này, bạn sẽ triển khai các thành phần cốt lõi của nền tảng Flyora theo mô hình Serverless trên AWS.\nMục tiêu là xây dựng hệ thống có khả năng mở rộng, tối ưu chi phí và dễ bảo trì.\nCác thành phần triển khai:\nFrontend: Lưu trữ \u0026amp; phân phối giao diện qua S3 + CloudFront Backend API: Xử lý logic nghiệp vụ qua API Gateway + AWS Lambda Cơ sở dữ liệu: Quản lý dữ liệu sản phẩm / đơn hàng bằng DynamoDB + S3 Xác thực người dùng: Thực hiện thông qua Amazon Cognito Chatbot: Hỗ trợ tư vấn sản phẩm, tích hợp vào UI (Nhóm AI thực hiện) Workshop được phân chia theo vai trò nhóm để dễ triển khai song song: Backend (BE), AI (Chatbot), và Frontend (FE).\nKiến trúc tổng thể Nội dung Workshop Giới thiệu mục tiêu \u0026amp; kết quả kỳ vọng\nBackend Workshop (BE) — Xây dựng API + Pipeline import dữ liệu tự động\nChuẩn bị \u0026amp; Cấu hình Lambda Trigger cho S3 Tạo Lambda tự động ghi dữ liệu CSV vào DynamoDB (S3 Trigger) Tạo API Gateway và tích hợp Lambda làm Backend API Kiểm thử API bằng Postman / API Gateway Console AI Workshop (Chatbot) — Hỗ trợ tư vấn sản phẩm\nTạo VPC \u0026amp; Cấu hình Sercurity Group cho RDS và Lambda Cấu hình RDS và kết nối với Dbeaver Tạo logic cho hàm lambda Frontend Workshop (FE) — Hiển thị dữ liệu \u0026amp; Hosting website\nHosting website với S3 Phân phối với CloudFront Kết nối với API Thiết lập CI/CD tự động deploy\nDọn dẹp tài nguyên để tránh phát sinh chi phí\nWorkshop này được thiết kế chạy trong phạm vi AWS Free Tier,\nkhông sử dụng EC2, không SSH, và không yêu cầu dịch vụ trả phí.\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/6-cleanup/","title":"Dọn dẹp tài nguyên để tránh phát sinh chi phí","tags":[],"description":"","content":"Dọn dẹp tài nguyên AWS Để tránh phát sinh chi phí không mong muốn, bạn cần xóa các tài nguyên AWS đã tạo trong workshop theo thứ tự sau:\n1. Xóa EventBridge Rule Vào AWS Console → EventBridge Chọn Rules Chọn rule DatabaseBackup Chọn Delete Xác nhận xóa 2. Xóa API Gateway Vào AWS Console → API Gateway Chọn FlyoraAPI Chọn Actions → Delete API Xác nhận xóa bằng cách nhập tên API Nhấn Delete 3. Xóa Lambda Functions Vào AWS Console → Lambda Xóa các Lambda functions sau: DynamoDB_API_Handler AutoImportCSVtoDynamoDB DatabaseBackupFunction Với mỗi function: Chọn function → Actions → Delete Xác nhận xóa 4. Xóa DynamoDB Tables Vào AWS Console → DynamoDB Chọn Tables Xóa tất cả các bảng đã tạo từ CSV: Chọn tất cả bảng → Delete Xác nhận xóa bằng cách nhập delete 5. Xóa S3 Buckets và Objects 5.1. Xóa Bucket Database Vào AWS Console → S3 Chọn bucket flyora-bucket-database Xóa tất cả objects trong bucket: Chọn Empty bucket Xác nhận bằng cách nhập permanently delete Sau khi bucket rỗng: Chọn bucket → Delete bucket Xác nhận bằng cách nhập tên bucket 5.2. Xóa Bucket Backup Chọn bucket flyora-bucket-backup Xóa tất cả objects: Chọn Empty bucket Xác nhận xóa Xóa bucket: Chọn Delete bucket Xác nhận bằng cách nhập tên bucket 6. Xóa IAM User và Access Key Vào AWS Console → IAM → Users Chọn user test Vào tab Security credentials Xóa Access Key đã tạo: Chọn Access Key → Actions → Delete Quay lại danh sách Users Chọn user test → Delete user Xác nhận xóa 7. Xóa IAM Roles 7.1. Xóa LambdaAPIAccessRole Vào AWS Console → IAM → Roles Chọn LambdaAPIAccessRole Gỡ bỏ các policies đã gắn: AmazonDynamoDBFullAccess CloudWatchLogsFullAccess AWSXRayDaemonWriteAccess Chọn Delete role Xác nhận xóa 7.2. Xóa LambdaS3DynamoDBRole Chọn LambdaS3DynamoDBRole Gỡ bỏ các policies: AmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Chọn Delete role Xác nhận xóa 7.3. Xóa LambdaDynamoDBBackupRole Chọn LambdaDynamoDBBackupRole Gỡ bỏ các policies: AmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Chọn Delete role Xác nhận xóa 8. Xóa CloudWatch Logs Vào AWS Console → CloudWatch Chọn Logs → Log groups Tìm và xóa các log groups liên quan: /aws/lambda/DynamoDB_API_Handler /aws/lambda/AutoImportCSVtoDynamoDB /aws/lambda/DatabaseBackupFunction /aws/apigateway/FlyoraAPI Chọn log group → Actions → Delete log group(s) Xác nhận xóa 9. Xóa X-Ray Traces (Tùy chọn) X-Ray traces sẽ tự động hết hạn sau 30 ngày và không tính phí lưu trữ, nhưng bạn có thể xóa thủ công nếu muốn.\nVào AWS Console → X-Ray Chọn Traces Traces sẽ tự động bị xóa sau thời gian lưu trữ mặc định 10. Xóa RDS và Subnet groups Vào subnet group chọn subnet group đã tạo và ấn delete Vào database ấn vào database đã tạo → Action → Delete 11. Xóa Lambda BirdShopChatBot và layer Vào function chọn BirdShopChatBot → Action → Delete Vào Layers chọn layer đã tạo ấn delete 12. Xóa VPC và NAT gateway và Elastic IP, EC2 Vào VPC chọn NAT gateway đã tạo → Action → Delete NAT gateway Chọn Elastic IP → Action → Release Elastic IP addresses Sau khi xóa xong NAT gateway và Elastic IP qua phần Your VPCs ấn VPC đã tạo → Action → Delete VPC Qua phần EC2 chọn Instances chọn EC2 đã tạo → Instances state → Terminate instances 13. Xóa Cloudfront Vào CloudFront, chọn phân phối đã tạo → Actions → Disable. Chờ cho đến khi trạng thái chuyển sang Disabled. Select the checkbox for the disabled distribution again. Choose Delete and confirm the deletion. The distribution cannot be recovered once deleted. Kiểm tra lại Sau khi hoàn tất các bước trên, hãy kiểm tra lại các dịch vụ sau để đảm bảo không còn tài nguyên nào:\n✅ EventBridge: Không còn rule nào ✅ API Gateway: Không còn API nào ✅ Lambda: Không còn function nào (3 functions) ✅ DynamoDB: Không còn bảng nào ✅ S3: Không còn bucket nào (2 buckets) ✅ Cloudfront: Không còn cấu hình phân phối nội dung ✅ IAM Users: Không còn user test ✅ IAM Roles: Không còn 3 roles đã tạo ✅ CloudWatch Logs: Không còn log groups liên quan ✅ X-Ray: Traces sẽ tự động hết hạn ✅ RDS: Xóa thành công ✅ NAT gateway: Không còn nữa ✅ Elastic IP: Không còn nữa ✅ EC2 : Đã được terminate Hãy chắc chắn rằng bạn đã xóa tất cả các tài nguyên để tránh phát sinh chi phí không mong muốn. Đặc biệt chú ý xóa S3 buckets vì chúng có thể tích lũy dữ liệu theo thời gian.\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/2-backend/2.6/","title":"S3 CSV Backup","tags":[],"description":"","content":"S3 CSV Backup Tạo IAM Role cho Lambda Vào AWS Management Console → tìm IAM. Chọn Roles → Create Role. Chọn Trusted entity type: AWS service. Chọn Use case: Lambda, sau đó nhấn Next. Gán quyền truy cập cho Role Gắn các policy sau:\nAmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Nhấn Next, đặt tên role là LambdaDynamoDBBackupRole.\nRole này cho phép Lambda quét toàn bộ bảng DynamoDB và lưu bản backup dưới dạng CSV vào S3 Bucket.\nTạo S3 Bucket Truy cập dịch vụ S3. Trong giao diện S3, chọn Create bucket. Trong màn hình Create bucket:\nBucket name: Nhập tên, ví dụ:\nflyora-bucket-backup (Nếu tên đã tồn tại, hãy thêm số phía sau.)\nGiữ nguyên các thiết lập mặc định còn lại.\nXem lại cấu hình và chọn Create bucket để hoàn tất. Cấu hình Lambda Trigger cho S3 Tạo Lambda Function Truy cập Lambda → Create function. Chọn Author from scratch. Đặt tên: DatabaseBackupFunction. Runtime: Python 3.14. Role: chọn LambdaDynamoDBBackupRole đã tạo ở bước trước. Vào Configuration → Environment variables Chọn Edit. Add environment variable Key: BUCKET_NAME Value: flyora-bucket-backup Chọn Save. Code\nimport boto3 import csv import io import os from datetime import datetime from boto3.dynamodb.conditions import Key s3 = boto3.client(\u0026#39;s3\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) def scan_all(table): \u0026#34;\u0026#34;\u0026#34;Scan toàn bộ bảng DynamoDB, xử lý paging.\u0026#34;\u0026#34;\u0026#34; items = [] response = table.scan() items.extend(response.get(\u0026#39;Items\u0026#39;, [])) while \u0026#39;LastEvaluatedKey\u0026#39; in response: response = table.scan(ExclusiveStartKey=response[\u0026#39;LastEvaluatedKey\u0026#39;]) items.extend(response.get(\u0026#39;Items\u0026#39;, [])) return items def lambda_handler(event, context): bucket = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] tables = [t.strip() for t in os.environ[\u0026#34;TABLE_LIST\u0026#34;].split(\u0026#34;,\u0026#34;)] timestamp = datetime.utcnow().strftime(\u0026#34;%Y-%m-%d-%H-%M-%S\u0026#34;) for table_name in tables: try: table = dynamodb.Table(table_name) data = scan_all(table) if not data: print(f\u0026#34;Table {table_name} EMPTY → skip\u0026#34;) continue # Lấy danh sách tất cả fields all_keys = sorted({key for item in data for key in item.keys()}) # Convert to CSV csv_buffer = io.StringIO() writer = csv.DictWriter(csv_buffer, fieldnames=all_keys) writer.writeheader() for item in data: writer.writerow({k: item.get(k, \u0026#34;\u0026#34;) for k in all_keys}) key = f\u0026#34;dynamo_backup/{table_name}/{table_name}_{timestamp}.csv\u0026#34; s3.put_object( Bucket=bucket, Key=key, Body=csv_buffer.getvalue().encode(\u0026#34;utf-8\u0026#34;) ) print(f\u0026#34;Backup xong bảng {table_name} → {key}\u0026#34;) except Exception as e: print(f\u0026#34;Lỗi khi backup bảng {table_name}: {e}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tables\u0026#34;: tables } Chọn Deploy. Cấu hình chạy tự động (Schedule) Lambda → Triggers → Add Trigger → EventBridge (Schedule)\nRule name: DatabaseBackup\nRule description: AutoBackup in 4 days\nRule type: Schedule expression\nSchedule expression: rate(4 days)\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/6-self-evaluation/","title":"Tự đánh giá","tags":[],"description":"","content":"Trong suốt thời gian thực tập tại Amazon Web Services (AWS) từ 07/09 đến 28/11, tôi đã có cơ hội tiếp xúc với môi trường làm việc chuyên nghiệp, học hỏi từ các kỹ sư và tham gia vào các dự án thực tế liên quan đến dịch vụ điện toán đám mây.\nTôi đã tham gia xây dựng chatbot sử dụng các dịch vụ AWS như Lambda, API Gateway, RDS PostgreSQL, Bedrock, Cohere Embedding, và triển khai kiến trúc trong VPC. Qua đó, tôi đã cải thiện đáng kể các kỹ năng như thiết kế kiến trúc cloud, lập trình, phân tích hệ thống, viết báo cáo kỹ thuật, teamwork và giao tiếp trong môi trường công nghệ.\nVề tác phong, tôi luôn cố gắng hoàn thành nhiệm vụ đúng hạn, tuân thủ quy trình, và chủ động trao đổi với mentor để tối ưu hiệu quả công việc.\nĐể phản ánh quá trình thực tập một cách khách quan, tôi xin tự đánh giá như sau:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ☐ ✅ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ✅ ☐ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ☐ ✅ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ☐ ✅ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ☐ ✅ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ☐ ✅ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ✅ ☐ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ☐ ✅ ☐ Cần cải thiện Cần cải thiện tính kỷ luật, đặc biệt là trong quản lý thời gian và tính nhất quán với các tiêu chuẩn quy trình làm việc. Cần nâng cao khả năng đánh giá vấn đề một cách kỹ lưỡng hơn trước khi lựa chọn giải pháp kỹ thuật. Cần tăng cường kỹ năng giao tiếp kỹ thuật để giải thích ý tưởng rõ ràng hơn trong các cuộc thảo luận. "},{"uri":"https://workshop-sample.fcjuni.com/vi/7-feedback/","title":"Chia sẻ, đóng góp ý kiến","tags":[],"description":"","content":"Đánh giá chung 1. Môi trường làm việc\nKhông khí làm việc tại FCJ rất cởi mở và mang tính xây dựng cao. Em ấn tượng nhất là sự nhiệt tình của các anh chị đồng nghiệp; mọi người luôn sẵn lòng hỗ trợ về mặt kỹ thuật lẫn quy trình, giúp em không cảm thấy bị lạc lõng. Không gian văn phòng hiện đại tạo cảm hứng làm việc tốt.\n2. Sự hỗ trợ của mentor / team admin\nEm đánh giá rất cao phương pháp hướng dẫn của Mentor: thay vì chỉ đưa ra đáp án ngay lập tức, anh/chị luôn gợi mở để em tự tư duy và tìm giải pháp trước, sau đó mới góp ý điều chỉnh. Điều này giúp em nhớ kiến thức lâu hơn. Team Admin cũng hỗ trợ rất chu đáo về mặt thủ tục và tài liệu, giúp quá trình thực tập diễn ra suôn sẻ.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCác task được giao có sự liên kết chặt chẽ với chuyên ngành em đang theo học, nhưng ở mức độ ứng dụng thực tế cao hơn. Em có cơ hội tiếp cận những công nghệ mới mà trường chưa dạy sâu, nhờ đó thu hẹp được khoảng cách giữa lý thuyết và thực tiễn.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nNgoài kiến thức chuyên môn, kỳ thực tập này là cơ hội tuyệt vời để em rèn luyện tư duy quản lý công việc và kỹ năng giao tiếp trong môi trường doanh nghiệp (Business Communication). Những chia sẻ về định hướng nghề nghiệp từ Mentor cũng là hành trang quý giá cho em sau này.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa tại FCJ đề cao sự tôn trọng và bình đẳng. Dù là thực tập sinh, em vẫn cảm thấy tiếng nói của mình được lắng nghe. Tinh thần \u0026ldquo;làm hết sức, chơi hết mình\u0026rdquo; và sự hỗ trợ lẫn nhau trong các giai đoạn cao điểm của dự án là điều em rất trân trọng.\n6. Chính sách / phúc lợi cho thực tập sinh\nChính sách hỗ trợ của công ty rất thỏa đáng, đặc biệt là sự linh hoạt về thời gian giúp em cân bằng việc học. Các buổi training nội bộ (Internal Sharing) rất chất lượng và là điểm cộng lớn nhất trong chính sách đào tạo.\nMột số câu hỏi khác Hài lòng nhất: Sự kiên nhẫn và tận tâm của Mentor cùng cơ hội được \u0026ldquo;thực chiến\u0026rdquo; trong dự án thật.\nGiới thiệu bạn bè: Chắc chắn là có. Đây là môi trường lý tưởng để các bạn sinh viên mới ra trường rèn luyện thái độ và kỹ năng chuyên môn chuẩn chỉnh.\nĐề xuất \u0026amp; mong muốn Tiếp tục chương trình: Em rất mong muốn có cơ hội trở thành nhân viên chính thức hoặc tiếp tục cộng tác trong các dự án tới. "},{"uri":"https://workshop-sample.fcjuni.com/vi/5-workshop/2-backend/2.7/","title":"Tích hợp aws x-ray","tags":[],"description":"","content":"Mục tiêu Sử dụng AWS X-Ray để theo dõi và kiểm thử toàn bộ luồng xử lý của API Gateway → Lambda → DynamoDB.\nX-Ray giúp quan sát trace, latency, lỗi, và các đoạn (segment/subsegment) để đảm bảo API hoạt động đúng và tối ưu.\nCác bước thực hiện 1. Truy cập dịch vụ IAM Vào AWS Console → IAM Chọn Roles → LambdaAPIAccessRole Chọn Add permissions → Attach policies → AWSXRayDaemonWriteAccess 2. Truy cập Lambda Vào AWS Console → Lambda Chọn Functions → DynamoDB_API_Handler -Configuration → Monitoring and operations tools → Additional monitoring tools → Edit Lambda service traces: Tick Enable 3. Truy cập dịch vụ Api gateway Vào AWS Console → Api gateway Chọn APIs → FlyoraAPI Stages → Logs and tracing → Edit Tick X-Ray tracing 4. Kiểm thử Vào AWS Console → Lambda Ở mục Test, Create new event\nEvent name: test\nDán mục dưới đây vào Event JSON:\n{ \u0026#34;resource\u0026#34;: \u0026#34;/{myProxy+}\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/api/v1/bird-types\u0026#34;, \u0026#34;httpMethod\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;headers\u0026#34;: {}, \u0026#34;multiValueHeaders\u0026#34;: {}, \u0026#34;queryStringParameters\u0026#34;: {}, \u0026#34;multiValueQueryStringParameters\u0026#34;: {}, \u0026#34;pathParameters\u0026#34;: {}, \u0026#34;stageVariables\u0026#34;: {}, \u0026#34;requestContext\u0026#34;: { \u0026#34;identity\u0026#34;: {} }, \u0026#34;body\u0026#34;: null, \u0026#34;isBase64Encoded\u0026#34;: false } Save -\u0026gt; Test\nTiếp theo vào AWS Console → X-ray Ở mục Traces, Xuất hiện id mới Bấm vào, xem chi tiết các trace đó\n"},{"uri":"https://workshop-sample.fcjuni.com/vi/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://workshop-sample.fcjuni.com/vi/tags/","title":"Tags","tags":[],"description":"","content":""}]